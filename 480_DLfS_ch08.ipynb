{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "480-DLfS-ch08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMfkY5rxrD4p22Y9xo4FLiV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/480_DLfS_ch08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunZb1eqUdur"
      },
      "source": [
        "# ゼロから作るDeep Learning ch08"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoB0f6lbUZP1"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgYXd3C3UhdE"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!echo \"Make a working folder and move to there.\"\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%cd 202107_Tool-A/Work400/deep-learning-from-scratch-master\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk85QIoEUjR-"
      },
      "source": [
        "%cd ch08"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMC7qkSCUlvv"
      },
      "source": [
        "---\n",
        "# 8.1 deep_convnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV9YiId2t6JH"
      },
      "source": [
        "!python deep_convnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlnvBOqt9Tr"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "\n",
        "\n",
        "class DeepConvNet:\n",
        "    \"\"\"認識率99%以上の高精度なConvNet\n",
        "\n",
        "    ネットワーク構成は下記の通り\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        affine - relu - dropout - affine - dropout - softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=50, output_size=10):\n",
        "        # 重みの初期化===========\n",
        "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
        "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
        "        \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
        "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
        "        self.params['b7'] = np.zeros(hidden_size)\n",
        "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b8'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成===========\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        \n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
        "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_oUTRVaFb"
      },
      "source": [
        "# 8.1 train_deepnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlFjr1_VhqV"
      },
      "source": [
        "!python train_deepnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVw2WIMwVlkA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "532171ba-18b1-4ce7-8208-b8fce938a488"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from deep_convnet import DeepConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()  \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=20, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"deep_convnet_params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.2995497341939126\n",
            "=== epoch:1, train acc:0.104, test acc:0.122 ===\n",
            "train loss:2.270188088631024\n",
            "train loss:2.2746172637306192\n",
            "train loss:2.264618395121528\n",
            "train loss:2.2705238930508393\n",
            "train loss:2.282670961891062\n",
            "train loss:2.24990072919695\n",
            "train loss:2.235297286566395\n",
            "train loss:2.2451213271045263\n",
            "train loss:2.2874337520933206\n",
            "train loss:2.254334439814209\n",
            "train loss:2.232484637407497\n",
            "train loss:2.161518585172276\n",
            "train loss:2.1859621765980854\n",
            "train loss:2.215212269084495\n",
            "train loss:2.1600032923726706\n",
            "train loss:2.1488511180070673\n",
            "train loss:2.1696895592458\n",
            "train loss:2.1598709992522562\n",
            "train loss:2.1318960227235193\n",
            "train loss:2.0919281934205753\n",
            "train loss:2.0884701083884014\n",
            "train loss:2.0983051809248745\n",
            "train loss:1.9853038904617153\n",
            "train loss:1.9860435157755416\n",
            "train loss:2.0873369438184817\n",
            "train loss:2.049576932864473\n",
            "train loss:1.942408652409653\n",
            "train loss:2.0273187187972384\n",
            "train loss:2.0463233723186067\n",
            "train loss:1.889725635446127\n",
            "train loss:1.8462860338700806\n",
            "train loss:1.8202406675771128\n",
            "train loss:1.9432992680934933\n",
            "train loss:1.925790469453088\n",
            "train loss:1.9098735910462907\n",
            "train loss:2.0218957874186243\n",
            "train loss:1.8106258155620165\n",
            "train loss:1.8003915085946929\n",
            "train loss:1.9634713928555771\n",
            "train loss:1.7235424982770495\n",
            "train loss:2.046911257860954\n",
            "train loss:1.8087579880049378\n",
            "train loss:1.8245194313947848\n",
            "train loss:1.886852312255112\n",
            "train loss:1.7265234664456457\n",
            "train loss:1.764955645890129\n",
            "train loss:1.6993883267469665\n",
            "train loss:1.6293900848526872\n",
            "train loss:1.8320055559899502\n",
            "train loss:1.8000757681857618\n",
            "train loss:1.7325091818195184\n",
            "train loss:1.870801314042434\n",
            "train loss:1.6789291307687821\n",
            "train loss:1.800314296671967\n",
            "train loss:1.7953758478106456\n",
            "train loss:1.6386315886770155\n",
            "train loss:1.7272261438103813\n",
            "train loss:1.7910611013554893\n",
            "train loss:1.8048263955065373\n",
            "train loss:1.9115353492960718\n",
            "train loss:1.7531823708310676\n",
            "train loss:1.9164472226884257\n",
            "train loss:1.8214379039592044\n",
            "train loss:1.9214462939276\n",
            "train loss:1.7480022058812572\n",
            "train loss:1.8255403896675046\n",
            "train loss:1.8697340310076407\n",
            "train loss:1.6579249248744556\n",
            "train loss:1.5410988843307782\n",
            "train loss:1.8337180708078475\n",
            "train loss:1.7626804365682753\n",
            "train loss:1.8470088960209403\n",
            "train loss:1.7269262109908772\n",
            "train loss:1.6486711723498895\n",
            "train loss:1.782552973319798\n",
            "train loss:1.790724118396251\n",
            "train loss:1.4880809399019441\n",
            "train loss:1.718639101433433\n",
            "train loss:1.7603586584623725\n",
            "train loss:1.6379754093426357\n",
            "train loss:1.5062947571222904\n",
            "train loss:1.593547266759467\n",
            "train loss:1.6854661176497023\n",
            "train loss:1.6646461429279458\n",
            "train loss:1.7812881281370216\n",
            "train loss:1.656508884733712\n",
            "train loss:1.7202290329897734\n",
            "train loss:1.6551435389271927\n",
            "train loss:1.700489030315814\n",
            "train loss:1.5678690980804055\n",
            "train loss:1.5403449915575473\n",
            "train loss:1.612962814224104\n",
            "train loss:1.5469408485533842\n",
            "train loss:1.6378618363532853\n",
            "train loss:1.5825707390450203\n",
            "train loss:1.504883973788635\n",
            "train loss:1.7196658145219936\n",
            "train loss:1.6016324739557855\n",
            "train loss:1.511649668602376\n",
            "train loss:1.679326104612204\n",
            "train loss:1.7190482028273204\n",
            "train loss:1.5343064854036763\n",
            "train loss:1.8671130636522326\n",
            "train loss:1.5023711367321706\n",
            "train loss:1.5463439092945073\n",
            "train loss:1.5687259486765932\n",
            "train loss:1.7038056492591906\n",
            "train loss:1.5120401909805008\n",
            "train loss:1.7695428142091594\n",
            "train loss:1.6342730634192342\n",
            "train loss:1.642582515474121\n",
            "train loss:1.6815717608612635\n",
            "train loss:1.4941679246069817\n",
            "train loss:1.6983782952269377\n",
            "train loss:1.5775350013130804\n",
            "train loss:1.5674501395098628\n",
            "train loss:1.505042298455285\n",
            "train loss:1.4359770019883147\n",
            "train loss:1.7425213556301948\n",
            "train loss:1.6821285919863607\n",
            "train loss:1.5729188744668434\n",
            "train loss:1.6019563446483533\n",
            "train loss:1.5351855864762083\n",
            "train loss:1.6352410025433983\n",
            "train loss:1.413876886132708\n",
            "train loss:1.385364859704154\n",
            "train loss:1.4580470261134095\n",
            "train loss:1.6512822306203474\n",
            "train loss:1.3740065031298556\n",
            "train loss:1.5523660909307488\n",
            "train loss:1.5100948655274857\n",
            "train loss:1.2590547563018382\n",
            "train loss:1.759386303423912\n",
            "train loss:1.5928858787073332\n",
            "train loss:1.4662970076891768\n",
            "train loss:1.286107542026889\n",
            "train loss:1.5556082642780225\n",
            "train loss:1.4194793864756157\n",
            "train loss:1.8014895205432786\n",
            "train loss:1.4258087838903648\n",
            "train loss:1.3959833202469556\n",
            "train loss:1.4055588436220336\n",
            "train loss:1.4119894023125434\n",
            "train loss:1.4925509788619522\n",
            "train loss:1.4282197476248617\n",
            "train loss:1.3842935189628096\n",
            "train loss:1.4194637208865089\n",
            "train loss:1.4336609799225053\n",
            "train loss:1.5225231837246491\n",
            "train loss:1.431112014305979\n",
            "train loss:1.4643303883196408\n",
            "train loss:1.6969939318765168\n",
            "train loss:1.4352548875072737\n",
            "train loss:1.6768190720812286\n",
            "train loss:1.4012885039755614\n",
            "train loss:1.509972032647341\n",
            "train loss:1.3954078291395917\n",
            "train loss:1.5437436510409226\n",
            "train loss:1.444210805598095\n",
            "train loss:1.3686711685680146\n",
            "train loss:1.594498887137759\n",
            "train loss:1.3170690919472698\n",
            "train loss:1.4578731809788406\n",
            "train loss:1.4215251260048667\n",
            "train loss:1.4008773407559751\n",
            "train loss:1.3010334648148119\n",
            "train loss:1.353189239830376\n",
            "train loss:1.501665676782808\n",
            "train loss:1.432730095286256\n",
            "train loss:1.4042909452992207\n",
            "train loss:1.1754415126752997\n",
            "train loss:1.3385680577926138\n",
            "train loss:1.39495407720777\n",
            "train loss:1.437409363792795\n",
            "train loss:1.508291968865452\n",
            "train loss:1.3369690908124332\n",
            "train loss:1.3579925868136693\n",
            "train loss:1.2544114976951797\n",
            "train loss:1.41341015879435\n",
            "train loss:1.4276834308446564\n",
            "train loss:1.3627400589441285\n",
            "train loss:1.3289398740116007\n",
            "train loss:1.3738157391712589\n",
            "train loss:1.428705905364496\n",
            "train loss:1.5262789102863905\n",
            "train loss:1.1826062202882937\n",
            "train loss:1.3567654135039908\n",
            "train loss:1.4350293762499131\n",
            "train loss:1.3816137581796102\n",
            "train loss:1.4671540741860098\n",
            "train loss:1.1548136830653721\n",
            "train loss:1.3080058545691982\n",
            "train loss:1.2796015473272133\n",
            "train loss:1.323412700404489\n",
            "train loss:1.2167377573436193\n",
            "train loss:1.4005291551521624\n",
            "train loss:1.2890081716547257\n",
            "train loss:1.3983394320736917\n",
            "train loss:1.271184623262022\n",
            "train loss:1.42140868894675\n",
            "train loss:1.2917295772664956\n",
            "train loss:1.3429185935692407\n",
            "train loss:1.3805281762876385\n",
            "train loss:1.2531691942251384\n",
            "train loss:1.2186479974302886\n",
            "train loss:1.3169722853905106\n",
            "train loss:1.3277191579567829\n",
            "train loss:1.293260975815597\n",
            "train loss:1.3562138756856286\n",
            "train loss:1.3097947054185983\n",
            "train loss:1.4170131647973994\n",
            "train loss:1.3895263409038818\n",
            "train loss:1.3451416116407982\n",
            "train loss:1.361688768130902\n",
            "train loss:1.2628135827202736\n",
            "train loss:1.4542083030367323\n",
            "train loss:1.293482993482837\n",
            "train loss:1.2452032830029933\n",
            "train loss:1.4826205967536696\n",
            "train loss:1.1882729503280596\n",
            "train loss:1.4220512241256305\n",
            "train loss:1.3387033759294338\n",
            "train loss:1.3758794754031565\n",
            "train loss:1.3358563614084726\n",
            "train loss:1.3949261928992271\n",
            "train loss:1.4523941901645108\n",
            "train loss:1.5035458148293204\n",
            "train loss:1.3338057888101453\n",
            "train loss:1.352755938034218\n",
            "train loss:1.2527151370665182\n",
            "train loss:1.3941283662765296\n",
            "train loss:1.282421051941765\n",
            "train loss:1.198091721859335\n",
            "train loss:1.1679288037039495\n",
            "train loss:1.3914001026285887\n",
            "train loss:1.3530971360855397\n",
            "train loss:1.312446630760927\n",
            "train loss:1.231382922771565\n",
            "train loss:1.2683856979405903\n",
            "train loss:1.3612104716049622\n",
            "train loss:1.4803340104904903\n",
            "train loss:1.2115535894175071\n",
            "train loss:1.3251275150064266\n",
            "train loss:1.2479580881457122\n",
            "train loss:1.204711592025153\n",
            "train loss:1.3381253048916637\n",
            "train loss:1.3883219985291737\n",
            "train loss:1.3790465006745631\n",
            "train loss:1.3511245923404789\n",
            "train loss:1.2333657276307752\n",
            "train loss:1.2786288444280631\n",
            "train loss:1.4778845211094083\n",
            "train loss:1.1977266033786111\n",
            "train loss:1.257478606211575\n",
            "train loss:1.4024647670065982\n",
            "train loss:1.2779147798714705\n",
            "train loss:1.293557630512187\n",
            "train loss:1.3428649995777078\n",
            "train loss:1.0892914410122354\n",
            "train loss:1.313945345430219\n",
            "train loss:1.2746780331040648\n",
            "train loss:1.3378357447382292\n",
            "train loss:1.0994630879954501\n",
            "train loss:1.12877513699528\n",
            "train loss:1.3103367978363272\n",
            "train loss:1.2688798595470139\n",
            "train loss:1.3431528815865308\n",
            "train loss:1.350680460320424\n",
            "train loss:1.244539481809323\n",
            "train loss:1.4359180238532931\n",
            "train loss:1.2508300745280108\n",
            "train loss:0.9725617183996939\n",
            "train loss:1.2813895834961861\n",
            "train loss:1.235807202985103\n",
            "train loss:1.271574619053213\n",
            "train loss:1.3869542148415794\n",
            "train loss:1.1942668499498097\n",
            "train loss:1.2925060278051703\n",
            "train loss:1.3914216177804872\n",
            "train loss:1.1362397252259593\n",
            "train loss:1.1584061727879864\n",
            "train loss:1.443493845985471\n",
            "train loss:1.2158813525343408\n",
            "train loss:1.3511431095334958\n",
            "train loss:1.1390189746178203\n",
            "train loss:1.2133034722893692\n",
            "train loss:1.2438356457900328\n",
            "train loss:1.2942550179141243\n",
            "train loss:1.347456011981586\n",
            "train loss:1.2302411138399671\n",
            "train loss:1.4197083981590684\n",
            "train loss:0.9682038422763378\n",
            "train loss:1.3171810755559668\n",
            "train loss:1.1675144028671325\n",
            "train loss:1.3487251829495241\n",
            "train loss:1.5572171069271254\n",
            "train loss:1.215046201900686\n",
            "train loss:1.2188111381957227\n",
            "train loss:1.2565648535603113\n",
            "train loss:1.1375667747602574\n",
            "train loss:1.2021493164196684\n",
            "train loss:1.3443192996452924\n",
            "train loss:1.2840961965463875\n",
            "train loss:1.3081161753524726\n",
            "train loss:1.1818626620421402\n",
            "train loss:1.2967156487950207\n",
            "train loss:1.2783514913731129\n",
            "train loss:1.0018573930178838\n",
            "train loss:1.1528022074240882\n",
            "train loss:1.1256645778054202\n",
            "train loss:1.2984310992982682\n",
            "train loss:1.371133104096852\n",
            "train loss:1.0776292493357962\n",
            "train loss:1.2798131268109894\n",
            "train loss:1.1613564803826564\n",
            "train loss:1.2780177689287222\n",
            "train loss:1.3770276185822297\n",
            "train loss:1.172563050478643\n",
            "train loss:1.3032132118908413\n",
            "train loss:1.2447871525534908\n",
            "train loss:1.4630791153998592\n",
            "train loss:1.1883365127867376\n",
            "train loss:1.2806879258781823\n",
            "train loss:1.2031902222337538\n",
            "train loss:1.186538989979147\n",
            "train loss:1.0101155264291544\n",
            "train loss:1.2090457773206702\n",
            "train loss:0.9886892882835183\n",
            "train loss:1.3183728996106894\n",
            "train loss:1.1751249319323425\n",
            "train loss:1.502333358207443\n",
            "train loss:1.2034648569588207\n",
            "train loss:1.1335742303407184\n",
            "train loss:1.1527338383693309\n",
            "train loss:1.1707227380380074\n",
            "train loss:1.03260755461231\n",
            "train loss:1.1212314284177667\n",
            "train loss:1.2687157631384423\n",
            "train loss:1.1390777561599565\n",
            "train loss:1.4029126385002357\n",
            "train loss:1.2192344804177093\n",
            "train loss:1.030867047226161\n",
            "train loss:1.2120976867517694\n",
            "train loss:1.346991137204349\n",
            "train loss:1.180736561688714\n",
            "train loss:1.203393092855277\n",
            "train loss:1.3929624000755976\n",
            "train loss:1.2193321091498353\n",
            "train loss:1.1580727618285203\n",
            "train loss:1.2970500697770513\n",
            "train loss:1.4319616319093504\n",
            "train loss:1.1677593258047394\n",
            "train loss:1.2084812306189794\n",
            "train loss:1.1900280186781957\n",
            "train loss:1.3125156299842557\n",
            "train loss:1.1497435443078174\n",
            "train loss:1.326583000426664\n",
            "train loss:1.1144407667726606\n",
            "train loss:1.265022591261478\n",
            "train loss:1.2093148638548377\n",
            "train loss:1.1444768499063918\n",
            "train loss:1.0517132570080276\n",
            "train loss:0.967985292563515\n",
            "train loss:1.2190571606964449\n",
            "train loss:1.1036424612059528\n",
            "train loss:1.2515826220516741\n",
            "train loss:1.1073513490293663\n",
            "train loss:1.4246195146707956\n",
            "train loss:0.9762623238479422\n",
            "train loss:1.3459901395668856\n",
            "train loss:1.2662573613815902\n",
            "train loss:1.2631369785119926\n",
            "train loss:1.1830502407609007\n",
            "train loss:1.1712138611026648\n",
            "train loss:1.277852772770424\n",
            "train loss:1.013242148398552\n",
            "train loss:1.3979383579175337\n",
            "train loss:1.2877817109598717\n",
            "train loss:1.213297198259596\n",
            "train loss:1.0309568691637492\n",
            "train loss:1.0900685599835673\n",
            "train loss:1.162164665015177\n",
            "train loss:1.2018862134394752\n",
            "train loss:1.0793720500763446\n",
            "train loss:1.24491073446369\n",
            "train loss:1.2532571652659812\n",
            "train loss:1.1652018026563054\n",
            "train loss:1.2014663623539554\n",
            "train loss:1.33524524428905\n",
            "train loss:1.134714497596957\n",
            "train loss:1.10368454310282\n",
            "train loss:0.9368436253175326\n",
            "train loss:1.1742874431276007\n",
            "train loss:1.1974682011193631\n",
            "train loss:1.2137234107838697\n",
            "train loss:1.3463231220143268\n",
            "train loss:1.2066414557153153\n",
            "train loss:1.2119820241491122\n",
            "train loss:1.1432972704272812\n",
            "train loss:1.0573261333200206\n",
            "train loss:1.105930435984719\n",
            "train loss:1.2169880663346544\n",
            "train loss:1.1923665092115743\n",
            "train loss:1.0865034697257159\n",
            "train loss:1.2428219434651053\n",
            "train loss:1.1736998524369897\n",
            "train loss:1.159987415440778\n",
            "train loss:1.2096946784160394\n",
            "train loss:1.151963273657529\n",
            "train loss:1.0472147045691733\n",
            "train loss:1.1445922239553372\n",
            "train loss:0.8859531299179433\n",
            "train loss:1.1868622420750397\n",
            "train loss:1.0447028536410834\n",
            "train loss:1.1574274298186578\n",
            "train loss:0.9629963889433438\n",
            "train loss:1.212209303847764\n",
            "train loss:1.0746066077447025\n",
            "train loss:0.9832242777461943\n",
            "train loss:1.1243832546222579\n",
            "train loss:1.1730443061916036\n",
            "train loss:1.1342742378930395\n",
            "train loss:1.046501013397205\n",
            "train loss:1.1348723730007282\n",
            "train loss:1.197578403700849\n",
            "train loss:1.2070565003965854\n",
            "train loss:1.0646299608033112\n",
            "train loss:1.1326172297963848\n",
            "train loss:1.0244171409340546\n",
            "train loss:1.1275681595970495\n",
            "train loss:1.2003071815192774\n",
            "train loss:0.9426515574091056\n",
            "train loss:1.1654106445896588\n",
            "train loss:1.1388812834870858\n",
            "train loss:1.1001289767497349\n",
            "train loss:1.0372445877276808\n",
            "train loss:1.1730046427351943\n",
            "train loss:1.1437706528078915\n",
            "train loss:1.085227204106034\n",
            "train loss:1.0617033315127737\n",
            "train loss:1.3091119991902387\n",
            "train loss:1.153818983698185\n",
            "train loss:1.1402354189962067\n",
            "train loss:1.1954152344989684\n",
            "train loss:1.0591416376525415\n",
            "train loss:1.1392997356176493\n",
            "train loss:1.0787263105994476\n",
            "train loss:1.0477235461197913\n",
            "train loss:1.2505853209898294\n",
            "train loss:1.002567843054356\n",
            "train loss:1.0391617182979846\n",
            "train loss:0.9896000881858182\n",
            "train loss:1.0411564470699357\n",
            "train loss:1.0516103120844082\n",
            "train loss:1.2456372234019841\n",
            "train loss:1.0843458748384642\n",
            "train loss:1.2063792837204903\n",
            "train loss:1.2407481830175784\n",
            "train loss:1.3330269493697355\n",
            "train loss:1.0627286379064693\n",
            "train loss:1.0638815956794314\n",
            "train loss:1.183180191168024\n",
            "train loss:1.0539623454553273\n",
            "train loss:1.093445147382905\n",
            "train loss:0.9613336381339053\n",
            "train loss:1.0541635002365826\n",
            "train loss:1.1876546305640934\n",
            "train loss:1.0148125522753124\n",
            "train loss:1.0022184904468827\n",
            "train loss:1.0217980066131125\n",
            "train loss:1.1399724858651739\n",
            "train loss:0.8497744946201159\n",
            "train loss:1.1661140012233229\n",
            "train loss:1.1028841696475895\n",
            "train loss:1.073553180514388\n",
            "train loss:1.2744276931098162\n",
            "train loss:1.1500688134018986\n",
            "train loss:1.0098003074887516\n",
            "train loss:1.1727652594234044\n",
            "train loss:0.9856667157603285\n",
            "train loss:1.1842387935643621\n",
            "train loss:1.252713758293414\n",
            "train loss:1.0377816833275204\n",
            "train loss:1.1356903530322326\n",
            "train loss:1.0972528489592355\n",
            "train loss:1.0575753399845909\n",
            "train loss:1.2889619538205699\n",
            "train loss:1.1295675212526006\n",
            "train loss:1.159799966367271\n",
            "train loss:1.1539123779488711\n",
            "train loss:1.0706486601831449\n",
            "train loss:1.0994869254494763\n",
            "train loss:0.945483156591029\n",
            "train loss:1.151810870020799\n",
            "train loss:1.0793342188188944\n",
            "train loss:1.173594478634956\n",
            "train loss:0.9094448877548591\n",
            "train loss:0.9354028027183857\n",
            "train loss:1.123891202049471\n",
            "train loss:1.0243401403268135\n",
            "train loss:1.2473865952994987\n",
            "train loss:1.0193836548961759\n",
            "train loss:1.2384116472107747\n",
            "train loss:0.9350340233969265\n",
            "train loss:1.0475047487254634\n",
            "train loss:1.264231178380295\n",
            "train loss:1.3736252091083023\n",
            "train loss:1.1729382640275587\n",
            "train loss:1.3009446873813346\n",
            "train loss:1.1685199281285528\n",
            "train loss:0.9972911415498951\n",
            "train loss:0.9514734699180561\n",
            "train loss:1.1360932539328592\n",
            "train loss:1.0557126604630738\n",
            "train loss:1.0882176171356717\n",
            "train loss:1.0800271838740252\n",
            "train loss:1.1254322231382323\n",
            "train loss:0.859300619792362\n",
            "train loss:1.1482417136383187\n",
            "train loss:1.06754282886258\n",
            "train loss:1.1182397239384916\n",
            "train loss:0.9722697745236398\n",
            "train loss:1.178001110224161\n",
            "train loss:1.1279297141070763\n",
            "train loss:1.1193784381190868\n",
            "train loss:1.0038442692724516\n",
            "train loss:1.1364368919357535\n",
            "train loss:1.1612036256678713\n",
            "train loss:1.1373592888712893\n",
            "train loss:1.2127993234441932\n",
            "train loss:1.0919182494219686\n",
            "train loss:1.0626315740956105\n",
            "train loss:1.0085357572127454\n",
            "train loss:1.172232620827541\n",
            "train loss:1.0880304773393565\n",
            "train loss:1.1101656005003402\n",
            "train loss:0.9409308579971898\n",
            "train loss:1.0175360903924016\n",
            "train loss:0.8838837960860695\n",
            "train loss:1.0362256430999615\n",
            "train loss:1.152760251618577\n",
            "train loss:1.1243647350948158\n",
            "train loss:1.2418304279425492\n",
            "train loss:0.9943123852327385\n",
            "train loss:1.04582114089259\n",
            "train loss:0.9781370171846046\n",
            "train loss:1.20865043020071\n",
            "train loss:1.3266248939725471\n",
            "train loss:1.0817817462586743\n",
            "train loss:0.9746893636240024\n",
            "train loss:0.9918939782517675\n",
            "train loss:1.0974460869510172\n",
            "train loss:1.2066574321269987\n",
            "train loss:0.9529077962748244\n",
            "train loss:1.1163803914886572\n",
            "train loss:0.9950744246456867\n",
            "train loss:1.1144473474157224\n",
            "train loss:1.0431865404345964\n",
            "train loss:1.1759617298534373\n",
            "train loss:1.2619620814829227\n",
            "train loss:1.0444433616641255\n",
            "train loss:1.0975129304648306\n",
            "train loss:1.0503726245041047\n",
            "train loss:1.0179558965531161\n",
            "train loss:1.2141894122467356\n",
            "train loss:0.9609109547256586\n",
            "train loss:1.1412467030135671\n",
            "train loss:0.9790472041597882\n",
            "train loss:1.1266333230955237\n",
            "train loss:1.0602587676198336\n",
            "train loss:1.2981009867629518\n",
            "train loss:1.136494755109096\n",
            "train loss:1.0262728599108777\n",
            "train loss:0.7986528503909104\n",
            "train loss:1.0261923045910628\n",
            "train loss:1.322987289565472\n",
            "train loss:1.0837674290547885\n",
            "train loss:1.136356567366547\n",
            "train loss:1.1145123620799335\n",
            "train loss:1.03187333313464\n",
            "train loss:1.0553306729362921\n",
            "train loss:1.0236385088426048\n",
            "train loss:1.1981625258599133\n",
            "train loss:1.184243316328486\n",
            "train loss:0.9557221319885031\n",
            "train loss:1.3086671208498273\n",
            "train loss:1.1864362441781529\n",
            "train loss:1.2256045154480812\n",
            "train loss:1.0312349112805208\n",
            "train loss:1.0083874196395712\n",
            "train loss:1.1495499565725504\n",
            "train loss:1.1618732164159897\n",
            "train loss:1.1819948986295217\n",
            "train loss:1.1996782976909852\n",
            "train loss:0.9790875621537052\n",
            "train loss:0.9417178538146495\n",
            "train loss:0.8715733241482525\n",
            "train loss:1.2130221204278144\n",
            "train loss:1.122328416117047\n",
            "train loss:1.0438104695275785\n",
            "=== epoch:2, train acc:0.981, test acc:0.978 ===\n",
            "train loss:1.192976261418059\n",
            "train loss:1.0887591104980647\n",
            "train loss:0.8363865177145872\n",
            "train loss:1.0315561233203092\n",
            "train loss:1.1674887458531404\n",
            "train loss:1.1006692409146495\n",
            "train loss:0.9951264271527193\n",
            "train loss:1.0284166344067958\n",
            "train loss:1.1821029895538002\n",
            "train loss:1.1502216553315088\n",
            "train loss:1.31637400777593\n",
            "train loss:1.0975392134174358\n",
            "train loss:1.0525155774627195\n",
            "train loss:1.000245701155887\n",
            "train loss:1.1288913056868048\n",
            "train loss:1.1569292499837065\n",
            "train loss:1.1566410488416385\n",
            "train loss:1.0761920753124408\n",
            "train loss:0.9129435052892684\n",
            "train loss:1.017551855423898\n",
            "train loss:0.9812858293917435\n",
            "train loss:1.129018162064877\n",
            "train loss:1.018365589866409\n",
            "train loss:1.103424232068842\n",
            "train loss:1.0952791063348555\n",
            "train loss:0.9862698560367653\n",
            "train loss:1.0786903095698224\n",
            "train loss:0.8747745006678065\n",
            "train loss:0.9345922992745926\n",
            "train loss:0.9501594037975868\n",
            "train loss:1.1956803647763363\n",
            "train loss:1.003045976852914\n",
            "train loss:1.381264138912425\n",
            "train loss:1.0510042953100984\n",
            "train loss:0.9266048751612764\n",
            "train loss:0.8637467841216535\n",
            "train loss:1.0724694531045034\n",
            "train loss:1.101308917056171\n",
            "train loss:1.0488243955675671\n",
            "train loss:1.0320290411119073\n",
            "train loss:0.9983672982238074\n",
            "train loss:1.1713494716673853\n",
            "train loss:0.9800951625814568\n",
            "train loss:1.084697678002343\n",
            "train loss:1.004521089750335\n",
            "train loss:0.918056150293059\n",
            "train loss:0.9475569857997228\n",
            "train loss:1.0531850499372806\n",
            "train loss:1.251954237299197\n",
            "train loss:0.9988376048226201\n",
            "train loss:1.1574052228244869\n",
            "train loss:1.095631706881027\n",
            "train loss:1.2466720305934356\n",
            "train loss:1.0779929495263518\n",
            "train loss:0.9756352321201551\n",
            "train loss:0.9217939674429556\n",
            "train loss:0.9689658973097747\n",
            "train loss:0.8550282287158006\n",
            "train loss:1.1387345817078833\n",
            "train loss:0.9742438557863342\n",
            "train loss:1.0484177291719867\n",
            "train loss:0.9598023969043922\n",
            "train loss:1.2279144016770096\n",
            "train loss:1.125994074498266\n",
            "train loss:1.2308199974047787\n",
            "train loss:1.1004086736309455\n",
            "train loss:0.9155697135785283\n",
            "train loss:1.0853006950835589\n",
            "train loss:1.062417694757696\n",
            "train loss:1.2135241352995596\n",
            "train loss:1.006619632049986\n",
            "train loss:1.0433184063898324\n",
            "train loss:0.9433241397581181\n",
            "train loss:0.9946069037406002\n",
            "train loss:1.2742287659157965\n",
            "train loss:1.0323622402748476\n",
            "train loss:1.1559718022363945\n",
            "train loss:0.8508296424851952\n",
            "train loss:1.0204169611979699\n",
            "train loss:1.2095128242157136\n",
            "train loss:1.106145113585961\n",
            "train loss:0.9684681050875739\n",
            "train loss:1.1288198965130956\n",
            "train loss:1.0263958483436257\n",
            "train loss:0.9956495356752986\n",
            "train loss:1.1927893660914708\n",
            "train loss:1.0113989905058494\n",
            "train loss:1.170131364574911\n",
            "train loss:1.0959913880909788\n",
            "train loss:0.8833139855034341\n",
            "train loss:1.0313910277634073\n",
            "train loss:1.0358393601007896\n",
            "train loss:1.0503681155948008\n",
            "train loss:1.0758329893312057\n",
            "train loss:1.1597117226675673\n",
            "train loss:1.0871315522863336\n",
            "train loss:1.158650993212057\n",
            "train loss:1.1394168952694903\n",
            "train loss:1.2735955631352325\n",
            "train loss:1.073735077008298\n",
            "train loss:1.0809175378300715\n",
            "train loss:1.0231315279403894\n",
            "train loss:1.1217953262913496\n",
            "train loss:0.9139215156466683\n",
            "train loss:1.107318669643484\n",
            "train loss:1.1010257915566688\n",
            "train loss:0.9599074168214762\n",
            "train loss:1.0346356567906412\n",
            "train loss:0.9867992599235376\n",
            "train loss:1.0432248942479498\n",
            "train loss:1.0437242341567845\n",
            "train loss:1.0057284031909373\n",
            "train loss:1.2574371578202124\n",
            "train loss:1.0685444109649793\n",
            "train loss:1.2778003316991684\n",
            "train loss:0.9897179829126931\n",
            "train loss:1.064576463040275\n",
            "train loss:1.0706230279996283\n",
            "train loss:1.157325457954425\n",
            "train loss:1.177523148790755\n",
            "train loss:1.1049759567767066\n",
            "train loss:1.1120722453080782\n",
            "train loss:1.0326304536445756\n",
            "train loss:1.0557833580443607\n",
            "train loss:1.0893336347597455\n",
            "train loss:1.121790675739281\n",
            "train loss:1.2360492131223904\n",
            "train loss:1.1894861849586853\n",
            "train loss:1.1610858080368374\n",
            "train loss:1.0444624432285134\n",
            "train loss:1.1013026742383778\n",
            "train loss:0.8492117916675066\n",
            "train loss:1.1438010721478367\n",
            "train loss:1.019943812463076\n",
            "train loss:0.9101748987737278\n",
            "train loss:1.0644938510433368\n",
            "train loss:1.223893807115453\n",
            "train loss:0.9956065266799602\n",
            "train loss:1.1209600859266795\n",
            "train loss:0.8349321408684319\n",
            "train loss:1.1874777529754947\n",
            "train loss:1.1046584749509538\n",
            "train loss:1.0658980553152617\n",
            "train loss:1.0802951234772256\n",
            "train loss:1.3019980567492684\n",
            "train loss:1.2373564784514237\n",
            "train loss:0.9119799255701829\n",
            "train loss:0.8720281717275675\n",
            "train loss:0.9106635696866359\n",
            "train loss:0.9582085573573486\n",
            "train loss:0.9362075060613222\n",
            "train loss:1.011248368252015\n",
            "train loss:1.1371910939316363\n",
            "train loss:1.1675653863775097\n",
            "train loss:1.1916168724087297\n",
            "train loss:1.2031663761813995\n",
            "train loss:1.0473799060743678\n",
            "train loss:0.916665988584664\n",
            "train loss:1.2042857179951048\n",
            "train loss:1.0594509077964738\n",
            "train loss:1.1245148429171652\n",
            "train loss:1.0854165406825769\n",
            "train loss:0.996624048389932\n",
            "train loss:1.0899425891937822\n",
            "train loss:0.9756406097707633\n",
            "train loss:0.9996798720275879\n",
            "train loss:1.0246464727508706\n",
            "train loss:0.9602568928241572\n",
            "train loss:1.0208565598424468\n",
            "train loss:0.9891105216138942\n",
            "train loss:1.0170228221548527\n",
            "train loss:1.1858250073933778\n",
            "train loss:1.1274883279825918\n",
            "train loss:1.1682452283943252\n",
            "train loss:1.0298684167549634\n",
            "train loss:1.1902616166200912\n",
            "train loss:0.9971822850511548\n",
            "train loss:1.1152799441389927\n",
            "train loss:1.043619446460189\n",
            "train loss:1.0438473630167215\n",
            "train loss:1.1146355955675988\n",
            "train loss:1.1074938453281278\n",
            "train loss:1.0856393706745628\n",
            "train loss:1.0603209845451438\n",
            "train loss:1.1070225841160026\n",
            "train loss:1.0021259723075062\n",
            "train loss:0.9879762387658454\n",
            "train loss:1.1850652185384067\n",
            "train loss:1.0386630907871923\n",
            "train loss:0.8730061327676585\n",
            "train loss:1.0456158095600332\n",
            "train loss:1.0850751366839495\n",
            "train loss:0.9295929353981033\n",
            "train loss:0.983138654486099\n",
            "train loss:1.1265536716809534\n",
            "train loss:1.1419951258044696\n",
            "train loss:1.0589723438431395\n",
            "train loss:1.0679914995198336\n",
            "train loss:1.0484241019609808\n",
            "train loss:0.9852425262988489\n",
            "train loss:0.987842470001494\n",
            "train loss:0.9752263128133717\n",
            "train loss:1.0304978971662884\n",
            "train loss:1.0650324856612614\n",
            "train loss:0.9648610696051372\n",
            "train loss:0.9202121031177729\n",
            "train loss:1.0463192870729703\n",
            "train loss:0.9897877311923005\n",
            "train loss:1.0231099547254778\n",
            "train loss:1.1068630306939995\n",
            "train loss:1.0181379158336161\n",
            "train loss:0.9478221314243068\n",
            "train loss:1.0381481535156623\n",
            "train loss:1.087428481758035\n",
            "train loss:1.0822866100546271\n",
            "train loss:0.8405800542688646\n",
            "train loss:1.0460330928737716\n",
            "train loss:1.009976206928665\n",
            "train loss:1.10130247152259\n",
            "train loss:0.952332589521385\n",
            "train loss:0.9453549251736574\n",
            "train loss:1.0561566872889436\n",
            "train loss:0.9926213641964022\n",
            "train loss:1.0538762414893\n",
            "train loss:1.0356021665013289\n",
            "train loss:1.28755907816634\n",
            "train loss:1.1148220734335421\n",
            "train loss:1.0717149794049197\n",
            "train loss:1.0677905391978377\n",
            "train loss:0.9512148676531658\n",
            "train loss:0.9247537216199482\n",
            "train loss:1.059772644021708\n",
            "train loss:0.9605743608114365\n",
            "train loss:0.9813313069925584\n",
            "train loss:1.0548583229195432\n",
            "train loss:0.9877761967893594\n",
            "train loss:0.9573125934967712\n",
            "train loss:0.9945034710042135\n",
            "train loss:1.1457752962115542\n",
            "train loss:1.0478923367217436\n",
            "train loss:1.2418505590758393\n",
            "train loss:1.0159995944231277\n",
            "train loss:0.9208320697436634\n",
            "train loss:1.009049332024189\n",
            "train loss:0.9955219409046778\n",
            "train loss:1.0919557210074011\n",
            "train loss:0.9391311508205804\n",
            "train loss:1.163001719052786\n",
            "train loss:0.8594013382293676\n",
            "train loss:1.0120609760037773\n",
            "train loss:0.9584860456334229\n",
            "train loss:1.0479251814461903\n",
            "train loss:1.170922058058367\n",
            "train loss:0.9769605572215457\n",
            "train loss:1.0093482497631012\n",
            "train loss:1.145113135151646\n",
            "train loss:1.1604395953972504\n",
            "train loss:1.0358156225432926\n",
            "train loss:1.167107412141246\n",
            "train loss:1.0289140625178304\n",
            "train loss:1.112551005860694\n",
            "train loss:1.0454953761826988\n",
            "train loss:1.0687056223756424\n",
            "train loss:1.0242867582926924\n",
            "train loss:1.008679202858903\n",
            "train loss:1.164447478634416\n",
            "train loss:0.9642859818355786\n",
            "train loss:0.9631193463744978\n",
            "train loss:1.0628596063439064\n",
            "train loss:1.1092557141672903\n",
            "train loss:0.921736180700181\n",
            "train loss:1.2268945556592148\n",
            "train loss:1.0350911553115096\n",
            "train loss:1.046775127698179\n",
            "train loss:0.9193080052930556\n",
            "train loss:1.151305819234222\n",
            "train loss:0.9309286858517553\n",
            "train loss:1.1098839571832229\n",
            "train loss:1.0541131881291428\n",
            "train loss:1.1308232782942573\n",
            "train loss:1.0795746426078094\n",
            "train loss:0.971698008580675\n",
            "train loss:0.9786501425635475\n",
            "train loss:1.1136461689509563\n",
            "train loss:1.1348347715512659\n",
            "train loss:1.0439395749024603\n",
            "train loss:0.9099384644251639\n",
            "train loss:1.096934107313003\n",
            "train loss:1.0265663988919802\n",
            "train loss:1.1827098932438995\n",
            "train loss:1.0012346222473587\n",
            "train loss:1.1277021967008325\n",
            "train loss:1.1361105237206122\n",
            "train loss:1.1307556843613198\n",
            "train loss:1.0576466420269137\n",
            "train loss:1.1002313087788964\n",
            "train loss:1.0632543723768975\n",
            "train loss:1.0243853334577573\n",
            "train loss:1.054661595118367\n",
            "train loss:0.9514724142343408\n",
            "train loss:1.2992126932478316\n",
            "train loss:1.0664616831701794\n",
            "train loss:0.9534134821843319\n",
            "train loss:1.0043099125237185\n",
            "train loss:1.0014606053167954\n",
            "train loss:1.064877810858019\n",
            "train loss:1.11082186521711\n",
            "train loss:0.9595366075807761\n",
            "train loss:0.9375358349241693\n",
            "train loss:1.116584203023516\n",
            "train loss:1.1567273865284047\n",
            "train loss:1.053128754481281\n",
            "train loss:0.9726100531706363\n",
            "train loss:0.9596304720087906\n",
            "train loss:1.1181537512609683\n",
            "train loss:0.9413263510233735\n",
            "train loss:1.1223831348842688\n",
            "train loss:1.0954778899071376\n",
            "train loss:1.0252179771306926\n",
            "train loss:1.011227290390393\n",
            "train loss:0.9011164755418736\n",
            "train loss:0.87041310167012\n",
            "train loss:1.0917767525589743\n",
            "train loss:0.9139598083865836\n",
            "train loss:0.9510528524155417\n",
            "train loss:0.9912122580361795\n",
            "train loss:0.8150817341562919\n",
            "train loss:0.9780687549738427\n",
            "train loss:0.9083784327391102\n",
            "train loss:0.9601728920588983\n",
            "train loss:0.9535493883149584\n",
            "train loss:1.0084797454789973\n",
            "train loss:1.0322500745385481\n",
            "train loss:0.8861854460903281\n",
            "train loss:1.0276069092227784\n",
            "train loss:1.011482595133477\n",
            "train loss:0.8908394968421489\n",
            "train loss:0.9445076544480078\n",
            "train loss:1.1946644708218443\n",
            "train loss:1.0031875594426218\n",
            "train loss:0.8597616035681775\n",
            "train loss:1.036427795248398\n",
            "train loss:1.1137877658487072\n",
            "train loss:0.899401738823686\n",
            "train loss:1.0675809197279205\n",
            "train loss:1.0734327944386084\n",
            "train loss:0.9343395576628493\n",
            "train loss:1.1094484056196188\n",
            "train loss:0.8702211793858176\n",
            "train loss:1.0760333348318716\n",
            "train loss:0.9131897467866953\n",
            "train loss:1.222032159051758\n",
            "train loss:0.9276351388653906\n",
            "train loss:1.1176336780116554\n",
            "train loss:1.0983047722979906\n",
            "train loss:1.1679706105398002\n",
            "train loss:1.0818419516272397\n",
            "train loss:1.0012291008651537\n",
            "train loss:0.9909396655350031\n",
            "train loss:0.8711890603488288\n",
            "train loss:1.018999547107809\n",
            "train loss:0.9528694249953789\n",
            "train loss:0.9815245413065803\n",
            "train loss:1.022041173925974\n",
            "train loss:0.901596679233519\n",
            "train loss:0.9270937797737426\n",
            "train loss:1.0559587934410966\n",
            "train loss:1.0632829966372905\n",
            "train loss:1.0536663298005415\n",
            "train loss:1.0726145687361983\n",
            "train loss:1.1442212532018372\n",
            "train loss:0.9685888648502099\n",
            "train loss:1.1936593601905325\n",
            "train loss:0.9402853799339228\n",
            "train loss:1.0735222990436353\n",
            "train loss:1.1178902669319055\n",
            "train loss:0.9007272084591611\n",
            "train loss:1.0976773786261251\n",
            "train loss:0.9379793105644469\n",
            "train loss:0.9637649214879235\n",
            "train loss:1.0225374387506725\n",
            "train loss:1.146570100480041\n",
            "train loss:0.9210535240697036\n",
            "train loss:1.1615808057934165\n",
            "train loss:0.9800277356804198\n",
            "train loss:1.0107356832553753\n",
            "train loss:1.0882208091680503\n",
            "train loss:0.8860563554993797\n",
            "train loss:1.2954773832337154\n",
            "train loss:1.059161330210878\n",
            "train loss:0.9484535588408914\n",
            "train loss:0.9787388844834968\n",
            "train loss:1.0483340847242117\n",
            "train loss:1.029936153486952\n",
            "train loss:1.0942499695479102\n",
            "train loss:0.9273784965257217\n",
            "train loss:0.9784568161421382\n",
            "train loss:1.0348376794902185\n",
            "train loss:1.002698547336421\n",
            "train loss:1.1343779188378424\n",
            "train loss:0.905515212651864\n",
            "train loss:0.8999246425019831\n",
            "train loss:1.0583918886664\n",
            "train loss:0.9447405431066569\n",
            "train loss:1.100056851387249\n",
            "train loss:1.232867582395518\n",
            "train loss:1.078060304399179\n",
            "train loss:1.1729584234338273\n",
            "train loss:0.9697897076604076\n",
            "train loss:0.9048739090162756\n",
            "train loss:0.9893023689517577\n",
            "train loss:1.0245390598060198\n",
            "train loss:0.9388884917166661\n",
            "train loss:0.9251378273488067\n",
            "train loss:1.0957486228423532\n",
            "train loss:1.0792658609523405\n",
            "train loss:0.9482457997577893\n",
            "train loss:1.0258017166679014\n",
            "train loss:0.9795093495436111\n",
            "train loss:1.1190513373340791\n",
            "train loss:1.202038915201458\n",
            "train loss:1.1163738472388673\n",
            "train loss:1.1654545610022573\n",
            "train loss:0.8930558675859028\n",
            "train loss:1.0286427218162444\n",
            "train loss:1.243473402786954\n",
            "train loss:1.0015707438207906\n",
            "train loss:1.0980933403561834\n",
            "train loss:0.8451017539239725\n",
            "train loss:0.8930190564619487\n",
            "train loss:0.9870518256902533\n",
            "train loss:1.0372421389871012\n",
            "train loss:0.904576413029553\n",
            "train loss:1.011872976541461\n",
            "train loss:0.9792458938443258\n",
            "train loss:0.9422890332641317\n",
            "train loss:0.9690390448983587\n",
            "train loss:0.8294732543777041\n",
            "train loss:1.0630272446691116\n",
            "train loss:0.9838672653104834\n",
            "train loss:0.9260995111436061\n",
            "train loss:0.9794485749589739\n",
            "train loss:1.0606307250826945\n",
            "train loss:1.0837082221346837\n",
            "train loss:1.0688515357228427\n",
            "train loss:0.8472174548244483\n",
            "train loss:1.0402248571612986\n",
            "train loss:0.9152445547237923\n",
            "train loss:1.0703981962620157\n",
            "train loss:1.112078164414516\n",
            "train loss:1.1488053300217238\n",
            "train loss:1.0402390693897654\n",
            "train loss:1.1477166623557855\n",
            "train loss:0.8250079250133371\n",
            "train loss:0.7562940520665056\n",
            "train loss:1.089466797336871\n",
            "train loss:1.0141304043545307\n",
            "train loss:1.1681680000050005\n",
            "train loss:0.9287446650987681\n",
            "train loss:1.134315726237261\n",
            "train loss:1.021967548797216\n",
            "train loss:0.9681486424563838\n",
            "train loss:1.0540235202541584\n",
            "train loss:1.0476007557890328\n",
            "train loss:1.1022508429495206\n",
            "train loss:1.05120329176971\n",
            "train loss:1.178708794781148\n",
            "train loss:0.8630992187937564\n",
            "train loss:1.0259592405126667\n",
            "train loss:1.1644824965140226\n",
            "train loss:1.0487419299670866\n",
            "train loss:1.064406409344458\n",
            "train loss:1.1273246166149822\n",
            "train loss:0.8915223404672968\n",
            "train loss:1.174492885139069\n",
            "train loss:0.8687688426562004\n",
            "train loss:1.229826218946617\n",
            "train loss:0.9740737291311232\n",
            "train loss:0.8929518830890077\n",
            "train loss:1.0629338639014156\n",
            "train loss:0.9802824884688459\n",
            "train loss:1.0719806065185191\n",
            "train loss:0.9139951108976012\n",
            "train loss:1.2132716995406527\n",
            "train loss:1.0542032448277232\n",
            "train loss:0.9718779068979236\n",
            "train loss:0.95713126753589\n",
            "train loss:0.9063932472927412\n",
            "train loss:1.0717333823034396\n",
            "train loss:0.887573409651673\n",
            "train loss:0.9326980061718692\n",
            "train loss:0.9107442932254104\n",
            "train loss:1.0518521038361854\n",
            "train loss:0.8813703148187055\n",
            "train loss:1.0602832025591384\n",
            "train loss:0.970484320376263\n",
            "train loss:1.0966888432024782\n",
            "train loss:1.015892205568502\n",
            "train loss:1.0920184390879681\n",
            "train loss:1.0045662021390016\n",
            "train loss:0.9335626376307201\n",
            "train loss:0.9588720642653642\n",
            "train loss:0.86399630817894\n",
            "train loss:1.2101305386132706\n",
            "train loss:0.9704328690172646\n",
            "train loss:1.180091198949831\n",
            "train loss:0.9458957036950274\n",
            "train loss:1.1037020474572459\n",
            "train loss:0.958706325516301\n",
            "train loss:1.0426424613784249\n",
            "train loss:1.0124532733349791\n",
            "train loss:0.8258362688320424\n",
            "train loss:0.9922615854903686\n",
            "train loss:1.0468401843304052\n",
            "train loss:0.9181925362782988\n",
            "train loss:1.0699910186651962\n",
            "train loss:0.942729219583123\n",
            "train loss:1.0523702136663105\n",
            "train loss:1.0786657364207002\n",
            "train loss:0.8650143948304087\n",
            "train loss:0.9043384360561739\n",
            "train loss:1.0706825725592422\n",
            "train loss:0.9666238837518065\n",
            "train loss:0.9518217251826667\n",
            "train loss:1.2165574066978664\n",
            "train loss:1.018144609723975\n",
            "train loss:1.105147472395382\n",
            "train loss:1.177959805021803\n",
            "train loss:1.0025064931727252\n",
            "train loss:1.0491976365336932\n",
            "train loss:1.0554299486572776\n",
            "train loss:1.052215914607727\n",
            "train loss:1.0964611015676435\n",
            "train loss:1.0680461049144434\n",
            "train loss:1.2048712016446403\n",
            "train loss:1.153878348917822\n",
            "train loss:0.9194196897388071\n",
            "train loss:1.0621268281889407\n",
            "train loss:1.0553394807954006\n",
            "train loss:1.0683633494591707\n",
            "train loss:1.1508515517614537\n",
            "train loss:0.7830733990939326\n",
            "train loss:0.9686482269708692\n",
            "train loss:0.9943817323941551\n",
            "train loss:0.9800255225592633\n",
            "train loss:0.9261290387735506\n",
            "train loss:0.8082524551042605\n",
            "train loss:1.1119072392851246\n",
            "train loss:1.049336934635335\n",
            "train loss:1.0009216543321704\n",
            "train loss:1.047339125460496\n",
            "train loss:1.0098916090638865\n",
            "train loss:0.989124538109906\n",
            "train loss:0.8927049068831779\n",
            "train loss:0.7520662785097305\n",
            "train loss:1.1389123366655685\n",
            "train loss:0.8747739203681705\n",
            "train loss:0.8705494785026066\n",
            "train loss:1.0051769491150473\n",
            "train loss:0.818705391096544\n",
            "train loss:0.7795583179827676\n",
            "train loss:0.8352896815099171\n",
            "train loss:0.9895607405869759\n",
            "train loss:1.0206727522074106\n",
            "train loss:1.0998061697091532\n",
            "train loss:1.1247837050895182\n",
            "train loss:0.83444546660454\n",
            "train loss:0.9272094306157251\n",
            "train loss:1.228727711370947\n",
            "train loss:0.9008242598660491\n",
            "train loss:1.1424931714171758\n",
            "train loss:0.9697055265575826\n",
            "train loss:0.9222147369528575\n",
            "train loss:1.0210112189522207\n",
            "train loss:0.8520696672594948\n",
            "train loss:1.2316912202014416\n",
            "train loss:1.0225438948684538\n",
            "train loss:0.891643078507404\n",
            "train loss:1.1567979250088944\n",
            "train loss:0.9437466538332266\n",
            "train loss:1.0228912072920904\n",
            "train loss:1.100435807423794\n",
            "train loss:0.9859261081487672\n",
            "train loss:0.9865268590243298\n",
            "train loss:0.8646369450081737\n",
            "train loss:0.9486508588172491\n",
            "train loss:0.9680309341033427\n",
            "train loss:1.2039690385461428\n",
            "train loss:0.8854831133463638\n",
            "train loss:0.9762950461250504\n",
            "train loss:1.2264445465854983\n",
            "train loss:0.9996418948952172\n",
            "train loss:1.092482238169434\n",
            "train loss:1.0868293690240787\n",
            "train loss:0.9325860372402913\n",
            "train loss:0.9966276840338221\n",
            "train loss:0.9183865981928184\n",
            "train loss:0.9769698741444418\n",
            "train loss:1.0416928742125804\n",
            "train loss:0.8716026298495738\n",
            "=== epoch:3, train acc:0.982, test acc:0.985 ===\n",
            "train loss:0.926671268822251\n",
            "train loss:1.052380825368032\n",
            "train loss:1.0974043607544843\n",
            "train loss:1.0237520074349724\n",
            "train loss:1.069316875673991\n",
            "train loss:1.0329956884380203\n",
            "train loss:1.0256343441352624\n",
            "train loss:0.8680366834192401\n",
            "train loss:1.0486760310385572\n",
            "train loss:1.1093466602354782\n",
            "train loss:1.2376606260911522\n",
            "train loss:0.9463251958962153\n",
            "train loss:1.1095373340440238\n",
            "train loss:0.9352135441723641\n",
            "train loss:0.996247462531084\n",
            "train loss:1.0833592512952104\n",
            "train loss:0.8988713500141731\n",
            "train loss:1.0177727308059974\n",
            "train loss:1.0214290647173385\n",
            "train loss:0.9895511087040615\n",
            "train loss:1.0024246685326434\n",
            "train loss:1.1343431904987862\n",
            "train loss:1.0882361146997008\n",
            "train loss:1.1244008847272247\n",
            "train loss:1.0381749322890321\n",
            "train loss:1.247125451534082\n",
            "train loss:0.8818511109896727\n",
            "train loss:0.9891353996714165\n",
            "train loss:1.167921680446281\n",
            "train loss:0.918275232314467\n",
            "train loss:1.0971291422838239\n",
            "train loss:1.0260742424064702\n",
            "train loss:0.867899515937538\n",
            "train loss:1.0758898357256366\n",
            "train loss:1.0885585310005341\n",
            "train loss:1.1511332028020405\n",
            "train loss:1.054791682228476\n",
            "train loss:0.8210067516125247\n",
            "train loss:1.0070347111935574\n",
            "train loss:0.9389514839974596\n",
            "train loss:1.1019823484370574\n",
            "train loss:0.6878666765611275\n",
            "train loss:0.8684626024135875\n",
            "train loss:1.0224868321064593\n",
            "train loss:0.9810952163578193\n",
            "train loss:0.8183723361531466\n",
            "train loss:0.9648725761546717\n",
            "train loss:1.0250489092191\n",
            "train loss:0.9791676208467353\n",
            "train loss:0.8903425074393406\n",
            "train loss:0.8385516757635761\n",
            "train loss:1.098226991662021\n",
            "train loss:0.968030300486171\n",
            "train loss:1.0632241205621176\n",
            "train loss:0.9349080142593038\n",
            "train loss:1.0277202479371301\n",
            "train loss:0.9412123773893145\n",
            "train loss:1.162850767455722\n",
            "train loss:0.9389144232578885\n",
            "train loss:1.0457335936757834\n",
            "train loss:1.0614136885239005\n",
            "train loss:1.1503664249428645\n",
            "train loss:0.9367701622347476\n",
            "train loss:0.9943521417204352\n",
            "train loss:1.106943826746338\n",
            "train loss:1.1000749899622981\n",
            "train loss:0.890378771256419\n",
            "train loss:1.0777109633273705\n",
            "train loss:0.9665441900371151\n",
            "train loss:0.9209623995869451\n",
            "train loss:0.9828859687974943\n",
            "train loss:1.1093929314337008\n",
            "train loss:0.9419631612117905\n",
            "train loss:0.9533758077470409\n",
            "train loss:1.0477266488203207\n",
            "train loss:1.0504754641068004\n",
            "train loss:0.9417928666165338\n",
            "train loss:1.0030770268084164\n",
            "train loss:0.8644337938554891\n",
            "train loss:0.9347835665066934\n",
            "train loss:1.0764067367637908\n",
            "train loss:1.0693212896499764\n",
            "train loss:1.156492091109425\n",
            "train loss:1.0000468823604323\n",
            "train loss:0.9714994494185478\n",
            "train loss:0.97020063903107\n",
            "train loss:1.0652845500386596\n",
            "train loss:0.9584664859009443\n",
            "train loss:0.89257848313105\n",
            "train loss:1.0452340902940231\n",
            "train loss:0.9638416029220553\n",
            "train loss:1.0734530176949688\n",
            "train loss:0.9735140327629805\n",
            "train loss:1.1417878957287286\n",
            "train loss:1.005038082324819\n",
            "train loss:0.9327662439863955\n",
            "train loss:0.9335030394807782\n",
            "train loss:0.9834907759198726\n",
            "train loss:0.9522009955432097\n",
            "train loss:0.8999390041664412\n",
            "train loss:0.8848519147411211\n",
            "train loss:1.0229621102580584\n",
            "train loss:1.221587001547875\n",
            "train loss:1.150433063619541\n",
            "train loss:0.9504886581049513\n",
            "train loss:0.9815102429840705\n",
            "train loss:1.0113580873588572\n",
            "train loss:1.0332366318580946\n",
            "train loss:1.031455828955592\n",
            "train loss:1.1446021214788096\n",
            "train loss:0.9481168599326688\n",
            "train loss:1.1557364228086031\n",
            "train loss:1.0670289882052162\n",
            "train loss:1.0090732330423924\n",
            "train loss:0.8983428682906003\n",
            "train loss:1.3215895066554308\n",
            "train loss:0.7949811225476152\n",
            "train loss:0.8477477620489906\n",
            "train loss:1.1546999055087481\n",
            "train loss:0.9534379310448341\n",
            "train loss:0.902643564107224\n",
            "train loss:0.9648487372261854\n",
            "train loss:0.9815744509060587\n",
            "train loss:1.031194239495693\n",
            "train loss:0.8302430276859023\n",
            "train loss:1.0537773791365153\n",
            "train loss:0.9567750566900777\n",
            "train loss:1.070065859507178\n",
            "train loss:0.83238722952365\n",
            "train loss:1.0129460198434388\n",
            "train loss:1.0072914303259335\n",
            "train loss:1.0214903440457759\n",
            "train loss:1.015845501002542\n",
            "train loss:0.8259041186013836\n",
            "train loss:0.949068489556071\n",
            "train loss:0.9543230885976436\n",
            "train loss:1.1894168369154208\n",
            "train loss:1.1779267939102285\n",
            "train loss:1.0324002604999918\n",
            "train loss:0.9404173291831458\n",
            "train loss:1.0337331181947045\n",
            "train loss:1.1641881300467452\n",
            "train loss:0.9669329321692598\n",
            "train loss:1.1629514607423166\n",
            "train loss:0.8457560007775439\n",
            "train loss:0.8854739469144226\n",
            "train loss:1.0923101170222425\n",
            "train loss:0.995642086903329\n",
            "train loss:0.806415258607262\n",
            "train loss:1.0544373717959792\n",
            "train loss:0.9986709165751827\n",
            "train loss:0.9686825558263398\n",
            "train loss:1.1949737885242682\n",
            "train loss:0.9687011150493199\n",
            "train loss:1.0934357200035922\n",
            "train loss:0.9060708712445887\n",
            "train loss:0.8905248039554128\n",
            "train loss:0.9544100791561572\n",
            "train loss:1.0279866966038507\n",
            "train loss:0.9750297966610029\n",
            "train loss:0.9528831776092906\n",
            "train loss:1.1934870659505123\n",
            "train loss:0.9266374443898465\n",
            "train loss:1.048410477506763\n",
            "train loss:1.046965677074415\n",
            "train loss:0.8852613349723761\n",
            "train loss:0.9500270358409079\n",
            "train loss:0.8835089630248767\n",
            "train loss:0.9395516982472377\n",
            "train loss:1.0284621298781855\n",
            "train loss:0.8799961005825772\n",
            "train loss:0.9464453380930147\n",
            "train loss:1.0459693063500868\n",
            "train loss:0.884732621022279\n",
            "train loss:0.9639147783705215\n",
            "train loss:1.015025841665294\n",
            "train loss:0.9117921556431102\n",
            "train loss:1.1347505833858482\n",
            "train loss:0.9931153688899402\n",
            "train loss:1.058576727303831\n",
            "train loss:0.7901196706845335\n",
            "train loss:0.8942435631222219\n",
            "train loss:0.8932303534813859\n",
            "train loss:0.9402477252985473\n",
            "train loss:0.9576639997867332\n",
            "train loss:1.0872823343241873\n",
            "train loss:0.9973367799969002\n",
            "train loss:1.0814874733665023\n",
            "train loss:0.9919441536002985\n",
            "train loss:1.0815424019225004\n",
            "train loss:0.9372119510614232\n",
            "train loss:1.0722908297543734\n",
            "train loss:1.022187131010512\n",
            "train loss:1.042281298425746\n",
            "train loss:0.8750314063562848\n",
            "train loss:0.8533886474243134\n",
            "train loss:0.9673571921058098\n",
            "train loss:0.9928300702134014\n",
            "train loss:0.9797656870609144\n",
            "train loss:1.1490744088340272\n",
            "train loss:0.9371606972996473\n",
            "train loss:0.9886555056659171\n",
            "train loss:0.9196876847388543\n",
            "train loss:1.0258648801497996\n",
            "train loss:0.8930473457018961\n",
            "train loss:1.0010473559873154\n",
            "train loss:1.0218625373834402\n",
            "train loss:1.0170304968291946\n",
            "train loss:0.8054166183475157\n",
            "train loss:0.9691643826437892\n",
            "train loss:0.809876850863331\n",
            "train loss:0.7370452308714751\n",
            "train loss:0.9840319888396468\n",
            "train loss:0.9050966443390827\n",
            "train loss:0.984936099236072\n",
            "train loss:1.0714502259461383\n",
            "train loss:1.0605597861105913\n",
            "train loss:1.0664565130435364\n",
            "train loss:1.1736499896995578\n",
            "train loss:0.958250812328105\n",
            "train loss:1.04378190395164\n",
            "train loss:0.9337682003213297\n",
            "train loss:1.0140101456184014\n",
            "train loss:0.9758171418215232\n",
            "train loss:1.066227131565589\n",
            "train loss:0.9086845843230641\n",
            "train loss:1.0091606240455804\n",
            "train loss:0.961630937546991\n",
            "train loss:1.0654937260275903\n",
            "train loss:1.1659669706396782\n",
            "train loss:0.9780654781263921\n",
            "train loss:0.9806540024736644\n",
            "train loss:1.1239496907200865\n",
            "train loss:0.9819515311565077\n",
            "train loss:1.042358870542293\n",
            "train loss:0.9793804498328429\n",
            "train loss:0.7321810774783652\n",
            "train loss:0.9794359197055329\n",
            "train loss:0.932436090621097\n",
            "train loss:0.9266279767581139\n",
            "train loss:1.0953914591622635\n",
            "train loss:1.0981918798676233\n",
            "train loss:0.8500316756909286\n",
            "train loss:0.9099193530561428\n",
            "train loss:0.9215202455452762\n",
            "train loss:0.8792623436743351\n",
            "train loss:1.05632631817035\n",
            "train loss:0.8320744990103195\n",
            "train loss:0.8672482294411\n",
            "train loss:0.8519492785779923\n",
            "train loss:0.9974964168284512\n",
            "train loss:0.9142904132732936\n",
            "train loss:1.0353371409859393\n",
            "train loss:1.0892629463732117\n",
            "train loss:1.113565880963572\n",
            "train loss:1.0837793810820069\n",
            "train loss:0.7217617921159849\n",
            "train loss:0.9625745576526942\n",
            "train loss:0.9346116527944591\n",
            "train loss:0.9541153127299492\n",
            "train loss:0.9212277913010467\n",
            "train loss:0.9737667899373751\n",
            "train loss:0.9018004674830862\n",
            "train loss:0.9663783097473455\n",
            "train loss:0.7923718941062567\n",
            "train loss:1.050104815367701\n",
            "train loss:0.9545736364150761\n",
            "train loss:0.8644611779101885\n",
            "train loss:0.929137822707723\n",
            "train loss:0.9694175625927985\n",
            "train loss:1.000516156741018\n",
            "train loss:0.9807338012749234\n",
            "train loss:1.0306900626785065\n",
            "train loss:1.0651078963310925\n",
            "train loss:1.0490752221200947\n",
            "train loss:1.0372397394326192\n",
            "train loss:1.0664798364353196\n",
            "train loss:0.9152620394083306\n",
            "train loss:1.0116032148176353\n",
            "train loss:0.9300165625829915\n",
            "train loss:1.0313595944010396\n",
            "train loss:0.9221127814923561\n",
            "train loss:1.019894162937302\n",
            "train loss:1.0405392210974065\n",
            "train loss:1.1046502615145126\n",
            "train loss:1.0900622762598122\n",
            "train loss:1.0286609701091265\n",
            "train loss:1.1053772409565692\n",
            "train loss:1.0262026284554469\n",
            "train loss:0.9273554910630233\n",
            "train loss:0.8570805409617176\n",
            "train loss:1.043324622401254\n",
            "train loss:0.8652400475241268\n",
            "train loss:1.0890847446978895\n",
            "train loss:1.1638766204084714\n",
            "train loss:1.0254290533121047\n",
            "train loss:0.957102912641018\n",
            "train loss:0.9794091522713709\n",
            "train loss:1.049292319815502\n",
            "train loss:1.01743678075943\n",
            "train loss:1.021587439478571\n",
            "train loss:0.9989111621086968\n",
            "train loss:1.1840814753284787\n",
            "train loss:1.0057945031245508\n",
            "train loss:1.0332075892652777\n",
            "train loss:1.0959881721730387\n",
            "train loss:0.7788339380320806\n",
            "train loss:0.7994247906664727\n",
            "train loss:0.9122662121844116\n",
            "train loss:1.0593467626861233\n",
            "train loss:1.145139248883566\n",
            "train loss:0.9553763392395891\n",
            "train loss:1.1001849650020246\n",
            "train loss:0.935277882775712\n",
            "train loss:1.0566551457665083\n",
            "train loss:0.7829206113575602\n",
            "train loss:1.1760117014165261\n",
            "train loss:0.9076914582222422\n",
            "train loss:0.9850879651490179\n",
            "train loss:0.9329041403675374\n",
            "train loss:0.9429392819429577\n",
            "train loss:0.7961751278891326\n",
            "train loss:0.8343304999472084\n",
            "train loss:1.032837464088383\n",
            "train loss:0.9249761218477975\n",
            "train loss:0.9642245309609204\n",
            "train loss:1.0175049592742356\n",
            "train loss:1.0424614774180825\n",
            "train loss:0.9704698771260962\n",
            "train loss:1.0341788897389295\n",
            "train loss:0.8960455664245407\n",
            "train loss:1.0082913715259798\n",
            "train loss:1.189343378717699\n",
            "train loss:0.976802800336543\n",
            "train loss:0.8968569951462946\n",
            "train loss:1.000555638794212\n",
            "train loss:0.9749476964846943\n",
            "train loss:0.9510737609712132\n",
            "train loss:0.9276336447386018\n",
            "train loss:1.139882249952923\n",
            "train loss:1.0562721376525153\n",
            "train loss:1.1588498073501399\n",
            "train loss:1.1938638800553738\n",
            "train loss:1.080368067132295\n",
            "train loss:0.838625370088624\n",
            "train loss:0.8305566431475647\n",
            "train loss:1.0631496238132203\n",
            "train loss:1.1665205476307587\n",
            "train loss:1.0549531805363064\n",
            "train loss:0.94294102763658\n",
            "train loss:0.8451451421864491\n",
            "train loss:1.0063401618134267\n",
            "train loss:1.0303746587933547\n",
            "train loss:0.9637404333672848\n",
            "train loss:0.9242705561735121\n",
            "train loss:1.0827886588142512\n",
            "train loss:0.9584398749688439\n",
            "train loss:0.8897659027364156\n",
            "train loss:1.1023350508654064\n",
            "train loss:0.8495427440171082\n",
            "train loss:0.9238766874112421\n",
            "train loss:0.9723323337734401\n",
            "train loss:0.8875429706264666\n",
            "train loss:1.0472523494730401\n",
            "train loss:0.9733108811529267\n",
            "train loss:1.165223527048227\n",
            "train loss:0.8873030760021123\n",
            "train loss:1.0191841501928405\n",
            "train loss:1.0310731663390944\n",
            "train loss:1.1846616700089847\n",
            "train loss:0.8737566524584731\n",
            "train loss:0.8503918279524041\n",
            "train loss:0.9461960787579571\n",
            "train loss:0.9436435776088562\n",
            "train loss:0.9098395605567623\n",
            "train loss:0.837371733736861\n",
            "train loss:0.9025697012315935\n",
            "train loss:0.9026236734794449\n",
            "train loss:0.9983504970361702\n",
            "train loss:1.0458142669663426\n",
            "train loss:0.930697109231539\n",
            "train loss:1.040128585212184\n",
            "train loss:0.9018691542916081\n",
            "train loss:0.8935737297163885\n",
            "train loss:1.1469654651123193\n",
            "train loss:1.0697878912187062\n",
            "train loss:1.19683416938383\n",
            "train loss:0.9623761506243274\n",
            "train loss:0.9186303584070106\n",
            "train loss:0.9411428790097562\n",
            "train loss:0.8980488112332006\n",
            "train loss:1.0849582749773483\n",
            "train loss:1.1947174421268643\n",
            "train loss:1.0160034186364109\n",
            "train loss:0.986402380553309\n",
            "train loss:0.7935973442389868\n",
            "train loss:0.9727276982350739\n",
            "train loss:1.21965143336471\n",
            "train loss:1.0871104806382237\n",
            "train loss:1.1001257072290378\n",
            "train loss:1.0351011070151124\n",
            "train loss:1.0753372741830138\n",
            "train loss:1.0204755370231218\n",
            "train loss:0.8894763038107772\n",
            "train loss:1.031477979898962\n",
            "train loss:0.9438584582480835\n",
            "train loss:0.9652246615720105\n",
            "train loss:1.0340529591227232\n",
            "train loss:1.0349137513674986\n",
            "train loss:1.0185626154266665\n",
            "train loss:1.2226221482189847\n",
            "train loss:1.1109102069530652\n",
            "train loss:1.1454454604962132\n",
            "train loss:0.9756483837207507\n",
            "train loss:0.9912027314694805\n",
            "train loss:1.0178932936724754\n",
            "train loss:1.0459634972588645\n",
            "train loss:1.024209740206771\n",
            "train loss:0.9110903890395342\n",
            "train loss:0.8849419635544696\n",
            "train loss:0.8850908090123187\n",
            "train loss:1.0453863347118233\n",
            "train loss:1.0540104021131158\n",
            "train loss:0.9520389803901204\n",
            "train loss:1.1248778426998098\n",
            "train loss:0.9703829437618218\n",
            "train loss:1.052923786956616\n",
            "train loss:0.9555644149011995\n",
            "train loss:0.9354014549032819\n",
            "train loss:0.9437425728524363\n",
            "train loss:0.9713021059865943\n",
            "train loss:0.8448624775420825\n",
            "train loss:0.9511141606024343\n",
            "train loss:0.8956957259419319\n",
            "train loss:0.9649886193103824\n",
            "train loss:0.9349493114611045\n",
            "train loss:0.8774489704565711\n",
            "train loss:0.953188892404603\n",
            "train loss:1.018804873800287\n",
            "train loss:0.9266613350256275\n",
            "train loss:0.8532661630473943\n",
            "train loss:0.8788293871839018\n",
            "train loss:1.1485913849811005\n",
            "train loss:1.075632006817411\n",
            "train loss:0.9817439215309967\n",
            "train loss:1.0636174031289527\n",
            "train loss:0.9654105862246378\n",
            "train loss:1.0869331277449117\n",
            "train loss:0.9629330246121723\n",
            "train loss:1.0392691792125779\n",
            "train loss:1.195496914794635\n",
            "train loss:0.9379111664521964\n",
            "train loss:1.0958685999155227\n",
            "train loss:0.8897437504549813\n",
            "train loss:0.9727780579763199\n",
            "train loss:0.9267409261551168\n",
            "train loss:0.9667179514886933\n",
            "train loss:1.0079070922294433\n",
            "train loss:0.9691574713851658\n",
            "train loss:1.0275544871547297\n",
            "train loss:1.0305023726275224\n",
            "train loss:0.9805660677221795\n",
            "train loss:1.0237983365840642\n",
            "train loss:0.9674518254834702\n",
            "train loss:0.884978713368656\n",
            "train loss:0.9730796647966323\n",
            "train loss:0.9903987651698566\n",
            "train loss:1.0903474841131406\n",
            "train loss:0.9916469687216495\n",
            "train loss:0.9833265176327997\n",
            "train loss:0.9297703689180206\n",
            "train loss:0.9194571341016871\n",
            "train loss:1.0015845409873991\n",
            "train loss:0.8947540313966481\n",
            "train loss:0.9585932686266903\n",
            "train loss:1.0072114794244151\n",
            "train loss:1.016396675522937\n",
            "train loss:1.07560315773533\n",
            "train loss:0.8502113706594142\n",
            "train loss:1.006355886581895\n",
            "train loss:0.95122394510219\n",
            "train loss:0.9577104431857657\n",
            "train loss:1.0853714969778665\n",
            "train loss:1.0250823130428834\n",
            "train loss:0.8621303170026599\n",
            "train loss:1.0652146653922718\n",
            "train loss:0.9837654693889124\n",
            "train loss:0.918682533721189\n",
            "train loss:1.0335421759005237\n",
            "train loss:0.9849106002322167\n",
            "train loss:0.9192936490681283\n",
            "train loss:1.067833933730714\n",
            "train loss:1.0335271728351314\n",
            "train loss:0.8674247381423541\n",
            "train loss:0.9108662162186915\n",
            "train loss:1.160184793685053\n",
            "train loss:0.973786078113709\n",
            "train loss:0.9701938302286286\n",
            "train loss:0.9938963278474988\n",
            "train loss:0.8237161752629593\n",
            "train loss:1.0854499144051635\n",
            "train loss:0.9000238076510424\n",
            "train loss:1.073832945299979\n",
            "train loss:1.060591402975359\n",
            "train loss:0.9562029464760015\n",
            "train loss:1.09083342613682\n",
            "train loss:1.0117509688956305\n",
            "train loss:0.9376202479743891\n",
            "train loss:1.0258896200170642\n",
            "train loss:1.006183702976302\n",
            "train loss:0.9767369994382799\n",
            "train loss:0.9506207157960871\n",
            "train loss:1.09610996322747\n",
            "train loss:0.933241367022536\n",
            "train loss:0.8395638006250131\n",
            "train loss:0.8708797491005699\n",
            "train loss:1.0301231669720137\n",
            "train loss:0.9988890715855091\n",
            "train loss:0.9577737113017669\n",
            "train loss:0.8985154598340563\n",
            "train loss:1.085350790707678\n",
            "train loss:1.0277230211120438\n",
            "train loss:0.9740254850945934\n",
            "train loss:1.0751445906680808\n",
            "train loss:1.0670449321972604\n",
            "train loss:0.9540264408805477\n",
            "train loss:1.0529967652380396\n",
            "train loss:1.1692765275959882\n",
            "train loss:0.8430324007280896\n",
            "train loss:0.9198172107490755\n",
            "train loss:1.1181936329208013\n",
            "train loss:1.0972695837580597\n",
            "train loss:0.9523626173082905\n",
            "train loss:1.1186497303672764\n",
            "train loss:1.0383162652532767\n",
            "train loss:1.064318107083253\n",
            "train loss:0.9077900487311515\n",
            "train loss:1.0221373186943479\n",
            "train loss:0.9905907043088469\n",
            "train loss:1.0356796284991017\n",
            "train loss:1.0585986830354202\n",
            "train loss:0.8702148887208284\n",
            "train loss:1.0684243391774446\n",
            "train loss:0.8219359928716252\n",
            "train loss:0.8497200092862712\n",
            "train loss:0.8981113230194377\n",
            "train loss:1.0085078098267952\n",
            "train loss:1.1123036437385223\n",
            "train loss:1.0608944828765308\n",
            "train loss:0.9490674400706252\n",
            "train loss:0.9381742898543857\n",
            "train loss:1.11138541296916\n",
            "train loss:0.9291018318194753\n",
            "train loss:1.0118607659366898\n",
            "train loss:1.0275115370758838\n",
            "train loss:1.1088303864847664\n",
            "train loss:0.9970922338551191\n",
            "train loss:0.9282442480422712\n",
            "train loss:1.163322037640128\n",
            "train loss:0.9581902096287557\n",
            "train loss:0.8927598202413298\n",
            "train loss:1.0030745933195948\n",
            "train loss:1.0357900711583061\n",
            "train loss:0.9444634423152583\n",
            "train loss:0.9001456928006832\n",
            "train loss:1.0109741115509585\n",
            "train loss:0.9461635865776046\n",
            "train loss:0.9716664468227088\n",
            "train loss:1.0580142049587826\n",
            "train loss:0.9951269142882256\n",
            "train loss:0.903429195444708\n",
            "train loss:0.9550141507953414\n",
            "train loss:0.8792342353827461\n",
            "train loss:1.077753057528981\n",
            "train loss:1.235052867007779\n",
            "train loss:0.8572773638357022\n",
            "train loss:0.8843803348131913\n",
            "train loss:1.0868158707837767\n",
            "train loss:0.9397370950925505\n",
            "train loss:0.902108327771191\n",
            "train loss:1.1230653894736213\n",
            "train loss:1.0879549511101196\n",
            "train loss:1.0644339871554884\n",
            "train loss:0.8671242530514637\n",
            "train loss:0.9070741980312039\n",
            "train loss:1.0305458450376161\n",
            "train loss:1.0304723316503663\n",
            "train loss:0.8964055974651345\n",
            "train loss:0.9565795764487358\n",
            "train loss:1.1235280697824883\n",
            "train loss:1.0626291886714965\n",
            "train loss:0.9944076358920008\n",
            "train loss:1.1142587580877412\n",
            "train loss:0.9123927093912392\n",
            "train loss:1.0758903406073903\n",
            "train loss:1.033992389300183\n",
            "train loss:1.074134016106995\n",
            "train loss:1.018446763586114\n",
            "train loss:0.9964474851313121\n",
            "train loss:0.8962486011199121\n",
            "=== epoch:4, train acc:0.987, test acc:0.986 ===\n",
            "train loss:1.0666339201380346\n",
            "train loss:1.1764847937541123\n",
            "train loss:1.0667556698501637\n",
            "train loss:1.1092019676794456\n",
            "train loss:1.0759487885170345\n",
            "train loss:0.9318971885494178\n",
            "train loss:1.0209723167764804\n",
            "train loss:1.0835548932924959\n",
            "train loss:1.14735814016383\n",
            "train loss:0.9513792600473846\n",
            "train loss:0.9233878227201435\n",
            "train loss:0.9398663853374833\n",
            "train loss:0.9472163484601666\n",
            "train loss:1.0306972512939512\n",
            "train loss:1.0685272617086616\n",
            "train loss:1.1258980728473804\n",
            "train loss:0.9504452785402668\n",
            "train loss:0.974462717300637\n",
            "train loss:1.031692538870043\n",
            "train loss:0.9242704235826388\n",
            "train loss:1.0856387492525583\n",
            "train loss:0.9810876891198916\n",
            "train loss:0.8136361892778861\n",
            "train loss:0.9219955867231324\n",
            "train loss:0.8452361792840081\n",
            "train loss:0.7391536061233341\n",
            "train loss:0.9741252772139226\n",
            "train loss:0.9896847809096689\n",
            "train loss:1.122343030117726\n",
            "train loss:0.8328403332181779\n",
            "train loss:0.8290199132990879\n",
            "train loss:1.0459219246954414\n",
            "train loss:0.8918009704557848\n",
            "train loss:0.8167535099429248\n",
            "train loss:0.867877208185677\n",
            "train loss:0.8938550060490443\n",
            "train loss:1.055653779419243\n",
            "train loss:1.035611312847802\n",
            "train loss:0.957981831391681\n",
            "train loss:0.9946636478469371\n",
            "train loss:1.0534679180133688\n",
            "train loss:1.2504006401336052\n",
            "train loss:0.9104046461747053\n",
            "train loss:1.0149834801971829\n",
            "train loss:0.917904311903154\n",
            "train loss:0.9741709470862158\n",
            "train loss:0.9909953109571394\n",
            "train loss:0.9757253205337487\n",
            "train loss:0.9764519083955163\n",
            "train loss:0.9737569764836193\n",
            "train loss:1.0511228607542196\n",
            "train loss:1.117728984315047\n",
            "train loss:0.9404899686156059\n",
            "train loss:1.029997193077768\n",
            "train loss:1.0483558232639063\n",
            "train loss:1.0027650256653848\n",
            "train loss:1.0824312595492471\n",
            "train loss:0.9404110688661325\n",
            "train loss:1.011028567691545\n",
            "train loss:0.8068360799522192\n",
            "train loss:1.0977014196876986\n",
            "train loss:0.9536473210265579\n",
            "train loss:1.0615403739176357\n",
            "train loss:0.9439159454068353\n",
            "train loss:0.9254739062649086\n",
            "train loss:1.1701115249794527\n",
            "train loss:1.0673142209640225\n",
            "train loss:1.1522602352206506\n",
            "train loss:0.6465575074034113\n",
            "train loss:0.9542412824467719\n",
            "train loss:0.8172398997651807\n",
            "train loss:1.0245463482469475\n",
            "train loss:1.1458828028195531\n",
            "train loss:0.6455002212398112\n",
            "train loss:0.9342415995612205\n",
            "train loss:0.9776270468250052\n",
            "train loss:1.0695114313640866\n",
            "train loss:0.9318123666298859\n",
            "train loss:0.9289525253166134\n",
            "train loss:1.0021215365389728\n",
            "train loss:0.9057099383988115\n",
            "train loss:1.023970870185738\n",
            "train loss:0.9393950433064063\n",
            "train loss:0.831042354938631\n",
            "train loss:0.9478061086656513\n",
            "train loss:0.9156147490848524\n",
            "train loss:1.1661703306322544\n",
            "train loss:1.0579030012172514\n",
            "train loss:0.950196402756178\n",
            "train loss:1.0349048318927339\n",
            "train loss:0.8752256862376459\n",
            "train loss:0.9063542012839301\n",
            "train loss:0.9150257022593843\n",
            "train loss:0.9810168017548255\n",
            "train loss:1.0568635361182896\n",
            "train loss:0.8401705443563714\n",
            "train loss:0.9644680145938078\n",
            "train loss:0.9460632039829652\n",
            "train loss:1.1243260787522742\n",
            "train loss:0.9910332306833363\n",
            "train loss:0.9382199874131856\n",
            "train loss:0.9973480258217684\n",
            "train loss:1.0075615352403724\n",
            "train loss:0.8905947520396142\n",
            "train loss:0.886226177440776\n",
            "train loss:1.137791934007144\n",
            "train loss:0.782257077716419\n",
            "train loss:1.055884913823627\n",
            "train loss:0.9138027905804813\n",
            "train loss:1.1715047919016417\n",
            "train loss:1.0532835533844098\n",
            "train loss:1.2575551172193737\n",
            "train loss:1.130333711013336\n",
            "train loss:0.8621978013340973\n",
            "train loss:0.8899322891843576\n",
            "train loss:1.1990894740192868\n",
            "train loss:0.9989818730153364\n",
            "train loss:0.9497343818387246\n",
            "train loss:1.1484706639382278\n",
            "train loss:1.045273131037368\n",
            "train loss:1.0220568750113528\n",
            "train loss:0.8096813954132029\n",
            "train loss:0.9942053049729838\n",
            "train loss:1.0343315918590452\n",
            "train loss:1.2105202320511268\n",
            "train loss:1.0351940972514238\n",
            "train loss:0.952841000054059\n",
            "train loss:0.7634804998293907\n",
            "train loss:0.969820726172487\n",
            "train loss:1.129220630034939\n",
            "train loss:1.0197483339708517\n",
            "train loss:0.8996224045761005\n",
            "train loss:0.8314293075592587\n",
            "train loss:0.8750839996877231\n",
            "train loss:1.0469087111848727\n",
            "train loss:0.8709626745442723\n",
            "train loss:0.8939275658440082\n",
            "train loss:0.9388892995833502\n",
            "train loss:0.9310057195694595\n",
            "train loss:0.994957455526891\n",
            "train loss:0.9575548818247671\n",
            "train loss:0.9881425642049396\n",
            "train loss:1.1429400503499487\n",
            "train loss:0.9900301872506705\n",
            "train loss:0.899502804492166\n",
            "train loss:1.024660500075695\n",
            "train loss:0.9697357548392409\n",
            "train loss:1.064938950837416\n",
            "train loss:1.0229558982025326\n",
            "train loss:1.1811982175412208\n",
            "train loss:0.8728456781309969\n",
            "train loss:0.9429765203923713\n",
            "train loss:0.8269346034369986\n",
            "train loss:0.8159393888099147\n",
            "train loss:0.9211503852976081\n",
            "train loss:0.8121076589391626\n",
            "train loss:0.925315728818894\n",
            "train loss:0.8864317248792116\n",
            "train loss:0.9312402064559278\n",
            "train loss:0.9776486098055891\n",
            "train loss:0.9959844266183095\n",
            "train loss:1.019930223252476\n",
            "train loss:0.7986865216149254\n",
            "train loss:0.8841928057696958\n",
            "train loss:1.0013234722494864\n",
            "train loss:0.9052292972850106\n",
            "train loss:0.8439028799099506\n",
            "train loss:1.1135663795988038\n",
            "train loss:0.9721158160575999\n",
            "train loss:1.1482516586763871\n",
            "train loss:0.7637557100635958\n",
            "train loss:0.8356629602217212\n",
            "train loss:0.9669082140448567\n",
            "train loss:0.9717359691684119\n",
            "train loss:0.9090397056871049\n",
            "train loss:0.970894992804631\n",
            "train loss:1.1709871690195697\n",
            "train loss:0.8950864759330878\n",
            "train loss:0.9809486863728941\n",
            "train loss:0.9064407035333778\n",
            "train loss:0.9460896214141826\n",
            "train loss:0.9184957312516331\n",
            "train loss:0.8879668977993844\n",
            "train loss:1.1458540759351323\n",
            "train loss:1.0704449181428684\n",
            "train loss:1.131920153295215\n",
            "train loss:0.914653583511325\n",
            "train loss:0.9004190778597906\n",
            "train loss:1.0081301427799039\n",
            "train loss:0.8845625083055846\n",
            "train loss:1.0097650967712335\n",
            "train loss:0.7320483034833734\n",
            "train loss:1.006860369771496\n",
            "train loss:0.8754818553462772\n",
            "train loss:0.9921342643435617\n",
            "train loss:0.9702954446593979\n",
            "train loss:0.9330623501558286\n",
            "train loss:0.967245392457519\n",
            "train loss:1.0524496937132735\n",
            "train loss:0.9361287209479964\n",
            "train loss:0.9437634293947365\n",
            "train loss:0.916488268702478\n",
            "train loss:1.045269308242328\n",
            "train loss:0.9937831182520924\n",
            "train loss:0.7955258277129196\n",
            "train loss:0.867634288530286\n",
            "train loss:0.938350036829939\n",
            "train loss:0.9979810025555602\n",
            "train loss:0.8726524331614094\n",
            "train loss:1.0560438177123626\n",
            "train loss:0.8631689108581452\n",
            "train loss:1.0431710624761021\n",
            "train loss:1.0408291153920946\n",
            "train loss:0.7116185689364487\n",
            "train loss:1.1295160534922832\n",
            "train loss:0.8940687435470577\n",
            "train loss:0.8553177393990162\n",
            "train loss:0.8440022719364332\n",
            "train loss:1.1363790210994735\n",
            "train loss:0.8576921727206898\n",
            "train loss:0.7999363723130334\n",
            "train loss:0.8857613075963965\n",
            "train loss:0.9419869148398194\n",
            "train loss:1.0596291065781358\n",
            "train loss:1.1700402821016216\n",
            "train loss:0.8805145721228672\n",
            "train loss:1.010172030206072\n",
            "train loss:0.6822909986132015\n",
            "train loss:0.7195774277276061\n",
            "train loss:0.9699185456654217\n",
            "train loss:0.867271870228126\n",
            "train loss:0.9468823791170909\n",
            "train loss:0.9914857313710189\n",
            "train loss:1.0783630289411663\n",
            "train loss:0.9311242732934834\n",
            "train loss:1.0429756591043482\n",
            "train loss:1.0074159516938892\n",
            "train loss:0.8925363185153233\n",
            "train loss:0.8769041863821931\n",
            "train loss:1.042103832740744\n",
            "train loss:0.945796549375118\n",
            "train loss:1.0490754119138683\n",
            "train loss:0.7489112566113522\n",
            "train loss:0.9882672281868872\n",
            "train loss:1.1459685783079703\n",
            "train loss:0.8625219975615736\n",
            "train loss:0.8825108000366151\n",
            "train loss:0.877948505571797\n",
            "train loss:0.9935511885502109\n",
            "train loss:0.9962969287835723\n",
            "train loss:0.9335450050666726\n",
            "train loss:0.9644938104724281\n",
            "train loss:0.9054020860112749\n",
            "train loss:1.062992766270452\n",
            "train loss:0.8691687404948132\n",
            "train loss:0.8797217262328979\n",
            "train loss:0.844777722095359\n",
            "train loss:0.8156255711832949\n",
            "train loss:0.9905801682975789\n",
            "train loss:0.9596658506380125\n",
            "train loss:0.9076929627649083\n",
            "train loss:0.9751041386588808\n",
            "train loss:0.9376889152481077\n",
            "train loss:0.9300820717361786\n",
            "train loss:1.0010506196172642\n",
            "train loss:1.012607161400964\n",
            "train loss:0.9414501156533263\n",
            "train loss:1.0015266942649503\n",
            "train loss:0.89395376701875\n",
            "train loss:1.1190521087705763\n",
            "train loss:0.9127288652720716\n",
            "train loss:0.9726811083624511\n",
            "train loss:0.9156779108536347\n",
            "train loss:0.8917513995347384\n",
            "train loss:1.1790285287098174\n",
            "train loss:0.8771534858011549\n",
            "train loss:1.0119585852224573\n",
            "train loss:1.1032797932470841\n",
            "train loss:0.8842323679035161\n",
            "train loss:0.8289872410698546\n",
            "train loss:0.8781659100408448\n",
            "train loss:0.895039835944247\n",
            "train loss:1.108778259513005\n",
            "train loss:1.1047360814565914\n",
            "train loss:0.9898772989886084\n",
            "train loss:0.9281501125362008\n",
            "train loss:0.9400781675067003\n",
            "train loss:1.2014678451937\n",
            "train loss:1.0830050222082732\n",
            "train loss:0.9058980931432565\n",
            "train loss:0.8579724594491522\n",
            "train loss:0.8984455192032926\n",
            "train loss:0.9670796719667548\n",
            "train loss:0.9962161711005288\n",
            "train loss:0.9394071192038544\n",
            "train loss:1.12219956006764\n",
            "train loss:0.9274536608983993\n",
            "train loss:1.133075599973005\n",
            "train loss:0.9506833157175234\n",
            "train loss:1.0132778541298928\n",
            "train loss:0.9521910100117484\n",
            "train loss:1.038105359146245\n",
            "train loss:1.0049500349994975\n",
            "train loss:0.9238395276390086\n",
            "train loss:0.9832623324272833\n",
            "train loss:0.8683995078470101\n",
            "train loss:0.8492970662980163\n",
            "train loss:1.0115134622351132\n",
            "train loss:0.9521546933445671\n",
            "train loss:0.9812773671523513\n",
            "train loss:0.8836844766814802\n",
            "train loss:1.0026283034991363\n",
            "train loss:0.7895963522449727\n",
            "train loss:1.1544415293374692\n",
            "train loss:0.8986275303156694\n",
            "train loss:0.8102778009468232\n",
            "train loss:0.8351428500709817\n",
            "train loss:1.0139388799016613\n",
            "train loss:0.9063153076882645\n",
            "train loss:1.0238432176861592\n",
            "train loss:0.9588490407692645\n",
            "train loss:0.8171688127880337\n",
            "train loss:1.0261823907558631\n",
            "train loss:1.0964408506054464\n",
            "train loss:0.9630246617971587\n",
            "train loss:0.989621047984831\n",
            "train loss:0.9719541412965286\n",
            "train loss:1.0538852510629524\n",
            "train loss:1.045214675965064\n",
            "train loss:0.952148195313302\n",
            "train loss:0.8303782407087007\n",
            "train loss:0.9311197085669862\n",
            "train loss:0.8770952903721383\n",
            "train loss:1.0581512037381247\n",
            "train loss:1.0121969289622244\n",
            "train loss:0.9288910673801226\n",
            "train loss:0.8958800285062799\n",
            "train loss:1.0699429363918735\n",
            "train loss:1.0162612219065232\n",
            "train loss:0.7910811563388837\n",
            "train loss:0.957553167638656\n",
            "train loss:1.0663446188223729\n",
            "train loss:1.0711497857086827\n",
            "train loss:1.0451199044723036\n",
            "train loss:0.8858161459230096\n",
            "train loss:0.9162854268555796\n",
            "train loss:0.9330106938762028\n",
            "train loss:0.9097904454125205\n",
            "train loss:0.9303836844438139\n",
            "train loss:0.9497399072973438\n",
            "train loss:1.0513447920306584\n",
            "train loss:0.8971777402036011\n",
            "train loss:0.8726832026454088\n",
            "train loss:0.9068339869541411\n",
            "train loss:0.8445505973570572\n",
            "train loss:1.0028568826188922\n",
            "train loss:1.082288958629267\n",
            "train loss:0.8635725528023541\n",
            "train loss:1.0973768179781842\n",
            "train loss:0.8241605915098807\n",
            "train loss:0.9266520954477171\n",
            "train loss:1.020206382572453\n",
            "train loss:1.038500055258031\n",
            "train loss:1.0604161207090121\n",
            "train loss:0.9760019016253529\n",
            "train loss:0.8091102249974014\n",
            "train loss:0.8694388898148528\n",
            "train loss:0.8038267911236884\n",
            "train loss:0.7711167739787419\n",
            "train loss:1.0343383211089372\n",
            "train loss:0.6700625199623659\n",
            "train loss:0.7773920946450378\n",
            "train loss:1.1367349997962486\n",
            "train loss:1.0072274471858902\n",
            "train loss:0.9866621044047248\n",
            "train loss:1.0544770683765905\n",
            "train loss:1.0254680998846328\n",
            "train loss:0.9507369401636775\n",
            "train loss:0.9930866507323516\n",
            "train loss:0.910424675996196\n",
            "train loss:1.0987081559808192\n",
            "train loss:0.924440718926601\n",
            "train loss:0.8067330800366076\n",
            "train loss:0.9911504871529192\n",
            "train loss:0.871570888728861\n",
            "train loss:0.8669362708187682\n",
            "train loss:0.9739204806433877\n",
            "train loss:1.104474905573407\n",
            "train loss:0.9297535693030136\n",
            "train loss:1.092129928267374\n",
            "train loss:0.8400812168875221\n",
            "train loss:1.0134683828691236\n",
            "train loss:0.8459231530704959\n",
            "train loss:0.8658336651713959\n",
            "train loss:1.004503344065682\n",
            "train loss:0.8691376572332671\n",
            "train loss:0.9944160550510509\n",
            "train loss:0.8792049871689747\n",
            "train loss:0.939981788872368\n",
            "train loss:0.944128265700803\n",
            "train loss:0.8458126612245995\n",
            "train loss:0.964478059401916\n",
            "train loss:0.8049863988950804\n",
            "train loss:0.9680871743566288\n",
            "train loss:1.0662283562462835\n",
            "train loss:0.9902085150175136\n",
            "train loss:0.8556493770875183\n",
            "train loss:0.9445615425306528\n",
            "train loss:0.9670834299399997\n",
            "train loss:1.189408510736587\n",
            "train loss:1.002728731312503\n",
            "train loss:0.9075231015977664\n",
            "train loss:0.8875976731168251\n",
            "train loss:0.964250088112617\n",
            "train loss:0.9040449618661859\n",
            "train loss:0.8432088400176524\n",
            "train loss:1.1235432959735348\n",
            "train loss:0.9622179054135448\n",
            "train loss:1.0521907372456838\n",
            "train loss:0.7354448700860664\n",
            "train loss:1.1191736634820981\n",
            "train loss:0.7992903619959482\n",
            "train loss:0.9819662010402394\n",
            "train loss:0.901867874898857\n",
            "train loss:1.0664252521779782\n",
            "train loss:0.9874674532237729\n",
            "train loss:1.09534954808102\n",
            "train loss:0.9742398126523927\n",
            "train loss:0.9286881918706403\n",
            "train loss:0.9900464876062678\n",
            "train loss:0.9295395272483065\n",
            "train loss:1.0222734976088568\n",
            "train loss:0.9302272283824791\n",
            "train loss:1.0553495564421649\n",
            "train loss:0.9610912597817811\n",
            "train loss:0.8090658228903643\n",
            "train loss:0.8577142660893063\n",
            "train loss:0.7499481680746779\n",
            "train loss:0.9059758247564167\n",
            "train loss:1.0652188104197071\n",
            "train loss:1.0227600745198928\n",
            "train loss:0.8912788444948007\n",
            "train loss:1.040616937145873\n",
            "train loss:1.034213331764533\n",
            "train loss:0.9283740036694639\n",
            "train loss:0.9841586205593121\n",
            "train loss:0.7820358107140318\n",
            "train loss:0.9499513939205048\n",
            "train loss:0.9043507868218477\n",
            "train loss:1.0064352838649828\n",
            "train loss:0.895023417752013\n",
            "train loss:0.7376100676413131\n",
            "train loss:0.8361872222009474\n",
            "train loss:0.9963616943320938\n",
            "train loss:1.0418021147652707\n",
            "train loss:0.9953794461869149\n",
            "train loss:0.7820680265870168\n",
            "train loss:0.8243684978214616\n",
            "train loss:0.8954275527381657\n",
            "train loss:0.8166998016708531\n",
            "train loss:1.0858741705848878\n",
            "train loss:1.0821348238181379\n",
            "train loss:0.8946024532454494\n",
            "train loss:0.6974008462083665\n",
            "train loss:1.0600773294328487\n",
            "train loss:1.0145124857529955\n",
            "train loss:0.9031732665008927\n",
            "train loss:0.8584518115637304\n",
            "train loss:1.0161619062509784\n",
            "train loss:0.9294658012840125\n",
            "train loss:1.1714898396425248\n",
            "train loss:0.933507780613627\n",
            "train loss:1.005057011211039\n",
            "train loss:1.0025242983501415\n",
            "train loss:0.9380487904307748\n",
            "train loss:1.0294162747023596\n",
            "train loss:0.7786645671071721\n",
            "train loss:0.8268689934342348\n",
            "train loss:0.9236527603321676\n",
            "train loss:0.9093180363133773\n",
            "train loss:1.017780189658538\n",
            "train loss:0.9197576992189584\n",
            "train loss:0.9398132567592695\n",
            "train loss:0.9761073244509264\n",
            "train loss:0.9046223487308981\n",
            "train loss:0.9253782358909085\n",
            "train loss:0.9054439278341627\n",
            "train loss:0.7842929709241738\n",
            "train loss:0.7837164874230491\n",
            "train loss:0.8572001330189525\n",
            "train loss:0.9891668265427361\n",
            "train loss:0.8249274297051514\n",
            "train loss:1.0274437030125483\n",
            "train loss:0.9808967047572988\n",
            "train loss:0.9069613069936158\n",
            "train loss:1.0003854720265501\n",
            "train loss:0.7532570746572042\n",
            "train loss:0.8493709985778355\n",
            "train loss:0.9613102780202619\n",
            "train loss:1.0207057120211274\n",
            "train loss:0.8597120001737095\n",
            "train loss:0.951900517443149\n",
            "train loss:1.0492495783030937\n",
            "train loss:0.9961979161647524\n",
            "train loss:0.8680332155652876\n",
            "train loss:0.7922321086649059\n",
            "train loss:0.8689749014728401\n",
            "train loss:0.7777621896473942\n",
            "train loss:0.9439994713658768\n",
            "train loss:0.9948473528021959\n",
            "train loss:0.7515950200886037\n",
            "train loss:0.7402885183026331\n",
            "train loss:0.9619620339651561\n",
            "train loss:1.1547603927545784\n",
            "train loss:0.8864115125286085\n",
            "train loss:1.0384744783470325\n",
            "train loss:0.9516892654538788\n",
            "train loss:0.7984646271671982\n",
            "train loss:0.9210926320842677\n",
            "train loss:1.0444622963930705\n",
            "train loss:0.9129997229972667\n",
            "train loss:0.9248204284582183\n",
            "train loss:1.0617136440152297\n",
            "train loss:0.993660359501356\n",
            "train loss:0.7969235807288857\n",
            "train loss:0.9142077967832004\n",
            "train loss:1.0313947017371425\n",
            "train loss:0.8997464117957807\n",
            "train loss:0.979947307555672\n",
            "train loss:0.7420841710169525\n",
            "train loss:1.0626717825259309\n",
            "train loss:1.0298558750889009\n",
            "train loss:0.8505109503737888\n",
            "train loss:1.2290524647270586\n",
            "train loss:1.117097381102195\n",
            "train loss:0.9977583573552445\n",
            "train loss:0.9573565215024429\n",
            "train loss:1.0061054498039825\n",
            "train loss:0.8829437447194439\n",
            "train loss:0.9049729510536393\n",
            "train loss:0.7016490417908037\n",
            "train loss:0.9617391824206848\n",
            "train loss:1.005481387531726\n",
            "train loss:0.8820795892573563\n",
            "train loss:0.8974492812799757\n",
            "train loss:1.0150880613108004\n",
            "train loss:1.028038107549416\n",
            "train loss:0.9523182480685485\n",
            "train loss:0.9679850212450397\n",
            "train loss:0.9022089422205548\n",
            "train loss:0.9385274407279066\n",
            "train loss:0.9669992661734852\n",
            "train loss:0.857107823437451\n",
            "train loss:1.0436071522643409\n",
            "train loss:1.0394249810398168\n",
            "train loss:0.977692651199102\n",
            "train loss:0.8470065212036505\n",
            "train loss:0.8253388450954183\n",
            "train loss:0.9607993154314985\n",
            "train loss:0.8720673670740879\n",
            "train loss:0.970818534562946\n",
            "train loss:0.9878764800850891\n",
            "train loss:0.9461322251816511\n",
            "train loss:1.1349568212866321\n",
            "train loss:1.074188390171229\n",
            "train loss:1.1360383184972\n",
            "train loss:0.9804482596187899\n",
            "train loss:0.9124020306413932\n",
            "train loss:0.8987743160978775\n",
            "train loss:0.9242392038261482\n",
            "train loss:0.8538428540480179\n",
            "train loss:0.8414893329344437\n",
            "train loss:0.8394558094470936\n",
            "train loss:0.8349278034132099\n",
            "train loss:1.1273748175824303\n",
            "train loss:0.9612979578097633\n",
            "train loss:1.0575633660828447\n",
            "train loss:1.0456553567393234\n",
            "train loss:1.081132398104412\n",
            "train loss:1.0540892956394348\n",
            "train loss:1.0109646194388875\n",
            "train loss:0.947475069093896\n",
            "train loss:1.1351165742059217\n",
            "train loss:1.1803846247439849\n",
            "train loss:0.8428372657799053\n",
            "train loss:0.9549143454476265\n",
            "train loss:0.8970833779608367\n",
            "train loss:1.0088892213008784\n",
            "train loss:0.7834427984813022\n",
            "train loss:1.0269697981766113\n",
            "train loss:1.0726402257587058\n",
            "train loss:0.8578689677141704\n",
            "train loss:0.8404621417288312\n",
            "train loss:0.8458965830361729\n",
            "train loss:0.760753122625748\n",
            "train loss:0.8989275756314314\n",
            "train loss:0.8291524386636934\n",
            "train loss:0.9067005580685715\n",
            "train loss:0.9692383793715962\n",
            "train loss:1.0881440030474352\n",
            "=== epoch:5, train acc:0.987, test acc:0.989 ===\n",
            "train loss:0.964546284890063\n",
            "train loss:0.85924785227252\n",
            "train loss:0.8663430082274288\n",
            "train loss:1.0525677293465592\n",
            "train loss:0.9281103151421796\n",
            "train loss:1.0901109918460725\n",
            "train loss:1.0951918190188719\n",
            "train loss:0.9754241986981195\n",
            "train loss:1.0070346754537156\n",
            "train loss:1.0565610787734996\n",
            "train loss:0.7801070608856506\n",
            "train loss:1.1229246476095747\n",
            "train loss:0.9698882262150845\n",
            "train loss:0.9238090894028756\n",
            "train loss:0.9736324450732455\n",
            "train loss:0.9317804502949375\n",
            "train loss:1.2090732753586935\n",
            "train loss:0.804228910571741\n",
            "train loss:0.8643240648756597\n",
            "train loss:1.1363710253099475\n",
            "train loss:1.061961212660315\n",
            "train loss:1.0784330846723964\n",
            "train loss:1.0423621700261045\n",
            "train loss:1.0904503160545684\n",
            "train loss:0.964907902841382\n",
            "train loss:0.9184833841029895\n",
            "train loss:0.9999339298262062\n",
            "train loss:0.9144182105297509\n",
            "train loss:0.9466722207010244\n",
            "train loss:1.0039003146722871\n",
            "train loss:1.0322480487376244\n",
            "train loss:1.12973762327783\n",
            "train loss:0.974862036237763\n",
            "train loss:0.8672047962731833\n",
            "train loss:0.9644331281988415\n",
            "train loss:0.8863537275982393\n",
            "train loss:0.9830880618183515\n",
            "train loss:0.9384659919929408\n",
            "train loss:1.0206706499500733\n",
            "train loss:1.2751832055207555\n",
            "train loss:0.9300507760977693\n",
            "train loss:0.9599469129631609\n",
            "train loss:0.9086985144761495\n",
            "train loss:0.940191665422143\n",
            "train loss:0.9266256016834226\n",
            "train loss:0.8880697982597653\n",
            "train loss:1.0821485792112433\n",
            "train loss:0.8597713913881975\n",
            "train loss:0.9879707760422487\n",
            "train loss:1.0269512406960144\n",
            "train loss:0.8896636424881192\n",
            "train loss:1.1048061798809377\n",
            "train loss:0.8950892139159103\n",
            "train loss:0.9560083675325953\n",
            "train loss:0.9137574309836168\n",
            "train loss:0.8821716644957149\n",
            "train loss:0.9241919706393237\n",
            "train loss:1.0536654186547387\n",
            "train loss:1.0253665632807694\n",
            "train loss:0.832241143414026\n",
            "train loss:0.9885148745244559\n",
            "train loss:0.9312464587229815\n",
            "train loss:0.8994364953790069\n",
            "train loss:0.9286824969123674\n",
            "train loss:0.9948107551779921\n",
            "train loss:0.9066119424924763\n",
            "train loss:0.8655734067096226\n",
            "train loss:0.7928593846259214\n",
            "train loss:1.0075988813772934\n",
            "train loss:1.0551868986251838\n",
            "train loss:0.810136501136834\n",
            "train loss:1.0242356789240734\n",
            "train loss:0.9329469887415891\n",
            "train loss:1.0805422282124397\n",
            "train loss:0.7526661613185104\n",
            "train loss:0.9280658366976294\n",
            "train loss:1.0755296055145798\n",
            "train loss:0.9700753197450888\n",
            "train loss:0.9694681423813297\n",
            "train loss:1.031485719837257\n",
            "train loss:1.090181260043919\n",
            "train loss:0.8752782322296739\n",
            "train loss:0.9068579573076118\n",
            "train loss:1.0774016208510895\n",
            "train loss:1.0699679988398794\n",
            "train loss:1.0157297865918131\n",
            "train loss:0.7043691889345393\n",
            "train loss:0.9537467172789375\n",
            "train loss:0.9101846158042787\n",
            "train loss:0.8686741242580271\n",
            "train loss:0.8540812715141624\n",
            "train loss:0.9347894654792906\n",
            "train loss:0.8161039711000714\n",
            "train loss:1.0788214071537776\n",
            "train loss:0.9526373805675079\n",
            "train loss:0.9482969890902672\n",
            "train loss:0.7823253113628407\n",
            "train loss:0.8277056252630305\n",
            "train loss:1.0357457181337768\n",
            "train loss:0.9289855684742723\n",
            "train loss:0.8484544030462334\n",
            "train loss:0.8534095642592958\n",
            "train loss:0.9448662141584886\n",
            "train loss:1.066598423232596\n",
            "train loss:0.9206255558540477\n",
            "train loss:0.8990776724060444\n",
            "train loss:0.8248129629829835\n",
            "train loss:0.9855765805678258\n",
            "train loss:0.968543015711386\n",
            "train loss:0.9828670910811241\n",
            "train loss:0.7884723353534775\n",
            "train loss:0.8888955153832272\n",
            "train loss:0.8587918903230727\n",
            "train loss:0.9574130362557939\n",
            "train loss:1.0074290948969218\n",
            "train loss:0.97299592886722\n",
            "train loss:0.7434787233796679\n",
            "train loss:0.9565213153278711\n",
            "train loss:0.948844431853971\n",
            "train loss:0.8350789998344098\n",
            "train loss:0.8923208935691238\n",
            "train loss:0.9780762755699446\n",
            "train loss:0.9574624932921034\n",
            "train loss:0.8480885512615852\n",
            "train loss:1.0158583524014417\n",
            "train loss:0.925359468878697\n",
            "train loss:0.95131213649633\n",
            "train loss:0.9948252141083864\n",
            "train loss:0.9497804200349436\n",
            "train loss:0.961066577149222\n",
            "train loss:0.9541600501241583\n",
            "train loss:0.9730488989856722\n",
            "train loss:0.9729960019542295\n",
            "train loss:0.8964675985362589\n",
            "train loss:1.10070700846714\n",
            "train loss:0.9527415907174941\n",
            "train loss:0.9516445419845898\n",
            "train loss:0.7516966660272735\n",
            "train loss:0.9234635388797731\n",
            "train loss:0.9836831374716705\n",
            "train loss:1.0029229021109873\n",
            "train loss:0.8620986632263558\n",
            "train loss:0.9428017958355533\n",
            "train loss:1.124577792176911\n",
            "train loss:0.9264911529351366\n",
            "train loss:0.9859570497934258\n",
            "train loss:1.0145380189092597\n",
            "train loss:0.7542587089796376\n",
            "train loss:0.9148103574766219\n",
            "train loss:0.8711828245535795\n",
            "train loss:1.001696312648753\n",
            "train loss:0.9814408298216826\n",
            "train loss:0.9909528713477065\n",
            "train loss:0.9540991292899309\n",
            "train loss:0.9751566907456747\n",
            "train loss:1.084237458369515\n",
            "train loss:0.8483996495688454\n",
            "train loss:0.9614075090496631\n",
            "train loss:0.8243356997512066\n",
            "train loss:1.0395362768962282\n",
            "train loss:1.0771000444148524\n",
            "train loss:0.9681004134864918\n",
            "train loss:1.1233722738213274\n",
            "train loss:0.9300905671393072\n",
            "train loss:1.0019270803426152\n",
            "train loss:1.0203876505901415\n",
            "train loss:0.9632617196542753\n",
            "train loss:1.0369751898110162\n",
            "train loss:1.0798958859116348\n",
            "train loss:0.9172100547071108\n",
            "train loss:0.9421305546351705\n",
            "train loss:1.0714782369107378\n",
            "train loss:0.8796824485019696\n",
            "train loss:0.8367612358336299\n",
            "train loss:1.1128473186566552\n",
            "train loss:0.9563365931256306\n",
            "train loss:1.037568387500985\n",
            "train loss:0.7184871026361414\n",
            "train loss:0.9891827866755368\n",
            "train loss:0.9126401340714524\n",
            "train loss:1.0245709354245824\n",
            "train loss:1.087665825950653\n",
            "train loss:1.089993404614588\n",
            "train loss:1.0229179215818676\n",
            "train loss:0.9924637426750074\n",
            "train loss:1.0814937002233829\n",
            "train loss:0.8629062821503365\n",
            "train loss:0.9194418298394809\n",
            "train loss:1.0009314290859153\n",
            "train loss:0.8353035711257368\n",
            "train loss:1.086574100409069\n",
            "train loss:0.7969939160509036\n",
            "train loss:0.9933241102520003\n",
            "train loss:0.8706292385173117\n",
            "train loss:0.7756497493524462\n",
            "train loss:0.8963631021827462\n",
            "train loss:0.9209062190898734\n",
            "train loss:1.0872889894390783\n",
            "train loss:0.8044778804599418\n",
            "train loss:0.8898169745166722\n",
            "train loss:0.9708416753406901\n",
            "train loss:0.759062575325559\n",
            "train loss:0.9119411634538959\n",
            "train loss:1.0379871577718576\n",
            "train loss:0.9270102321857369\n",
            "train loss:0.8817690259604679\n",
            "train loss:1.1154877678200235\n",
            "train loss:0.9405277517456829\n",
            "train loss:0.903047058813903\n",
            "train loss:0.851304412205895\n",
            "train loss:0.8393036220566179\n",
            "train loss:0.9126719140274517\n",
            "train loss:1.080637476487656\n",
            "train loss:0.994324986799466\n",
            "train loss:0.8744235219245846\n",
            "train loss:0.9649803758194566\n",
            "train loss:0.9274577052952195\n",
            "train loss:0.8130687835914787\n",
            "train loss:0.9064777321639904\n",
            "train loss:0.895717808694959\n",
            "train loss:0.9874937833070141\n",
            "train loss:0.9067975169809789\n",
            "train loss:0.9088266015468001\n",
            "train loss:0.834386670024475\n",
            "train loss:1.0474733380548782\n",
            "train loss:1.0407687882989456\n",
            "train loss:0.942844367176464\n",
            "train loss:0.9734855186397796\n",
            "train loss:0.9604894532069848\n",
            "train loss:1.0280501221599516\n",
            "train loss:0.9465223020290988\n",
            "train loss:0.7434798276047254\n",
            "train loss:0.8451143824671863\n",
            "train loss:0.8830517103566037\n",
            "train loss:1.0194348398870985\n",
            "train loss:1.004373271485089\n",
            "train loss:0.8678707300958733\n",
            "train loss:0.9489818815948732\n",
            "train loss:0.9021079013653446\n",
            "train loss:0.9273683180375092\n",
            "train loss:1.050311950520958\n",
            "train loss:0.9992535815442478\n",
            "train loss:0.9281096694024537\n",
            "train loss:1.0635157746961363\n",
            "train loss:1.007644772482918\n",
            "train loss:0.9197752785894283\n",
            "train loss:0.9864318967956346\n",
            "train loss:0.887086571157836\n",
            "train loss:1.1533328360286306\n",
            "train loss:1.0667308555792352\n",
            "train loss:0.7659346520884083\n",
            "train loss:1.0183659880489058\n",
            "train loss:0.9106537103977498\n",
            "train loss:0.8031211228074241\n",
            "train loss:1.009383741709356\n",
            "train loss:0.8568705286613998\n",
            "train loss:0.8692949379860342\n",
            "train loss:0.8204476529986241\n",
            "train loss:0.8856655548936778\n",
            "train loss:1.0123121156333328\n",
            "train loss:0.6770283054563775\n",
            "train loss:0.8174411594001529\n",
            "train loss:1.0529469070339428\n",
            "train loss:0.9077981855275621\n",
            "train loss:0.804643743087598\n",
            "train loss:0.7466570968608943\n",
            "train loss:0.6996936155994607\n",
            "train loss:1.0721655979169262\n",
            "train loss:0.9567314502036396\n",
            "train loss:0.8011223751257193\n",
            "train loss:0.9450785583388509\n",
            "train loss:0.910540196730035\n",
            "train loss:0.8893844899469913\n",
            "train loss:0.8574727790283104\n",
            "train loss:1.000253816238278\n",
            "train loss:0.8268197104048153\n",
            "train loss:0.7993033788944628\n",
            "train loss:0.8412740894638432\n",
            "train loss:1.0120068898954382\n",
            "train loss:0.914738399124049\n",
            "train loss:0.9380980985173203\n",
            "train loss:0.8785191018191439\n",
            "train loss:0.9358999642498459\n",
            "train loss:0.8127499625893988\n",
            "train loss:1.0069802170830175\n",
            "train loss:0.9108592921034151\n",
            "train loss:0.9396239693851102\n",
            "train loss:1.1464545105375334\n",
            "train loss:1.0180532394970865\n",
            "train loss:0.8028500911333617\n",
            "train loss:0.9088640978395405\n",
            "train loss:1.005900763198706\n",
            "train loss:1.1019705350923144\n",
            "train loss:0.8088453538877352\n",
            "train loss:0.9006141432764271\n",
            "train loss:1.0304171607443855\n",
            "train loss:1.055683541432908\n",
            "train loss:1.0464764157847886\n",
            "train loss:1.0424208993272561\n",
            "train loss:0.8250489298088414\n",
            "train loss:1.0365299037844977\n",
            "train loss:0.9344800678862508\n",
            "train loss:0.9009391160059992\n",
            "train loss:0.6924725915477505\n",
            "train loss:0.8062292017225373\n",
            "train loss:0.9885957419912912\n",
            "train loss:0.8498578838618522\n",
            "train loss:0.9512949414653388\n",
            "train loss:0.8998131762627135\n",
            "train loss:1.1104005539891701\n",
            "train loss:0.8773048242335538\n",
            "train loss:1.064382043456467\n",
            "train loss:0.8934151590081494\n",
            "train loss:0.8296184658489727\n",
            "train loss:0.7975299067536136\n",
            "train loss:1.0084410190183215\n",
            "train loss:0.8255015628461817\n",
            "train loss:0.9322858758164911\n",
            "train loss:0.8759931512580995\n",
            "train loss:1.0046071645538013\n",
            "train loss:0.9885089356201402\n",
            "train loss:0.7866941377274871\n",
            "train loss:1.1085988412401333\n",
            "train loss:1.0620832267381266\n",
            "train loss:0.9375428363414688\n",
            "train loss:0.9369890271657519\n",
            "train loss:1.0127075672186066\n",
            "train loss:0.811176597832452\n",
            "train loss:0.8863190416401785\n",
            "train loss:0.9131909384243657\n",
            "train loss:0.9676041146576911\n",
            "train loss:1.2009194394972724\n",
            "train loss:0.880401606903483\n",
            "train loss:0.9843864185605392\n",
            "train loss:1.0249443266950886\n",
            "train loss:1.1018601160204\n",
            "train loss:0.7909560715989937\n",
            "train loss:1.0807166488589548\n",
            "train loss:0.9439024400478349\n",
            "train loss:0.933409631784196\n",
            "train loss:1.0220822785913815\n",
            "train loss:0.9588640816725399\n",
            "train loss:0.871379517268645\n",
            "train loss:0.959050919660194\n",
            "train loss:0.9323297190222357\n",
            "train loss:0.7800896513552664\n",
            "train loss:0.8653626728513093\n",
            "train loss:0.8840438144548477\n",
            "train loss:1.061467887160211\n",
            "train loss:0.8829208872847104\n",
            "train loss:0.9261816251934448\n",
            "train loss:0.8428516987191265\n",
            "train loss:0.8052169565622279\n",
            "train loss:1.0566572638518208\n",
            "train loss:0.7699825435846952\n",
            "train loss:0.9916465572355895\n",
            "train loss:1.0604503103807625\n",
            "train loss:1.1803776318395633\n",
            "train loss:0.9080685439169639\n",
            "train loss:0.8491337393313052\n",
            "train loss:0.8483925865539768\n",
            "train loss:0.9224161220296253\n",
            "train loss:0.9547892197620456\n",
            "train loss:0.8732938631545791\n",
            "train loss:0.8010776298737735\n",
            "train loss:0.9417860142208856\n",
            "train loss:1.1065488016977802\n",
            "train loss:0.9270441071257739\n",
            "train loss:0.9086543697185244\n",
            "train loss:0.9505417070538142\n",
            "train loss:0.7063560823500222\n",
            "train loss:0.9826918686622418\n",
            "train loss:0.8207927498323541\n",
            "train loss:0.8089337529445322\n",
            "train loss:0.8314135156886242\n",
            "train loss:0.7805984103303365\n",
            "train loss:0.9946976066273484\n",
            "train loss:0.8436408347807273\n",
            "train loss:1.0604912760356096\n",
            "train loss:0.8201451979251572\n",
            "train loss:1.0266465520278416\n",
            "train loss:0.9046010627860709\n",
            "train loss:0.9182796431078625\n",
            "train loss:0.8134217087368641\n",
            "train loss:0.899040868724488\n",
            "train loss:0.774382980866948\n",
            "train loss:1.0650601378828854\n",
            "train loss:0.8718179339415589\n",
            "train loss:0.9285787608537849\n",
            "train loss:0.9365039747615923\n",
            "train loss:0.9375728917470028\n",
            "train loss:0.905709203607331\n",
            "train loss:1.0425768697371856\n",
            "train loss:0.8393572571969729\n",
            "train loss:0.8415418885216646\n",
            "train loss:0.8874991063717188\n",
            "train loss:0.9322520370371347\n",
            "train loss:0.876777737698934\n",
            "train loss:1.000561043616011\n",
            "train loss:0.9921849501730996\n",
            "train loss:0.9581546244117143\n",
            "train loss:1.0506241670338834\n",
            "train loss:0.9489680008998193\n",
            "train loss:0.9760933309129054\n",
            "train loss:0.9088964029792602\n",
            "train loss:1.011967253939195\n",
            "train loss:0.9716072907050173\n",
            "train loss:1.0450560100444024\n",
            "train loss:0.8956837676733848\n",
            "train loss:0.8138789205701863\n",
            "train loss:0.8885890614378452\n",
            "train loss:1.0368673917164906\n",
            "train loss:0.875958699353246\n",
            "train loss:1.0164226523720918\n",
            "train loss:0.7742253861135964\n",
            "train loss:0.9543342393031219\n",
            "train loss:0.8294360133901535\n",
            "train loss:0.9260987554301012\n",
            "train loss:0.9344696604934828\n",
            "train loss:0.9053131905173791\n",
            "train loss:0.8572676821499173\n",
            "train loss:1.034803808644398\n",
            "train loss:0.8854811613094893\n",
            "train loss:0.9932648798037461\n",
            "train loss:1.076829617193188\n",
            "train loss:1.1010942090091373\n",
            "train loss:0.8028929017343431\n",
            "train loss:0.9992043663088879\n",
            "train loss:0.9433952837021802\n",
            "train loss:1.0202860334396338\n",
            "train loss:0.9774948335802982\n",
            "train loss:0.8795320194292622\n",
            "train loss:1.071442511066086\n",
            "train loss:0.828375616165332\n",
            "train loss:0.8589580880621726\n",
            "train loss:0.888695855310684\n",
            "train loss:0.9178062870673799\n",
            "train loss:0.7301278240946677\n",
            "train loss:0.9288996225033509\n",
            "train loss:0.859156648095929\n",
            "train loss:1.1068805742600432\n",
            "train loss:0.957880585493696\n",
            "train loss:1.0369720379425909\n",
            "train loss:0.92935720960817\n",
            "train loss:0.944676583526971\n",
            "train loss:0.9943464186150696\n",
            "train loss:1.071097851570835\n",
            "train loss:0.820934152464495\n",
            "train loss:0.9741545425717888\n",
            "train loss:0.9228242650169264\n",
            "train loss:0.8736578225553524\n",
            "train loss:0.958771510380291\n",
            "train loss:0.7734367508737853\n",
            "train loss:0.9948607279687317\n",
            "train loss:0.9354199739176413\n",
            "train loss:0.9092351891602403\n",
            "train loss:0.9744228037420827\n",
            "train loss:0.9169344788490088\n",
            "train loss:0.9705614952818523\n",
            "train loss:1.0115406859669855\n",
            "train loss:0.8748497830400259\n",
            "train loss:0.8844001375589811\n",
            "train loss:0.98031142810446\n",
            "train loss:1.0777636555853356\n",
            "train loss:0.9819544957057442\n",
            "train loss:0.8334388097057992\n",
            "train loss:0.988142966809464\n",
            "train loss:0.8213513939824356\n",
            "train loss:1.0537726326089754\n",
            "train loss:1.0433666143792464\n",
            "train loss:0.7701125046109195\n",
            "train loss:0.9385897054980732\n",
            "train loss:1.0270653517901402\n",
            "train loss:0.8841553690228378\n",
            "train loss:0.9005531447186977\n",
            "train loss:1.0848543857713286\n",
            "train loss:1.017332958974614\n",
            "train loss:0.8756122159400238\n",
            "train loss:0.9737841578200499\n",
            "train loss:0.918263911084766\n",
            "train loss:1.1608660550996548\n",
            "train loss:0.9571150508461101\n",
            "train loss:0.8344356560284654\n",
            "train loss:0.8854190372403176\n",
            "train loss:1.0240539336170902\n",
            "train loss:1.0117900331602843\n",
            "train loss:0.9892030523695277\n",
            "train loss:0.903464064895618\n",
            "train loss:0.9616192779060694\n",
            "train loss:0.9688342989706143\n",
            "train loss:1.0299661387536505\n",
            "train loss:0.8571016983355492\n",
            "train loss:0.9197405795198008\n",
            "train loss:0.8335750043290229\n",
            "train loss:0.9071347777870784\n",
            "train loss:0.7512230660700387\n",
            "train loss:0.9837397492724813\n",
            "train loss:0.737751459335167\n",
            "train loss:0.9115205059280176\n",
            "train loss:0.9200225361264072\n",
            "train loss:0.9583948479591584\n",
            "train loss:0.812693579204702\n",
            "train loss:0.8937945882689011\n",
            "train loss:0.8397029298798802\n",
            "train loss:1.0299204424717403\n",
            "train loss:0.9866713511154426\n",
            "train loss:0.8303045762088067\n",
            "train loss:0.7805403495021243\n",
            "train loss:0.9806695256356168\n",
            "train loss:1.01286527320192\n",
            "train loss:1.0177215722021056\n",
            "train loss:1.0144456980924632\n",
            "train loss:1.1108357285609574\n",
            "train loss:0.8780518876658958\n",
            "train loss:0.7179764806164696\n",
            "train loss:0.8040019042145307\n",
            "train loss:0.9434266141597711\n",
            "train loss:0.8653084790555948\n",
            "train loss:0.8719317961660623\n",
            "train loss:0.9835440130650842\n",
            "train loss:1.118442024053461\n",
            "train loss:0.781707235927113\n",
            "train loss:0.9379739326427813\n",
            "train loss:0.812593708480434\n",
            "train loss:0.8658959650421183\n",
            "train loss:0.8286647971863169\n",
            "train loss:1.0379107984725449\n",
            "train loss:0.677665255382648\n",
            "train loss:1.0668405438826989\n",
            "train loss:1.058753391033087\n",
            "train loss:1.0656358210645072\n",
            "train loss:1.0051853377728615\n",
            "train loss:1.1205254965634142\n",
            "train loss:0.9539271327694105\n",
            "train loss:0.9470983285815668\n",
            "train loss:0.9767892694074974\n",
            "train loss:0.8394188159660678\n",
            "train loss:0.8653025018368525\n",
            "train loss:0.9749256678651465\n",
            "train loss:0.899887430926998\n",
            "train loss:0.7718253803653262\n",
            "train loss:0.8959159115647176\n",
            "train loss:0.8952641830772903\n",
            "train loss:0.9627801535581768\n",
            "train loss:0.8666102247767786\n",
            "train loss:0.8218835209619911\n",
            "train loss:0.9659485541604653\n",
            "train loss:0.8696793720441097\n",
            "train loss:0.7799117495764822\n",
            "train loss:0.9376702977417195\n",
            "train loss:0.7379334538217874\n",
            "train loss:0.8114722381608139\n",
            "train loss:0.9341094574528072\n",
            "train loss:0.7640626005501236\n",
            "train loss:1.039176493819409\n",
            "train loss:0.8519102792595244\n",
            "train loss:0.9207251624333651\n",
            "train loss:1.058469115928774\n",
            "train loss:0.9659779638732985\n",
            "train loss:0.8607192050782376\n",
            "train loss:0.8957004572938332\n",
            "train loss:0.9250806291481513\n",
            "train loss:0.9968577379123552\n",
            "train loss:0.8952032650900528\n",
            "train loss:0.9051358431021146\n",
            "train loss:0.9826309419115595\n",
            "train loss:0.8509078249905877\n",
            "train loss:0.9697534194768633\n",
            "train loss:0.873673839097938\n",
            "train loss:0.938894283746862\n",
            "train loss:1.0092948644795185\n",
            "train loss:0.9065453214556812\n",
            "train loss:0.8648099719717156\n",
            "train loss:0.921095479132055\n",
            "train loss:0.8243149728456519\n",
            "train loss:1.020693396232964\n",
            "train loss:0.9234587879645139\n",
            "train loss:0.8099452357445055\n",
            "train loss:0.9692675961267689\n",
            "train loss:0.9512719008650208\n",
            "train loss:1.0726487838022662\n",
            "train loss:0.934453779699352\n",
            "train loss:0.6970433229273599\n",
            "train loss:0.8025470517904845\n",
            "train loss:0.9969360471624006\n",
            "train loss:0.9786558115859164\n",
            "train loss:1.0334997074929766\n",
            "train loss:1.1723210670662996\n",
            "train loss:1.0660074897573424\n",
            "train loss:1.0142062014476745\n",
            "train loss:0.8706995598656817\n",
            "train loss:0.9926260857444135\n",
            "train loss:0.9181723275521466\n",
            "train loss:1.0309922832576697\n",
            "train loss:0.8956994167272745\n",
            "train loss:0.8752399387980199\n",
            "train loss:0.7663957977448013\n",
            "train loss:0.879993643739142\n",
            "train loss:1.0113738989739243\n",
            "train loss:0.8792749209821001\n",
            "=== epoch:6, train acc:0.988, test acc:0.992 ===\n",
            "train loss:0.7560758193447468\n",
            "train loss:0.8218705024766939\n",
            "train loss:0.9363120832548963\n",
            "train loss:1.0390048768235085\n",
            "train loss:1.0025405071314117\n",
            "train loss:0.9656718005198688\n",
            "train loss:1.1322459802394174\n",
            "train loss:0.8678659919275782\n",
            "train loss:1.2550448736641855\n",
            "train loss:0.8933763760225435\n",
            "train loss:0.9602897405197102\n",
            "train loss:0.9350915971201679\n",
            "train loss:0.9146274951947917\n",
            "train loss:0.9204122832812187\n",
            "train loss:0.8992663691319407\n",
            "train loss:0.9290345492744784\n",
            "train loss:0.8708860505790317\n",
            "train loss:0.9843169722789276\n",
            "train loss:0.8519057310533511\n",
            "train loss:0.8876007551444922\n",
            "train loss:0.8984561149366788\n",
            "train loss:0.8518092639727023\n",
            "train loss:0.9879406473284673\n",
            "train loss:0.8969371729621455\n",
            "train loss:1.0563235351877138\n",
            "train loss:0.9548922567071467\n",
            "train loss:0.9476620679903442\n",
            "train loss:0.8899671997439071\n",
            "train loss:0.9046905755943854\n",
            "train loss:0.805094821875868\n",
            "train loss:0.8958168978565648\n",
            "train loss:0.7939446361200524\n",
            "train loss:0.940034205108547\n",
            "train loss:1.0274726888791954\n",
            "train loss:0.9711244567986781\n",
            "train loss:0.9280990768104338\n",
            "train loss:0.9858128184455934\n",
            "train loss:1.0092502811666797\n",
            "train loss:0.8571556888367824\n",
            "train loss:0.9323210770703261\n",
            "train loss:0.9468049358615016\n",
            "train loss:1.0333074057834748\n",
            "train loss:0.8683471261631931\n",
            "train loss:1.1718781678546377\n",
            "train loss:1.03036870412111\n",
            "train loss:0.8100778159413493\n",
            "train loss:0.9038845418066357\n",
            "train loss:1.0366256587577085\n",
            "train loss:0.9982885900570453\n",
            "train loss:0.9340797808727989\n",
            "train loss:0.8019540726365776\n",
            "train loss:0.9674966662038365\n",
            "train loss:0.8391865208735296\n",
            "train loss:0.8739451473316473\n",
            "train loss:0.7947832819131803\n",
            "train loss:0.9237755745596278\n",
            "train loss:0.800633633414572\n",
            "train loss:0.8634482789363781\n",
            "train loss:0.9818376575040132\n",
            "train loss:1.014557806234557\n",
            "train loss:0.8549945013175982\n",
            "train loss:0.919957332463436\n",
            "train loss:0.8593664160199483\n",
            "train loss:0.9080297772385358\n",
            "train loss:0.9127588182386908\n",
            "train loss:1.0991370544224397\n",
            "train loss:0.8511656159496009\n",
            "train loss:0.8255444616425679\n",
            "train loss:0.9582958370079591\n",
            "train loss:0.8966296350313905\n",
            "train loss:1.0249600591229011\n",
            "train loss:0.9229863270406411\n",
            "train loss:1.1281820226373724\n",
            "train loss:0.9445763110556398\n",
            "train loss:0.8209773524279089\n",
            "train loss:0.9727602144395021\n",
            "train loss:1.01204418401235\n",
            "train loss:0.8942599069620145\n",
            "train loss:0.7802613176382641\n",
            "train loss:0.9891863273925722\n",
            "train loss:0.9262678297852086\n",
            "train loss:0.8458626589275198\n",
            "train loss:0.8667419482085228\n",
            "train loss:0.8583186944347707\n",
            "train loss:0.9491723403475935\n",
            "train loss:0.8689453765432427\n",
            "train loss:0.857020373050714\n",
            "train loss:0.9623389399597277\n",
            "train loss:0.8057069832445033\n",
            "train loss:0.9757025129345446\n",
            "train loss:0.8042263661337858\n",
            "train loss:0.994434691540675\n",
            "train loss:1.075652784733926\n",
            "train loss:1.0496516681044945\n",
            "train loss:0.9762036995950851\n",
            "train loss:0.9403112408885737\n",
            "train loss:1.048667588972802\n",
            "train loss:0.902379383789853\n",
            "train loss:0.895245284930856\n",
            "train loss:1.0864040834543238\n",
            "train loss:0.7541976022135582\n",
            "train loss:0.8604212323610754\n",
            "train loss:1.0182953128547112\n",
            "train loss:0.884302621936113\n",
            "train loss:1.018928930186346\n",
            "train loss:1.0099383964200974\n",
            "train loss:0.985665772622014\n",
            "train loss:0.8117877981708601\n",
            "train loss:0.9116612498741715\n",
            "train loss:0.8100012900543679\n",
            "train loss:0.9556370443393462\n",
            "train loss:0.9653903595499043\n",
            "train loss:0.9687157858169\n",
            "train loss:0.8412909255029046\n",
            "train loss:0.9106786485077342\n",
            "train loss:1.1025648937480528\n",
            "train loss:0.8393095364069972\n",
            "train loss:0.9839295769463087\n",
            "train loss:1.0037387434548262\n",
            "train loss:0.9248512455981139\n",
            "train loss:1.0697169682740024\n",
            "train loss:0.9192148012670488\n",
            "train loss:0.8027068183102269\n",
            "train loss:0.8926091539111681\n",
            "train loss:0.8550457717653025\n",
            "train loss:1.0565450983405291\n",
            "train loss:0.8850513040178032\n",
            "train loss:0.982602038622163\n",
            "train loss:0.955173517169831\n",
            "train loss:0.9241277158600831\n",
            "train loss:1.0145437918492215\n",
            "train loss:0.7782618562280519\n",
            "train loss:0.819120259097002\n",
            "train loss:1.034512921311226\n",
            "train loss:1.010955655706123\n",
            "train loss:0.8942908006117026\n",
            "train loss:0.9925240158729223\n",
            "train loss:1.0237569335036485\n",
            "train loss:1.001967671172518\n",
            "train loss:0.8881170107018234\n",
            "train loss:0.7612138081623907\n",
            "train loss:0.9962135565225638\n",
            "train loss:0.9310042126512834\n",
            "train loss:0.9774527354132845\n",
            "train loss:0.9845903358093495\n",
            "train loss:0.9555187590822907\n",
            "train loss:1.012544634754908\n",
            "train loss:0.8294274222310769\n",
            "train loss:0.9598369301110768\n",
            "train loss:0.9308520200024664\n",
            "train loss:0.941328423870152\n",
            "train loss:0.8870628081675314\n",
            "train loss:0.913832462363724\n",
            "train loss:0.7999378450829469\n",
            "train loss:0.9666257574881925\n",
            "train loss:0.9862084161709262\n",
            "train loss:0.9960575798727284\n",
            "train loss:0.9116464693448243\n",
            "train loss:0.8567117499327486\n",
            "train loss:0.8388745987187219\n",
            "train loss:0.9780561326395866\n",
            "train loss:0.9768408998832644\n",
            "train loss:0.7594745753237127\n",
            "train loss:0.7583176999520319\n",
            "train loss:1.054849182031326\n",
            "train loss:0.8447453984640005\n",
            "train loss:0.9400312905332733\n",
            "train loss:0.7279624731778128\n",
            "train loss:0.978387237034616\n",
            "train loss:0.9191526610569302\n",
            "train loss:0.9690110907346208\n",
            "train loss:1.0651209347963415\n",
            "train loss:0.9439302980140956\n",
            "train loss:0.8279076091894558\n",
            "train loss:0.8996530243895527\n",
            "train loss:0.8392627003920384\n",
            "train loss:1.09386225776872\n",
            "train loss:0.934756003076464\n",
            "train loss:1.0521866743578931\n",
            "train loss:0.8912087018259754\n",
            "train loss:1.0876090214432184\n",
            "train loss:1.0574755554040096\n",
            "train loss:0.8701528141176074\n",
            "train loss:0.9215500399382907\n",
            "train loss:0.8998844236828062\n",
            "train loss:0.8350230439811412\n",
            "train loss:0.8952392691656081\n",
            "train loss:1.0437849409018034\n",
            "train loss:0.9137979945262248\n",
            "train loss:0.9410387595410882\n",
            "train loss:1.0705377288896676\n",
            "train loss:0.8925369127351551\n",
            "train loss:0.8069556027462268\n",
            "train loss:1.0257846427945294\n",
            "train loss:0.9988090265136682\n",
            "train loss:0.801778003880525\n",
            "train loss:1.0143974770045567\n",
            "train loss:1.0470270858429775\n",
            "train loss:0.9333820271049413\n",
            "train loss:0.8538873342443993\n",
            "train loss:0.9116698968879291\n",
            "train loss:0.8290987279272203\n",
            "train loss:0.9214098941489552\n",
            "train loss:1.0608985265261512\n",
            "train loss:0.8628035330427776\n",
            "train loss:0.77624123017018\n",
            "train loss:0.9614319400384445\n",
            "train loss:0.8137027571793481\n",
            "train loss:0.7931762118858507\n",
            "train loss:0.973474893782933\n",
            "train loss:0.9508066901352064\n",
            "train loss:0.8678756600006804\n",
            "train loss:0.8977194737807928\n",
            "train loss:0.8046111431811327\n",
            "train loss:0.9122924533080347\n",
            "train loss:0.8330257760246088\n",
            "train loss:0.8217336034607459\n",
            "train loss:0.8216029598757352\n",
            "train loss:0.9009527732277057\n",
            "train loss:0.8693149319690212\n",
            "train loss:0.922355426704991\n",
            "train loss:0.9200065036193014\n",
            "train loss:0.7928638053856001\n",
            "train loss:1.0161786843062195\n",
            "train loss:0.9168994554599977\n",
            "train loss:1.0163589970659916\n",
            "train loss:0.9717609079236632\n",
            "train loss:1.1122153743634318\n",
            "train loss:0.8971778443114721\n",
            "train loss:1.0789556169016326\n",
            "train loss:0.7798488866432023\n",
            "train loss:1.0175619339099873\n",
            "train loss:0.8551099217451725\n",
            "train loss:0.8807306238128403\n",
            "train loss:0.8979129910628348\n",
            "train loss:0.8735856636876441\n",
            "train loss:0.9510461070031755\n",
            "train loss:1.0029289786269762\n",
            "train loss:1.0113134865958862\n",
            "train loss:0.9812395465165222\n",
            "train loss:0.8400808359697263\n",
            "train loss:0.8818421982314966\n",
            "train loss:0.8357823221408321\n",
            "train loss:1.0371519606305597\n",
            "train loss:1.0181251994804332\n",
            "train loss:1.0373961200679236\n",
            "train loss:0.8676341889398216\n",
            "train loss:0.965369265055419\n",
            "train loss:0.8038807245953695\n",
            "train loss:1.0089931706507278\n",
            "train loss:0.9713171291503082\n",
            "train loss:0.8538022025906055\n",
            "train loss:0.9923546620852032\n",
            "train loss:0.79048470349613\n",
            "train loss:0.9184574359801124\n",
            "train loss:0.8135209375612695\n",
            "train loss:0.8749660374977323\n",
            "train loss:0.8305756529999706\n",
            "train loss:0.8992458928751909\n",
            "train loss:0.9180059755739552\n",
            "train loss:1.105263063316705\n",
            "train loss:0.889576652207327\n",
            "train loss:0.8253787590610767\n",
            "train loss:0.9752254857616397\n",
            "train loss:0.8756565002116894\n",
            "train loss:1.0478733729336458\n",
            "train loss:1.0030511351160118\n",
            "train loss:1.0981847379761158\n",
            "train loss:0.8599572704531668\n",
            "train loss:0.8307938354570287\n",
            "train loss:0.7720827260818857\n",
            "train loss:0.8600890063787707\n",
            "train loss:0.8485959905576965\n",
            "train loss:1.015567105591909\n",
            "train loss:0.9493987147074021\n",
            "train loss:1.0035165641108805\n",
            "train loss:1.0050649718976048\n",
            "train loss:0.8961816483883943\n",
            "train loss:0.9467996158327477\n",
            "train loss:0.8538434607160577\n",
            "train loss:0.9025338616357367\n",
            "train loss:0.9938224673316454\n",
            "train loss:0.7207607932468316\n",
            "train loss:0.7586502677201656\n",
            "train loss:0.8346115356322628\n",
            "train loss:0.8771511548129878\n",
            "train loss:0.7978105044501724\n",
            "train loss:1.0330557611255968\n",
            "train loss:1.0358780495089532\n",
            "train loss:0.8131859119861622\n",
            "train loss:0.9199955892514989\n",
            "train loss:0.9316243252767249\n",
            "train loss:0.9125299147763143\n",
            "train loss:1.0750692319216193\n",
            "train loss:0.9176468804166293\n",
            "train loss:0.9382077051616504\n",
            "train loss:1.0620286217836217\n",
            "train loss:0.852876731131684\n",
            "train loss:0.7001665569148032\n",
            "train loss:0.8471346985840781\n",
            "train loss:0.7806795743458566\n",
            "train loss:0.8892833195937878\n",
            "train loss:0.9864635942683573\n",
            "train loss:0.8724929788348369\n",
            "train loss:0.9316483834287865\n",
            "train loss:1.0429909303322011\n",
            "train loss:0.7991998103160429\n",
            "train loss:0.9924467057788269\n",
            "train loss:0.8730197182125946\n",
            "train loss:1.0782184513681798\n",
            "train loss:0.7968404144162317\n",
            "train loss:0.9961821163145688\n",
            "train loss:0.8641225109836418\n",
            "train loss:0.7740992500679127\n",
            "train loss:1.0657336327258133\n",
            "train loss:0.9496884081958608\n",
            "train loss:1.0070662997616027\n",
            "train loss:0.8964172408677569\n",
            "train loss:0.910690943554157\n",
            "train loss:0.913405100676358\n",
            "train loss:1.032371881665676\n",
            "train loss:0.9691220307153905\n",
            "train loss:0.8636166743542694\n",
            "train loss:1.0468468213975677\n",
            "train loss:0.8768692020525788\n",
            "train loss:0.9616675754573971\n",
            "train loss:0.9229152929235811\n",
            "train loss:1.1212423681536179\n",
            "train loss:0.7696917928531974\n",
            "train loss:0.9919637592675078\n",
            "train loss:0.8119512642500674\n",
            "train loss:0.9430851401716311\n",
            "train loss:1.0821855623086696\n",
            "train loss:1.0184186642808069\n",
            "train loss:0.9691168745612343\n",
            "train loss:0.8646141729350948\n",
            "train loss:1.2238337193957511\n",
            "train loss:0.8855779787282916\n",
            "train loss:0.7910551550948823\n",
            "train loss:0.9604064763973211\n",
            "train loss:0.913651749668964\n",
            "train loss:0.9933266667867339\n",
            "train loss:1.1002554715155322\n",
            "train loss:0.8076832520391748\n",
            "train loss:0.90308222155347\n",
            "train loss:1.0561775015968238\n",
            "train loss:0.8721438207746562\n",
            "train loss:0.8291577254558894\n",
            "train loss:1.0406973102030281\n",
            "train loss:1.0731888207290148\n",
            "train loss:0.9918786319662897\n",
            "train loss:0.9293056273878526\n",
            "train loss:1.0105286865338892\n",
            "train loss:0.9971652667106803\n",
            "train loss:1.0169360389223503\n",
            "train loss:0.8409056784409489\n",
            "train loss:0.8390298040822318\n",
            "train loss:0.9426766928985307\n",
            "train loss:1.0332927308753554\n",
            "train loss:0.8275717588057176\n",
            "train loss:0.9445942177680979\n",
            "train loss:0.7781350450735326\n",
            "train loss:0.9654591970187099\n",
            "train loss:0.8935668313182622\n",
            "train loss:0.9438943143376752\n",
            "train loss:0.8153289804450127\n",
            "train loss:0.9827828832187215\n",
            "train loss:0.7729276588505419\n",
            "train loss:0.9472802397245748\n",
            "train loss:0.9151487715194976\n",
            "train loss:0.9229695706674791\n",
            "train loss:0.930800816070095\n",
            "train loss:1.056101491905401\n",
            "train loss:0.9758636632078346\n",
            "train loss:0.9789670168678251\n",
            "train loss:0.8501472747925254\n",
            "train loss:0.8510292771325338\n",
            "train loss:0.8775127076508098\n",
            "train loss:0.999914021186995\n",
            "train loss:0.5461840936487617\n",
            "train loss:0.9173552637576193\n",
            "train loss:0.9166792927840683\n",
            "train loss:0.886746580886302\n",
            "train loss:0.9547847539064814\n",
            "train loss:0.8676830009307415\n",
            "train loss:0.8778772872405327\n",
            "train loss:0.8206536349218101\n",
            "train loss:0.8458482078763601\n",
            "train loss:0.978384579694856\n",
            "train loss:0.9525980423969976\n",
            "train loss:0.917614459478924\n",
            "train loss:0.9796621696049455\n",
            "train loss:0.8936925329190831\n",
            "train loss:0.8928116408860789\n",
            "train loss:0.781813000408982\n",
            "train loss:0.6860062882242791\n",
            "train loss:0.8389766962645347\n",
            "train loss:0.8743923474895847\n",
            "train loss:0.9581220246758407\n",
            "train loss:0.9668878404242061\n",
            "train loss:0.8721802559135194\n",
            "train loss:0.8866519021555329\n",
            "train loss:0.925025660995435\n",
            "train loss:1.11394195385403\n",
            "train loss:0.9863916639365616\n",
            "train loss:0.9167585936994861\n",
            "train loss:0.853099182986678\n",
            "train loss:0.9204976872885479\n",
            "train loss:0.8912994483937967\n",
            "train loss:0.9905997467104066\n",
            "train loss:0.885386728489908\n",
            "train loss:0.8137298176750999\n",
            "train loss:0.9594109549683958\n",
            "train loss:0.7777862218064827\n",
            "train loss:0.8050888216134275\n",
            "train loss:0.9512633945716636\n",
            "train loss:0.9349018651962113\n",
            "train loss:1.0035428245512796\n",
            "train loss:0.9158083292634709\n",
            "train loss:0.8753268882661829\n",
            "train loss:0.9656688162943832\n",
            "train loss:1.0105021659508704\n",
            "train loss:0.7746415447565907\n",
            "train loss:0.9171123024044718\n",
            "train loss:0.7816646077039073\n",
            "train loss:0.9931297128380071\n",
            "train loss:0.9010517280301704\n",
            "train loss:0.7492097719739135\n",
            "train loss:0.8851580118415118\n",
            "train loss:1.075814402544256\n",
            "train loss:1.0054040338772143\n",
            "train loss:0.9351321444480276\n",
            "train loss:0.8249169839154922\n",
            "train loss:0.9793965548022048\n",
            "train loss:0.9469985459070882\n",
            "train loss:0.8954340849430025\n",
            "train loss:1.060000819971393\n",
            "train loss:0.8523413008633572\n",
            "train loss:1.0720978569227515\n",
            "train loss:0.9313409422037292\n",
            "train loss:0.7615956467888024\n",
            "train loss:0.8076126538265561\n",
            "train loss:0.9500039705135159\n",
            "train loss:1.0678424753996019\n",
            "train loss:0.9278722738731418\n",
            "train loss:0.7552466213284333\n",
            "train loss:0.9231039531630578\n",
            "train loss:0.7661140367340606\n",
            "train loss:0.9285996182453354\n",
            "train loss:0.9232011957537677\n",
            "train loss:0.7840152550315491\n",
            "train loss:0.8456252229671645\n",
            "train loss:0.8980465652818078\n",
            "train loss:0.8186542121125052\n",
            "train loss:0.8944943794417665\n",
            "train loss:1.0225479258769696\n",
            "train loss:0.9798014808401813\n",
            "train loss:0.8985886815508554\n",
            "train loss:0.9149631639604965\n",
            "train loss:0.8135341359618153\n",
            "train loss:0.8718593670077206\n",
            "train loss:1.0045940527072235\n",
            "train loss:0.9420111965966871\n",
            "train loss:0.9800075444874241\n",
            "train loss:0.9617098379695342\n",
            "train loss:0.8490255057126916\n",
            "train loss:0.9052015976762391\n",
            "train loss:0.8594878691586366\n",
            "train loss:0.9254093750426403\n",
            "train loss:0.7286682845988376\n",
            "train loss:0.8946518651114113\n",
            "train loss:1.019843362403831\n",
            "train loss:0.9073989202823755\n",
            "train loss:0.8211057419360229\n",
            "train loss:1.0674518157825978\n",
            "train loss:0.7671189083205637\n",
            "train loss:0.8527229251140773\n",
            "train loss:0.8632716100130352\n",
            "train loss:0.961517305846093\n",
            "train loss:0.8772284633403404\n",
            "train loss:0.8784083453627346\n",
            "train loss:0.893003592121274\n",
            "train loss:1.0477948831474777\n",
            "train loss:0.8464386150941897\n",
            "train loss:0.6959399850763325\n",
            "train loss:0.8880598424446188\n",
            "train loss:1.0186430657353926\n",
            "train loss:0.9605293595851971\n",
            "train loss:0.9264246204306822\n",
            "train loss:0.8276546982135088\n",
            "train loss:0.7743284300761585\n",
            "train loss:0.9764583705056304\n",
            "train loss:0.9108411479512565\n",
            "train loss:0.8841831812134937\n",
            "train loss:0.9901379634641314\n",
            "train loss:0.8369537796212272\n",
            "train loss:0.9874503099082221\n",
            "train loss:0.9423543808811978\n",
            "train loss:0.9502936410991701\n",
            "train loss:0.8583638943441853\n",
            "train loss:1.0010200535781422\n",
            "train loss:0.8502503151079602\n",
            "train loss:0.7359188142927444\n",
            "train loss:0.9704583706814567\n",
            "train loss:0.8569788401795734\n",
            "train loss:0.8041535715872747\n",
            "train loss:0.9011682563840084\n",
            "train loss:1.009753556381893\n",
            "train loss:0.8881404159686659\n",
            "train loss:1.0384407732444516\n",
            "train loss:0.8435935713710266\n",
            "train loss:1.0728390371323078\n",
            "train loss:0.8874159706445188\n",
            "train loss:0.902427015808745\n",
            "train loss:0.833434191938336\n",
            "train loss:0.9251037294722257\n",
            "train loss:0.8059090926226314\n",
            "train loss:0.8236353739597976\n",
            "train loss:0.9994451365736965\n",
            "train loss:1.0370356839825359\n",
            "train loss:1.001116391943939\n",
            "train loss:0.8305967610793046\n",
            "train loss:0.7484608462385574\n",
            "train loss:0.9322079027957427\n",
            "train loss:0.9515774272382213\n",
            "train loss:0.8873549071980099\n",
            "train loss:0.9154089340282175\n",
            "train loss:0.878134989685289\n",
            "train loss:1.0547553258418862\n",
            "train loss:0.9585010981719173\n",
            "train loss:0.9846848586291678\n",
            "train loss:0.8698361827960723\n",
            "train loss:1.0171611198292287\n",
            "train loss:0.901194136721126\n",
            "train loss:1.0311420854361848\n",
            "train loss:0.9430050547217675\n",
            "train loss:1.034915534523776\n",
            "train loss:0.7541345252288282\n",
            "train loss:0.9189174476218949\n",
            "train loss:0.9542918464430558\n",
            "train loss:0.7951135064638433\n",
            "train loss:0.9558520690980393\n",
            "train loss:0.8826428540659026\n",
            "train loss:1.008148879945218\n",
            "train loss:0.8565155749413242\n",
            "train loss:0.8629589332015546\n",
            "train loss:0.8253362382761221\n",
            "train loss:0.9701330979366456\n",
            "train loss:0.8525387932535786\n",
            "train loss:0.892979739678763\n",
            "train loss:0.8483014730981789\n",
            "train loss:1.0290293937511819\n",
            "train loss:0.86472110017463\n",
            "train loss:0.875832947495384\n",
            "train loss:0.8315793579696353\n",
            "train loss:0.8698393912530737\n",
            "train loss:0.8088050792745527\n",
            "train loss:1.0344132720767403\n",
            "train loss:0.9585133551834873\n",
            "train loss:0.9649526991300804\n",
            "train loss:0.9763662920109931\n",
            "train loss:0.8139679218930662\n",
            "train loss:1.0821877730049332\n",
            "train loss:0.8916174527753471\n",
            "train loss:0.7345315323611278\n",
            "train loss:0.8460238383735083\n",
            "train loss:0.8931947687384332\n",
            "train loss:0.9580482153202682\n",
            "train loss:0.9327939862294801\n",
            "train loss:0.9153980904199079\n",
            "train loss:0.8790155410464676\n",
            "train loss:0.9184735596221562\n",
            "train loss:0.8717081311746894\n",
            "train loss:0.9540363162040273\n",
            "train loss:0.7439830792797499\n",
            "train loss:0.9563410618357893\n",
            "train loss:0.9314820891797561\n",
            "train loss:0.9791411859919767\n",
            "train loss:0.9419428164240172\n",
            "train loss:0.9784748272889356\n",
            "train loss:0.900618086181763\n",
            "train loss:0.8453712055585278\n",
            "train loss:0.7794789509831737\n",
            "train loss:0.8586750570989352\n",
            "train loss:1.0229591345135316\n",
            "train loss:0.8340541151331842\n",
            "train loss:0.8674995382322802\n",
            "train loss:0.8822496602647018\n",
            "train loss:0.893768345857693\n",
            "train loss:0.8262130952975038\n",
            "train loss:0.855131980082199\n",
            "train loss:0.9241175579622022\n",
            "train loss:0.9399252638461735\n",
            "train loss:0.9823441968082857\n",
            "train loss:0.9105474621082066\n",
            "train loss:1.0103029800171135\n",
            "train loss:0.7713322737411246\n",
            "train loss:0.9548701371496032\n",
            "train loss:1.0516837994385664\n",
            "train loss:0.9900439004730553\n",
            "=== epoch:7, train acc:0.993, test acc:0.989 ===\n",
            "train loss:0.926017724306038\n",
            "train loss:0.9040510718294259\n",
            "train loss:0.7912175944937466\n",
            "train loss:0.794221998253006\n",
            "train loss:0.8730177632176079\n",
            "train loss:0.8883702393039634\n",
            "train loss:0.7816380239048195\n",
            "train loss:0.9247687074381945\n",
            "train loss:0.8586986930981653\n",
            "train loss:0.6995659398108935\n",
            "train loss:0.8697309368837345\n",
            "train loss:0.8717243201813375\n",
            "train loss:1.0036276433778777\n",
            "train loss:0.9767673624636793\n",
            "train loss:1.0510757533346138\n",
            "train loss:0.7960408854054534\n",
            "train loss:0.9227569405221088\n",
            "train loss:0.8110268695300807\n",
            "train loss:1.0591915178522846\n",
            "train loss:1.2136031715160738\n",
            "train loss:0.9461112364418605\n",
            "train loss:0.8734645980529572\n",
            "train loss:0.7930257287260506\n",
            "train loss:0.9238945703864689\n",
            "train loss:0.8312011020074963\n",
            "train loss:0.8794363399021208\n",
            "train loss:1.1261568963678246\n",
            "train loss:0.7841028201419834\n",
            "train loss:0.9561231550416689\n",
            "train loss:0.8887303376141538\n",
            "train loss:0.8925139130485178\n",
            "train loss:1.004726300185916\n",
            "train loss:0.9805235298136228\n",
            "train loss:0.7772371835957196\n",
            "train loss:0.9959481178604903\n",
            "train loss:0.8809110697837768\n",
            "train loss:0.8787057583607267\n",
            "train loss:0.9670025075520724\n",
            "train loss:1.0112326678204575\n",
            "train loss:0.8634055824676409\n",
            "train loss:1.0103940477546458\n",
            "train loss:0.8915122025995725\n",
            "train loss:0.9866639065224007\n",
            "train loss:0.9633707160971987\n",
            "train loss:0.9321826648483547\n",
            "train loss:0.936805715665129\n",
            "train loss:1.0289574609928245\n",
            "train loss:0.8468050595313072\n",
            "train loss:1.164800816328847\n",
            "train loss:1.076894469254019\n",
            "train loss:0.7543444064860881\n",
            "train loss:0.9175283005895128\n",
            "train loss:0.8276017634983122\n",
            "train loss:1.0496004374647918\n",
            "train loss:0.8675061688894122\n",
            "train loss:1.0050689429654178\n",
            "train loss:0.8360070796407836\n",
            "train loss:0.8474916521534207\n",
            "train loss:0.7986940458456264\n",
            "train loss:0.9445175969516504\n",
            "train loss:0.8664557927364149\n",
            "train loss:0.9688377576672713\n",
            "train loss:0.8586288726621771\n",
            "train loss:0.7750832318126086\n",
            "train loss:1.1138903066580053\n",
            "train loss:0.8117870644036885\n",
            "train loss:1.016634023227646\n",
            "train loss:0.7536474298261626\n",
            "train loss:0.7880695824173497\n",
            "train loss:0.9702532474142531\n",
            "train loss:1.0473069142845362\n",
            "train loss:1.0271760847486509\n",
            "train loss:0.7011637210668372\n",
            "train loss:0.8238196509080318\n",
            "train loss:0.9345990401653402\n",
            "train loss:0.7756856350431138\n",
            "train loss:0.8858646487089823\n",
            "train loss:1.046358963516649\n",
            "train loss:1.1305524613233553\n",
            "train loss:0.9445750467113627\n",
            "train loss:0.9095700154093223\n",
            "train loss:0.8443993845664273\n",
            "train loss:0.9293093601210785\n",
            "train loss:0.9543790466932827\n",
            "train loss:0.8287714125077993\n",
            "train loss:0.8303555193264641\n",
            "train loss:0.8546845583129636\n",
            "train loss:0.7834476180950527\n",
            "train loss:0.9096350756540069\n",
            "train loss:1.0479575599059356\n",
            "train loss:0.7895942859926702\n",
            "train loss:0.9391243907513837\n",
            "train loss:0.8604029113310234\n",
            "train loss:0.9981949162373509\n",
            "train loss:0.8662276228244159\n",
            "train loss:0.8940874331112725\n",
            "train loss:0.8579528826952871\n",
            "train loss:0.8936273317075221\n",
            "train loss:0.7447668237025172\n",
            "train loss:0.8314486515491288\n",
            "train loss:0.8144248990164646\n",
            "train loss:0.9294382345897307\n",
            "train loss:0.8930524110781468\n",
            "train loss:1.0076735283986813\n",
            "train loss:0.8080360692176535\n",
            "train loss:0.8026368285945976\n",
            "train loss:1.0036667446047802\n",
            "train loss:1.0514218933480297\n",
            "train loss:0.9389067659408562\n",
            "train loss:0.8753457341747773\n",
            "train loss:0.9710288880061257\n",
            "train loss:0.8677870508855711\n",
            "train loss:0.9423511664254896\n",
            "train loss:0.7922469391925036\n",
            "train loss:1.0242826533133218\n",
            "train loss:0.7636217105909499\n",
            "train loss:0.8467589617581137\n",
            "train loss:0.9389604356874952\n",
            "train loss:0.8735251110781027\n",
            "train loss:1.0683311320392581\n",
            "train loss:0.9163438229738163\n",
            "train loss:0.8334360702065148\n",
            "train loss:0.9835481993232962\n",
            "train loss:0.8683297437337676\n",
            "train loss:0.800392259808275\n",
            "train loss:0.8234579504865563\n",
            "train loss:1.0590619479143317\n",
            "train loss:0.8995071709323059\n",
            "train loss:1.1529827722157806\n",
            "train loss:0.9159563446002013\n",
            "train loss:0.8183456634723585\n",
            "train loss:0.9818124201426446\n",
            "train loss:0.9705782270395321\n",
            "train loss:0.8965275147408209\n",
            "train loss:0.8659955267313117\n",
            "train loss:0.9431004189001316\n",
            "train loss:0.8270090509781466\n",
            "train loss:0.8127922023582951\n",
            "train loss:0.8751835846618612\n",
            "train loss:0.8560843471276672\n",
            "train loss:0.9501510998388318\n",
            "train loss:0.7429627061279027\n",
            "train loss:0.7981881482888811\n",
            "train loss:0.8463181253203571\n",
            "train loss:1.1306795108047962\n",
            "train loss:0.7668356435498382\n",
            "train loss:0.9017116623553366\n",
            "train loss:0.8215328298842084\n",
            "train loss:0.9725839979632853\n",
            "train loss:0.9663969356341846\n",
            "train loss:0.9401868079554115\n",
            "train loss:0.8175155881559744\n",
            "train loss:0.716086887285429\n",
            "train loss:0.9825284858203865\n",
            "train loss:0.9598407881718485\n",
            "train loss:0.790021945185977\n",
            "train loss:1.0326421890672213\n",
            "train loss:0.8725917183170133\n",
            "train loss:0.9248868720708294\n",
            "train loss:0.9084151452458545\n",
            "train loss:0.9517247909965292\n",
            "train loss:0.9053810766978474\n",
            "train loss:0.9197469002510129\n",
            "train loss:0.9629835763279614\n",
            "train loss:0.7436172456301849\n",
            "train loss:0.9094182477472207\n",
            "train loss:0.9288013220503011\n",
            "train loss:0.8211481925351792\n",
            "train loss:0.8262782711517507\n",
            "train loss:1.188932206796839\n",
            "train loss:0.9130599853358029\n",
            "train loss:0.9445932919130824\n",
            "train loss:1.043042781931308\n",
            "train loss:0.748079853013701\n",
            "train loss:0.8563425056403454\n",
            "train loss:0.9448185683367422\n",
            "train loss:0.9071284925965264\n",
            "train loss:0.8607932516604478\n",
            "train loss:1.2513496449768595\n",
            "train loss:0.6532701773687447\n",
            "train loss:0.9734183966119321\n",
            "train loss:0.9499462684263633\n",
            "train loss:0.7866146783864981\n",
            "train loss:1.0266144968614215\n",
            "train loss:0.8892465758135211\n",
            "train loss:0.9121735297299368\n",
            "train loss:0.8527304736011244\n",
            "train loss:0.7657794439923119\n",
            "train loss:0.8899685905692336\n",
            "train loss:0.9615855492344474\n",
            "train loss:0.9183506610713854\n",
            "train loss:0.9762051590478049\n",
            "train loss:1.0796377032763866\n",
            "train loss:0.8460435129705278\n",
            "train loss:0.9417947476347099\n",
            "train loss:0.7792857131930861\n",
            "train loss:0.7565375733255008\n",
            "train loss:0.8432937339754218\n",
            "train loss:0.7737345320317985\n",
            "train loss:0.7881515113329685\n",
            "train loss:0.760231005974838\n",
            "train loss:0.9660886982035335\n",
            "train loss:0.9176522790089181\n",
            "train loss:0.895674681986304\n",
            "train loss:0.9556479462315888\n",
            "train loss:0.7234606391042848\n",
            "train loss:0.8040730341887468\n",
            "train loss:0.911917260523929\n",
            "train loss:1.0985519139312336\n",
            "train loss:0.9898427580819888\n",
            "train loss:0.885018947213607\n",
            "train loss:1.1114684156340244\n",
            "train loss:1.1729316315418117\n",
            "train loss:1.0608164313241313\n",
            "train loss:0.8878521034885813\n",
            "train loss:0.6994604159279121\n",
            "train loss:0.8872471334571563\n",
            "train loss:0.9805149439503235\n",
            "train loss:0.9349276345396232\n",
            "train loss:0.8310667897206782\n",
            "train loss:0.8815478005881845\n",
            "train loss:0.8569640414885545\n",
            "train loss:0.86842506310745\n",
            "train loss:1.087124627325036\n",
            "train loss:0.9119535030381004\n",
            "train loss:0.8894322334831736\n",
            "train loss:0.9907158247923528\n",
            "train loss:0.917241898508556\n",
            "train loss:0.7334194993541697\n",
            "train loss:1.0048880738208863\n",
            "train loss:0.9509398348912593\n",
            "train loss:0.8957670443195431\n",
            "train loss:0.9832165994088641\n",
            "train loss:0.7784536255160576\n",
            "train loss:1.1844241102976023\n",
            "train loss:0.8970181162905031\n",
            "train loss:0.9686007212590145\n",
            "train loss:1.016443849487883\n",
            "train loss:0.8765346970823554\n",
            "train loss:0.9657672118414201\n",
            "train loss:1.0561317116276214\n",
            "train loss:1.0572630538331536\n",
            "train loss:1.1032772660919854\n",
            "train loss:0.9434270980285269\n",
            "train loss:0.8315946072976859\n",
            "train loss:0.8600072499044868\n",
            "train loss:0.9961005904889356\n",
            "train loss:0.8711723784580634\n",
            "train loss:0.9064486941164653\n",
            "train loss:0.6454882421964762\n",
            "train loss:0.8658940782377816\n",
            "train loss:0.8578973216123821\n",
            "train loss:0.9584785425626933\n",
            "train loss:0.7792568379091088\n",
            "train loss:0.8939636704560192\n",
            "train loss:0.8288759377659867\n",
            "train loss:0.9887767410567485\n",
            "train loss:0.9073659955530416\n",
            "train loss:0.7987752779731007\n",
            "train loss:0.9274897772499862\n",
            "train loss:0.8313922473703974\n",
            "train loss:0.8621456417766918\n",
            "train loss:1.0698092588888257\n",
            "train loss:0.7970698808115076\n",
            "train loss:0.8647468907709087\n",
            "train loss:0.864097236812994\n",
            "train loss:0.9113824162331465\n",
            "train loss:0.8776919295456019\n",
            "train loss:0.9545329736637719\n",
            "train loss:0.8073504451012182\n",
            "train loss:0.7980658619936061\n",
            "train loss:0.7497820502902972\n",
            "train loss:0.8997120991394106\n",
            "train loss:0.8901705423387335\n",
            "train loss:0.9904511266396289\n",
            "train loss:0.8700785478964497\n",
            "train loss:0.8099720438489351\n",
            "train loss:0.9609916127201024\n",
            "train loss:0.7797298958953128\n",
            "train loss:0.909112598294993\n",
            "train loss:0.7826760591476581\n",
            "train loss:0.8891635390582038\n",
            "train loss:0.7981524136999174\n",
            "train loss:0.9443304979745356\n",
            "train loss:0.858532983628487\n",
            "train loss:0.9302807804995862\n",
            "train loss:0.8767375150072507\n",
            "train loss:0.8642289247774033\n",
            "train loss:0.9313388267318274\n",
            "train loss:0.8394858043895337\n",
            "train loss:0.7870504065338547\n",
            "train loss:0.8434076246018265\n",
            "train loss:0.8699863895182043\n",
            "train loss:0.9421806424306253\n",
            "train loss:0.9274436487590519\n",
            "train loss:0.956175638316669\n",
            "train loss:0.753326370231581\n",
            "train loss:1.0472309635346313\n",
            "train loss:0.8909892299130155\n",
            "train loss:0.8854278390485615\n",
            "train loss:1.0869284016908525\n",
            "train loss:0.833020104821447\n",
            "train loss:0.8699964902880101\n",
            "train loss:0.855723647031989\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-wQAypFHZ35"
      },
      "source": [
        "# 8.3.4 half_float_network.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDb1ZqGsHea9"
      },
      "source": [
        "!python half_float_network.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wpm-ta5HihJ"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_convnet import DeepConvNet\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()\n",
        "network.load_params(\"deep_convnet_params.pkl\")\n",
        "\n",
        "sampled = 10000 # 高速化のため\n",
        "x_test = x_test[:sampled]\n",
        "t_test = t_test[:sampled]\n",
        "\n",
        "print(\"caluculate accuracy (float64) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n",
        "\n",
        "# float16に型変換\n",
        "x_test = x_test.astype(np.float16)\n",
        "for param in network.params.values():\n",
        "    param[...] = param.astype(np.float16)\n",
        "\n",
        "print(\"caluculate accuracy (float16) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B_mM3VXWG1j"
      },
      "source": [
        "---\n",
        "Tools for intelligent interaction systems a  \n",
        "Master's and Docotal programs in intelligent and mechanical interaction systems, University of Tsukuba, Japan.  \n",
        "KAMEDA Yoshinari, SHIBUYA Takeshi  \n",
        "\n",
        "知能システムツール演習a  \n",
        "知能機能システム学位プログラム (筑波大学大学院)  \n",
        "担当：亀田能成，澁谷長史  \n",
        "\n",
        "2021/07/26.  \n"
      ]
    }
  ]
}