{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC5-4_SAGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/5_gan_generation/GC5_4_SAGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJ6rwCNHq0B"
      },
      "source": [
        "# 5.4 SAGANの作成\n",
        "\n",
        "- 本ファイルでは、Self-Attention GANのネットワークを実装と学習をします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q10tF_pnHq0C"
      },
      "source": [
        "# 5.4 学習目標\n",
        "\n",
        "1.\tSAGANを実装できるようになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUcHIjQjaskV"
      },
      "source": [
        "---\n",
        "# Preparation by make_folders_and_data_downloads.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTPIJSpnbffM"
      },
      "source": [
        "# Working directory is set to /root\n",
        "\n",
        "%cd ~"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HnqsteVudlz"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GodJF_jbudl0"
      },
      "source": [
        "# フォルダ「data」が存在しない場合は作成する\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrQJqfjXudl1"
      },
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OFdR4Liudl2"
      },
      "source": [
        "# sklernのversionが0.20より低い場合はバージョンを更新します\n",
        "# pip install -U scikit-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rt80iUmudl3"
      },
      "source": [
        "# MNISTの手書き数字画像をダウンロードし読み込みます（1-2分ほど時間がかかります）\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "#mnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\")  \n",
        "mnist = fetch_openml('mnist_784', version=1, data_home=\"./data/\", as_frame=False)  \n",
        "# data_homeは保存先を指定します\n",
        "# Issue #153 2020年12月にリリースされたsklearn 0.24.0以降の仕様変更に合わせる場合\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFVYkY_Cudl4"
      },
      "source": [
        "# データの取り出し\n",
        "X = mnist.data\n",
        "y = mnist.target\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyjWP2teudl4"
      },
      "source": [
        "# MNISTのデータの1つ目を可視化する\n",
        "plt.imshow(X[0].reshape(28, 28), cmap='gray')\n",
        "print(\"この画像データのラベルは{}です\".format(y[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3fHck2kudl5"
      },
      "source": [
        "# フォルダ「data」の下にフォルダ「img_78」を作成する\n",
        "data_dir_path = \"./data/img_78/\"\n",
        "if not os.path.exists(data_dir_path):\n",
        "    os.mkdir(data_dir_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETQnM_ZYudl5"
      },
      "source": [
        "# MNISTから数字7、8の画像だけフォルダ「img_78」に画像として保存していく\n",
        "count7=0\n",
        "count8=0\n",
        "max_num=200  # 画像は200枚ずつ作成する\n",
        "\n",
        "for i in range(len(X)):\n",
        "    \n",
        "    # 画像7の作成\n",
        "    if (y[i] is \"7\") and (count7<max_num):\n",
        "        file_path=\"./data/img_78/img_7_\"+str(count7)+\".jpg\"\n",
        "        im_f=(X[i].reshape(28, 28))  # 画像を28×28の形に変形\n",
        "        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 画像をPILに\n",
        "        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64に拡大\n",
        "        pil_img_f.save(file_path)  # 保存\n",
        "        count7+=1 \n",
        "    \n",
        "    # 画像8の作成\n",
        "    if (y[i] is \"8\") and (count8<max_num):\n",
        "        file_path=\"./data/img_78/img_8_\"+str(count8)+\".jpg\"\n",
        "        im_f=(X[i].reshape(28, 28))  # 画像を28*28の形に変形\n",
        "        pil_img_f = Image.fromarray(im_f.astype(np.uint8))  # 画像をPILに\n",
        "        pil_img_f = pil_img_f.resize((64, 64), Image.BICUBIC)  # 64×64に拡大\n",
        "        pil_img_f.save(file_path)  # 保存\n",
        "        count8+=1 \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtVbOcIFHq0D"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "書籍の指示に従い、本章で使用するデータを用意します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-IBbz6dHq0E"
      },
      "source": [
        "# パッケージのimport\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchvision import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKjwhklpHq0L"
      },
      "source": [
        "# Setup seeds\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuHPoTVgHq0Q"
      },
      "source": [
        "# Self-Attentionモジュールの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ELTcrvwHq0Q"
      },
      "source": [
        "class Self_Attention(nn.Module):\n",
        "    \"\"\" Self-AttentionのLayer\"\"\"\n",
        "\n",
        "    def __init__(self, in_dim):\n",
        "        super(Self_Attention, self).__init__()\n",
        "\n",
        "        # 1×1の畳み込み層によるpointwise convolutionを用意\n",
        "        self.query_conv = nn.Conv2d(\n",
        "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(\n",
        "            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(\n",
        "            in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
        "\n",
        "        # Attention Map作成時の規格化のソフトマックス\n",
        "        self.softmax = nn.Softmax(dim=-2)\n",
        "\n",
        "        # 元の入力xとSelf-Attention Mapであるoを足し算するときの係数\n",
        "        # output = x +gamma*o\n",
        "        # 最初はgamma=0で、学習させていく\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 入力変数\n",
        "        X = x\n",
        "\n",
        "        # 畳み込みをしてから、サイズを変形する。 B,C',W,H→B,C',N へ\n",
        "        proj_query = self.query_conv(X).view(\n",
        "            X.shape[0], -1, X.shape[2]*X.shape[3])  # サイズ：B,C',N\n",
        "        proj_query = proj_query.permute(0, 2, 1)  # 転置操作\n",
        "        proj_key = self.key_conv(X).view(\n",
        "            X.shape[0], -1, X.shape[2]*X.shape[3])  # サイズ：B,C',N\n",
        "\n",
        "        # かけ算\n",
        "        S = torch.bmm(proj_query, proj_key)  # bmmはバッチごとの行列かけ算です\n",
        "\n",
        "        # 規格化\n",
        "        attention_map_T = self.softmax(S)  # 行i方向の和を1にするソフトマックス関数\n",
        "        attention_map = attention_map_T.permute(0, 2, 1)  # 転置をとる\n",
        "\n",
        "        # Self-Attention Mapを計算する\n",
        "        proj_value = self.value_conv(X).view(\n",
        "            X.shape[0], -1, X.shape[2]*X.shape[3])  # サイズ：B,C,N\n",
        "        o = torch.bmm(proj_value, attention_map.permute(\n",
        "            0, 2, 1))  # Attention Mapは転置してかけ算\n",
        "\n",
        "        # Self-Attention MapであるoのテンソルサイズをXにそろえて、出力にする\n",
        "        o = o.view(X.shape[0], X.shape[1], X.shape[2], X.shape[3])\n",
        "        out = x+self.gamma*o\n",
        "\n",
        "        return out, attention_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVhRSg_FHq0V"
      },
      "source": [
        "# Generatorの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dkqfa33Hq0W"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim=20, image_size=64):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(z_dim, image_size * 8,\n",
        "                                                      kernel_size=4, stride=1)),\n",
        "            nn.BatchNorm2d(image_size * 8),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size * 8, image_size * 4,\n",
        "                                                      kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(image_size * 4),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size * 4, image_size * 2,\n",
        "                                                      kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(image_size * 2),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # Self-Attention層を追加\n",
        "        self.self_attntion1 = Self_Attention(in_dim=image_size * 2)\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.ConvTranspose2d(image_size * 2, image_size,\n",
        "                                                      kernel_size=4, stride=2, padding=1)),\n",
        "            nn.BatchNorm2d(image_size),\n",
        "            nn.ReLU(inplace=True))\n",
        "\n",
        "        # Self-Attention層を追加\n",
        "        self.self_attntion2 = Self_Attention(in_dim=image_size)\n",
        "\n",
        "        self.last = nn.Sequential(\n",
        "            nn.ConvTranspose2d(image_size, 1, kernel_size=4,\n",
        "                               stride=2, padding=1),\n",
        "            nn.Tanh())\n",
        "        # 注意：白黒画像なので出力チャネルは1つだけ\n",
        "\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.layer1(z)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out, attention_map1 = self.self_attntion1(out)\n",
        "        out = self.layer4(out)\n",
        "        out, attention_map2 = self.self_attntion2(out)\n",
        "        out = self.last(out)\n",
        "\n",
        "        return out, attention_map1, attention_map2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY07cmr0Hq0b"
      },
      "source": [
        "# 動作確認\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "G = Generator(z_dim=20, image_size=64)\n",
        "\n",
        "# 入力する乱数\n",
        "input_z = torch.randn(1, 20)\n",
        "\n",
        "# テンソルサイズを(1, 20, 1, 1)に変形\n",
        "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "\n",
        "# 偽画像を出力\n",
        "fake_images, attention_map1, attention_map2 = G(input_z)\n",
        "\n",
        "img_transformed = fake_images[0][0].detach().numpy()\n",
        "plt.imshow(img_transformed, 'gray')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbiD6npMHq0e"
      },
      "source": [
        "# Discriminatorの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCCds42bHq0f"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, z_dim=20, image_size=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.Conv2d(1, image_size, kernel_size=4,\n",
        "                                             stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "        # 注意：白黒画像なので入力チャネルは1つだけ\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.Conv2d(image_size, image_size*2, kernel_size=4,\n",
        "                                             stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        self.layer3 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.Conv2d(image_size*2, image_size*4, kernel_size=4,\n",
        "                                             stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        # Self-Attention層を追加\n",
        "        self.self_attntion1 = Self_Attention(in_dim=image_size*4)\n",
        "\n",
        "        self.layer4 = nn.Sequential(\n",
        "            # Spectral Normalizationを追加\n",
        "            nn.utils.spectral_norm(nn.Conv2d(image_size*4, image_size*8, kernel_size=4,\n",
        "                                             stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.1, inplace=True))\n",
        "\n",
        "        # Self-Attention層を追加\n",
        "        self.self_attntion2 = Self_Attention(in_dim=image_size*8)\n",
        "\n",
        "        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out, attention_map1 = self.self_attntion1(out)\n",
        "        out = self.layer4(out)\n",
        "        out, attention_map2 = self.self_attntion2(out)\n",
        "        out = self.last(out)\n",
        "\n",
        "        return out, attention_map1, attention_map2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EADRQjSNHq0i"
      },
      "source": [
        "# 動作確認\n",
        "D = Discriminator(z_dim=20, image_size=64)\n",
        "\n",
        "# 偽画像を生成\n",
        "input_z = torch.randn(1, 20)\n",
        "input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "fake_images, _, _ = G(input_z)\n",
        "\n",
        "# 偽画像をDに入力\n",
        "d_out, attention_map1, attention_map2 = D(fake_images)\n",
        "\n",
        "# 出力d_outにSigmoidをかけて0から1に変換\n",
        "print(nn.Sigmoid()(d_out))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHc4LNUWHq0l"
      },
      "source": [
        "# DataLoaderの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da55pqU2Hq0m"
      },
      "source": [
        "def make_datapath_list():\n",
        "    \"\"\"学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する。 \"\"\"\n",
        "\n",
        "    train_img_list = list()  # 画像ファイルパスを格納\n",
        "\n",
        "    for img_idx in range(200):\n",
        "        img_path = \"./data/img_78/img_7_\" + str(img_idx)+'.jpg'\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "        img_path = \"./data/img_78/img_8_\" + str(img_idx)+'.jpg'\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    return train_img_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHs9HCE-Hq0r"
      },
      "source": [
        "class ImageTransform():\n",
        "    \"\"\"画像の前処理クラス\"\"\"\n",
        "\n",
        "    def __init__(self, mean, std):\n",
        "        self.data_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.data_transform(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNJB3q0XHq0t"
      },
      "source": [
        "class GAN_Img_Dataset(data.Dataset):\n",
        "    \"\"\"画像のDatasetクラス。PyTorchのDatasetクラスを継承\"\"\"\n",
        "\n",
        "    def __init__(self, file_list, transform):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''前処理をした画像のTensor形式のデータを取得'''\n",
        "\n",
        "        img_path = self.file_list[index]\n",
        "        img = Image.open(img_path)  # [高さ][幅]白黒\n",
        "\n",
        "        # 画像の前処理\n",
        "        img_transformed = self.transform(img)\n",
        "\n",
        "        return img_transformed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj6eVeLCHq0w"
      },
      "source": [
        "# DataLoaderの作成と動作確認\n",
        "\n",
        "# ファイルリストを作成\n",
        "train_img_list=make_datapath_list()\n",
        "\n",
        "# Datasetを作成\n",
        "mean = (0.5,)\n",
        "std = (0.5,)\n",
        "train_dataset = GAN_Img_Dataset(\n",
        "    file_list=train_img_list, transform=ImageTransform(mean, std))\n",
        "\n",
        "# DataLoaderを作成\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(train_dataloader)  # イテレータに変換\n",
        "imges = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(imges.size())  # torch.Size([64, 1, 64, 64])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InLWkgysHq0z"
      },
      "source": [
        "# 学習させる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YXeMeAC1Hq00"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(G, D, dataloader, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "\n",
        "    # 最適化手法の設定\n",
        "    g_lr, d_lr = 0.0001, 0.0004\n",
        "    beta1, beta2 = 0.0, 0.9\n",
        "    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n",
        "    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n",
        "\n",
        "    # 誤差関数を定義 → hinge version of the adversarial lossに変更\n",
        "    # criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    # パラメータをハードコーディング\n",
        "    z_dim = 20\n",
        "    mini_batch_size = 64\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    G.to(device)\n",
        "    D.to(device)\n",
        "\n",
        "    G.train()  # モデルを訓練モードに\n",
        "    D.train()  # モデルを訓練モードに\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # 画像の枚数\n",
        "    num_train_imgs = len(dataloader.dataset)\n",
        "    batch_size = dataloader.batch_size\n",
        "\n",
        "    # イテレーションカウンタをセット\n",
        "    iteration = 1\n",
        "    logs = []\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        # 開始時刻を保存\n",
        "        t_epoch_start = time.time()\n",
        "        epoch_g_loss = 0.0  # epochの損失和\n",
        "        epoch_d_loss = 0.0  # epochの損失和\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "        print('-------------')\n",
        "        print('（train）')\n",
        "\n",
        "        # データローダーからminibatchずつ取り出すループ\n",
        "        for imges in dataloader:\n",
        "\n",
        "            # --------------------\n",
        "            # 1. Discriminatorの学習\n",
        "            # --------------------\n",
        "            # ミニバッチがサイズが1だと、バッチノーマライゼーションでエラーになるのでさける\n",
        "            if imges.size()[0] == 1:\n",
        "                continue\n",
        "\n",
        "            # GPUが使えるならGPUにデータを送る\n",
        "            imges = imges.to(device)\n",
        "\n",
        "            # 正解ラベルと偽ラベルを作成\n",
        "            # epochの最後のイテレーションはミニバッチの数が少なくなる\n",
        "            mini_batch_size = imges.size()[0]\n",
        "            #label_real = torch.full((mini_batch_size,), 1).to(device)\n",
        "            #label_fake = torch.full((mini_batch_size,), 0).to(device)\n",
        "\n",
        "            # 真の画像を判定\n",
        "            d_out_real, _, _ = D(imges)\n",
        "\n",
        "            # 偽の画像を生成して判定\n",
        "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
        "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "            fake_images, _, _ = G(input_z)\n",
        "            d_out_fake, _, _ = D(fake_images)\n",
        "\n",
        "            # 誤差を計算→hinge version of the adversarial lossに変更\n",
        "            # d_loss_real = criterion(d_out_real.view(-1), label_real)\n",
        "            # d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n",
        "\n",
        "            d_loss_real = torch.nn.ReLU()(1.0 - d_out_real).mean()\n",
        "            # 誤差　d_out_realが1以上で誤差0になる。d_out_real>1で、\n",
        "            # 1.0 - d_out_realが負の場合ReLUで0にする\n",
        "\n",
        "            d_loss_fake = torch.nn.ReLU()(1.0 + d_out_fake).mean()\n",
        "            # 誤差　d_out_fakeが-1以下なら誤差0になる。d_out_fake<-1で、\n",
        "            # 1.0 + d_out_realが負の場合ReLUで0にする\n",
        "\n",
        "            d_loss = d_loss_real + d_loss_fake\n",
        "\n",
        "            # バックプロパゲーション\n",
        "            g_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # --------------------\n",
        "            # 2. Generatorの学習\n",
        "            # --------------------\n",
        "            # 偽の画像を生成して判定\n",
        "            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n",
        "            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n",
        "            fake_images, _, _ = G(input_z)\n",
        "            d_out_fake, _, _ = D(fake_images)\n",
        "\n",
        "            # 誤差を計算→hinge version of the adversarial lossに変更\n",
        "            #g_loss = criterion(d_out_fake.view(-1), label_real)\n",
        "            g_loss = - d_out_fake.mean()\n",
        "\n",
        "            # バックプロパゲーション\n",
        "            g_optimizer.zero_grad()\n",
        "            d_optimizer.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            # --------------------\n",
        "            # 3. 記録\n",
        "            # --------------------\n",
        "            epoch_d_loss += d_loss.item()\n",
        "            epoch_g_loss += g_loss.item()\n",
        "            iteration += 1\n",
        "\n",
        "        # epochのphaseごとのlossと正解率\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n",
        "            epoch, epoch_d_loss/batch_size, epoch_g_loss/batch_size))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "    # print(\"総イテレーション回数:\", iteration)\n",
        "\n",
        "    return G, D\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-nXAePVHq03"
      },
      "source": [
        "# ネットワークの初期化\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        # Conv2dとConvTranspose2dの初期化\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        # BatchNorm2dの初期化\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "# 初期化の実施\n",
        "G.apply(weights_init)\n",
        "D.apply(weights_init)\n",
        "\n",
        "print(\"ネットワークの初期化完了\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uicNESSKHq06"
      },
      "source": [
        "# 学習・検証を実行する\n",
        "# 15分ほどかかる\n",
        "num_epochs = 300\n",
        "G_update, D_update = train_model(\n",
        "    G, D, dataloader=train_dataloader, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2e6p74yHq09"
      },
      "source": [
        "# 生成画像と訓練データを可視化する\n",
        "# 本セルは、良い感じの画像が生成されるまで、何度か実行をし直しています\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 入力の乱数生成\n",
        "batch_size = 8\n",
        "z_dim = 20\n",
        "fixed_z = torch.randn(batch_size, z_dim)\n",
        "fixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n",
        "\n",
        "# 画像生成\n",
        "G_update.eval()\n",
        "fake_images, am1, am2 = G_update(fixed_z.to(device))\n",
        "\n",
        "# 訓練データ\n",
        "batch_iterator = iter(train_dataloader)  # イテレータに変換\n",
        "imges = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "\n",
        "\n",
        "# 出力\n",
        "fig = plt.figure(figsize=(15, 6))\n",
        "for i in range(0, 5):\n",
        "    # 上段に訓練データを\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(imges[i][0].cpu().detach().numpy(), 'gray')\n",
        "\n",
        "    # 下段に生成データを表示する\n",
        "    plt.subplot(2, 5, 5+i+1)\n",
        "    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYKyJcprHq0_"
      },
      "source": [
        "# Attentiom Mapを出力\n",
        "fig = plt.figure(figsize=(15, 6))\n",
        "for i in range(0, 5):\n",
        "\n",
        "    # 上段に生成した画像データを\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(fake_images[i][0].cpu().detach().numpy(), 'gray')\n",
        "\n",
        "    # 下段にAttentin Map1の画像中央のピクセルのデータを\n",
        "    plt.subplot(2, 5, 5+i+1)\n",
        "    am = am1[i].view(16, 16, 16, 16)\n",
        "    am = am[7][7]  # 中央に着目\n",
        "    plt.imshow(am.cpu().detach().numpy(), 'Reds')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "277qT3egHq1C"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLSDq8V-bU5B"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/02. "
      ]
    }
  ]
}