{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "GC8-2-3_bert_base.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/8_nlp_sentiment_bert/GC8_2_3_bert_base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meuV1368is67"
      },
      "source": [
        "# BERTモデルの実装\n",
        "本ファイルでは、BERTの基本モデル、BERTのMasked Language Modelタスク、単語のベクトル表現の確認を実装します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zOPabaVis69"
      },
      "source": [
        "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0iHTxV-is6-"
      },
      "source": [
        "# 8.2 学習目標\n",
        "\n",
        "1.\tBERTのEmbeddingsモジュールの動作を理解し、実装できる\n",
        "2.\tBERTのSelf-Attentionを活用したTransformer部分であるBertLayerモジュールの動作を理解し、実装できる\n",
        "3.\tBERTのPoolerモジュールの動作を理解し、実装できる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-vXtqQKis6-"
      },
      "source": [
        "# 8.3 学習目標\n",
        "\n",
        "1.\tBERTの学習済みモデルを自分の実装モデルにロードできる\n",
        "2.\tBERT用の単語分割クラスなど、言語データの前処理部分を実装できる\n",
        "3.\tBERTで単語ベクトルを取り出して確認する内容を実装できる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"8_nlp_sentiment_bert\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEjiGAp8is6-"
      },
      "source": [
        "---\n",
        "# 8.2 節 BERTの実装\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdvRD4KDR2TO"
      },
      "source": [
        "!pip install attrdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjNunWclis6_"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ_srp4Dis6_"
      },
      "source": [
        "## BERT_Baseのネットワークの設定ファイルの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-yAgkUYis7A"
      },
      "source": [
        "# 設定をconfig.jsonから読み込み、JSONの辞書変数をオブジェクト変数に変換\n",
        "import json\n",
        "\n",
        "config_file = \"./weights/bert_config.json\"\n",
        "\n",
        "# ファイルを開き、JSONとして読み込む\n",
        "json_file = open(config_file, 'r')\n",
        "config = json.load(json_file)\n",
        "\n",
        "# 出力確認\n",
        "config\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX3OniaIis7B"
      },
      "source": [
        "# こう書くのは面倒・・・\n",
        "config['hidden_size']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm7nZkZcis7B"
      },
      "source": [
        "# 辞書変数をオブジェクト変数に\n",
        "from attrdict import AttrDict\n",
        "\n",
        "config = AttrDict(config)\n",
        "config.hidden_size\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJAK6q2Dis7B"
      },
      "source": [
        "## BERT用にLayerNormalization層を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b19AQfRis7B"
      },
      "source": [
        "# BERT用にLayerNormalization層を定義します。\n",
        "# 実装の細かな点をTensorFlowに合わせています。\n",
        "\n",
        "\n",
        "class BertLayerNorm(nn.Module):\n",
        "    \"\"\"LayerNormalization層 \"\"\"\n",
        "\n",
        "    def __init__(self, hidden_size, eps=1e-12):\n",
        "        super(BertLayerNorm, self).__init__()\n",
        "        self.gamma = nn.Parameter(torch.ones(hidden_size))  # weightのこと\n",
        "        self.beta = nn.Parameter(torch.zeros(hidden_size))  # biasのこと\n",
        "        self.variance_epsilon = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        u = x.mean(-1, keepdim=True)\n",
        "        s = (x - u).pow(2).mean(-1, keepdim=True)\n",
        "        x = (x - u) / torch.sqrt(s + self.variance_epsilon)\n",
        "        return self.gamma * x + self.beta\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttzlN9f1is7C"
      },
      "source": [
        "## Embeddingsモジュールの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG9KgwBRis7C"
      },
      "source": [
        "# BERTのEmbeddingsモジュールです\n",
        "\n",
        "\n",
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"文章の単語ID列と、1文目か2文目かの情報を、埋め込みベクトルに変換する\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "\n",
        "        # 3つのベクトル表現の埋め込み\n",
        "\n",
        "        # Token Embedding：単語IDを単語ベクトルに変換、\n",
        "        # vocab_size = 30522でBERTの学習済みモデルで使用したボキャブラリーの量\n",
        "        # hidden_size = 768 で特徴量ベクトルの長さは768\n",
        "        self.word_embeddings = nn.Embedding(\n",
        "            config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        # （注釈）padding_idx=0はidx=0の単語のベクトルは0にする。BERTのボキャブラリーのidx=0が[PAD]である。\n",
        "\n",
        "        # Transformer Positional Embedding：位置情報テンソルをベクトルに変換\n",
        "        # Transformerの場合はsin、cosからなる固定値だったが、BERTは学習させる\n",
        "        # max_position_embeddings = 512　で文の長さは512単語\n",
        "        self.position_embeddings = nn.Embedding(\n",
        "            config.max_position_embeddings, config.hidden_size)\n",
        "\n",
        "        # Sentence Embedding：文章の1文目、2文目の情報をベクトルに変換\n",
        "        # type_vocab_size = 2\n",
        "        self.token_type_embeddings = nn.Embedding(\n",
        "            config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # 作成したLayerNormalization層\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "        # Dropout　'hidden_dropout_prob': 0.1\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        '''\n",
        "        input_ids： [batch_size, seq_len]の文章の単語IDの羅列\n",
        "        token_type_ids：[batch_size, seq_len]の各単語が1文目なのか、2文目なのかを示すid\n",
        "        '''\n",
        "\n",
        "        # 1. Token Embeddings\n",
        "        # 単語IDを単語ベクトルに変換\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "\n",
        "        # 2. Sentence Embedding\n",
        "        # token_type_idsがない場合は文章の全単語を1文目として、0にする\n",
        "        # そこで、input_idsと同じサイズのゼロテンソルを作成\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        # 3. Transformer Positional Embedding：\n",
        "        # [0, 1, 2 ・・・]と文章の長さだけ、数字が1つずつ昇順に入った\n",
        "        # [batch_size, seq_len]のテンソルposition_idsを作成\n",
        "        # position_idsを入力して、position_embeddings層から768次元のテンソルを取り出す\n",
        "        seq_length = input_ids.size(1)  # 文章の長さ\n",
        "        position_ids = torch.arange(\n",
        "            seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "\n",
        "        # 3つの埋め込みテンソルを足し合わせる [batch_size, seq_len, hidden_size]\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "\n",
        "        # LayerNormalizationとDropoutを実行\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "        return embeddings\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkZ-RU21is7D"
      },
      "source": [
        "## BertLayerモジュール\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fciTm-wpis7D"
      },
      "source": [
        "class BertLayer(nn.Module):\n",
        "    '''BERTのBertLayerモジュールです。Transformerになります'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "\n",
        "        # Self-Attention部分\n",
        "        self.attention = BertAttention(config)\n",
        "\n",
        "        # Self-Attentionの出力を処理する全結合層\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "\n",
        "        # Self-Attentionによる特徴量とBertLayerへの元の入力を足し算する層\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
        "        '''\n",
        "        hidden_states：Embedderモジュールの出力テンソル[batch_size, seq_len, hidden_size]\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキング\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "        if attention_show_flg == True:\n",
        "            '''attention_showのときは、attention_probsもリターンする'''\n",
        "            attention_output, attention_probs = self.attention(\n",
        "                hidden_states, attention_mask, attention_show_flg)\n",
        "            intermediate_output = self.intermediate(attention_output)\n",
        "            layer_output = self.output(intermediate_output, attention_output)\n",
        "            return layer_output, attention_probs\n",
        "\n",
        "        elif attention_show_flg == False:\n",
        "            attention_output = self.attention(\n",
        "                hidden_states, attention_mask, attention_show_flg)\n",
        "            intermediate_output = self.intermediate(attention_output)\n",
        "            layer_output = self.output(intermediate_output, attention_output)\n",
        "\n",
        "            return layer_output  # [batch_size, seq_length, hidden_size]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G7IjdKgis7D"
      },
      "source": [
        "class BertAttention(nn.Module):\n",
        "    '''BertLayerモジュールのSelf-Attention部分です'''\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.selfattn = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask, attention_show_flg=False):\n",
        "        '''\n",
        "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "        if attention_show_flg == True:\n",
        "            '''attention_showのときは、attention_probsもリターンする'''\n",
        "            self_output, attention_probs = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
        "            attention_output = self.output(self_output, input_tensor)\n",
        "            return attention_output, attention_probs\n",
        "        \n",
        "        elif attention_show_flg == False:\n",
        "            self_output = self.selfattn(input_tensor, attention_mask, attention_show_flg)\n",
        "            attention_output = self.output(self_output, input_tensor)\n",
        "            return attention_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWNlaTfais7E"
      },
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    '''BertAttentionのSelf-Attentionです'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        # num_attention_heads': 12\n",
        "\n",
        "        self.attention_head_size = int(\n",
        "            config.hidden_size / config.num_attention_heads)  # 768/12=64\n",
        "        self.all_head_size = self.num_attention_heads * \\\n",
        "            self.attention_head_size  # = 'hidden_size': 768\n",
        "\n",
        "        # Self-Attentionの特徴量を作成する全結合層\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        '''multi-head Attention用にテンソルの形を変換する\n",
        "        [batch_size, seq_len, hidden] → [batch_size, 12, seq_len, hidden/12] \n",
        "        '''\n",
        "        new_x_shape = x.size()[\n",
        "            :-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, attention_show_flg=False):\n",
        "        '''\n",
        "        hidden_states：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "        # 入力を全結合層で特徴量変換（注意、multi-head Attentionの全部をまとめて変換しています）\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        # multi-head Attention用にテンソルの形を変換\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # 特徴量同士を掛け算して似ている度合をAttention_scoresとして求める\n",
        "        attention_scores = torch.matmul(\n",
        "            query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / \\\n",
        "            math.sqrt(self.attention_head_size)\n",
        "\n",
        "        # マスクがある部分にはマスクをかけます\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        # （備考）\n",
        "        # マスクが掛け算でなく足し算なのが直感的でないですが、このあとSoftmaxで正規化するので、\n",
        "        # マスクされた部分は-infにしたいです。 attention_maskには、0か-infが\n",
        "        # もともと入っているので足し算にしています。\n",
        "\n",
        "        # Attentionを正規化する\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # ドロップアウトします\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Attention Mapを掛け算します\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        # multi-head Attentionのテンソルの形をもとに戻す\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[\n",
        "            :-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "\n",
        "        # attention_showのときは、attention_probsもリターンする\n",
        "        if attention_show_flg == True:\n",
        "            return context_layer, attention_probs\n",
        "        elif attention_show_flg == False:\n",
        "            return context_layer\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdF5FmMXis7E"
      },
      "source": [
        "class BertSelfOutput(nn.Module):\n",
        "    '''BertSelfAttentionの出力を処理する全結合層です'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # 'hidden_dropout_prob': 0.1\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        '''\n",
        "        hidden_states：BertSelfAttentionの出力テンソル\n",
        "        input_tensor：Embeddingsモジュールもしくは前段のBertLayerからの出力\n",
        "        '''\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Qo3tI_is7F"
      },
      "source": [
        "def gelu(x):\n",
        "    '''Gaussian Error Linear Unitという活性化関数です。\n",
        "    LeLUが0でカクっと不連続なので、そこを連続になるように滑らかにした形のLeLUです。\n",
        "    '''\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        \n",
        "        # 全結合層：'hidden_size': 768、'intermediate_size': 3072\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        \n",
        "        # 活性化関数gelu\n",
        "        self.intermediate_act_fn = gelu\n",
        "            \n",
        "    def forward(self, hidden_states):\n",
        "        '''\n",
        "        hidden_states： BertAttentionの出力テンソル\n",
        "        '''\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)  # GELUによる活性化\n",
        "        return hidden_states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiY7K26his7F"
      },
      "source": [
        "class BertOutput(nn.Module):\n",
        "    '''BERTのTransformerBlockモジュールのFeedForwardです'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "\n",
        "        # 全結合層：'intermediate_size': 3072、'hidden_size': 768\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "        # 'hidden_dropout_prob': 0.1\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        '''\n",
        "        hidden_states： BertIntermediateの出力テンソル\n",
        "        input_tensor：BertAttentionの出力テンソル\n",
        "        '''\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHkcPDUBis7F"
      },
      "source": [
        "## BertLayerモジュールの繰り返し部分"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8wWhEDHis7F"
      },
      "source": [
        "# BertLayerモジュールの繰り返し部分モジュールの繰り返し部分です\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''BertLayerモジュールの繰り返し部分モジュールの繰り返し部分です'''\n",
        "        super(BertEncoder, self).__init__()\n",
        "\n",
        "        # config.num_hidden_layers の値、すなわち12 個のBertLayerモジュールを作ります\n",
        "        self.layer = nn.ModuleList([BertLayer(config)\n",
        "                                    for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True, attention_show_flg=False):\n",
        "        '''\n",
        "        hidden_states：Embeddingsモジュールの出力\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        output_all_encoded_layers：返り値を全TransformerBlockモジュールの出力にするか、\n",
        "        それとも、最終層だけにするかのフラグ。\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "\n",
        "        # 返り値として使うリスト\n",
        "        all_encoder_layers = []\n",
        "\n",
        "        # BertLayerモジュールの処理を繰り返す\n",
        "        for layer_module in self.layer:\n",
        "\n",
        "            if attention_show_flg == True:\n",
        "                '''attention_showのときは、attention_probsもリターンする'''\n",
        "                hidden_states, attention_probs = layer_module(\n",
        "                    hidden_states, attention_mask, attention_show_flg)\n",
        "            elif attention_show_flg == False:\n",
        "                hidden_states = layer_module(\n",
        "                    hidden_states, attention_mask, attention_show_flg)\n",
        "\n",
        "            # 返り値にBertLayerから出力された特徴量を12層分、すべて使用する場合の処理\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "\n",
        "        # 返り値に最後のBertLayerから出力された特徴量だけを使う場合の処理\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "\n",
        "        # attention_showのときは、attention_probs（最後の12段目）もリターンする\n",
        "        if attention_show_flg == True:\n",
        "            return all_encoder_layers, attention_probs\n",
        "        elif attention_show_flg == False:\n",
        "            return all_encoder_layers\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xC_iIZDAis7G"
      },
      "source": [
        "## BertPoolerモジュール\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tj_khlo_is7G"
      },
      "source": [
        "class BertPooler(nn.Module):\n",
        "    '''入力文章の1単語目[cls]の特徴量を変換して保持するためのモジュール'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertPooler, self).__init__()\n",
        "\n",
        "        # 全結合層、'hidden_size': 768\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        # 1単語目の特徴量を取得\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "\n",
        "        # 全結合層で特徴量変換\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "\n",
        "        # 活性化関数Tanhを計算\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "\n",
        "        return pooled_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxP2ufrPis7G"
      },
      "source": [
        "## 動作確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2OCG6PIis7G"
      },
      "source": [
        "# 動作確認\n",
        "\n",
        "# 入力の単語ID列、batch_sizeは2つ\n",
        "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
        "print(\"入力の単語ID列のテンソルサイズ：\", input_ids.shape)\n",
        "\n",
        "# マスク\n",
        "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
        "print(\"入力のマスクのテンソルサイズ：\", attention_mask.shape)\n",
        "\n",
        "# 文章のID。2つのミニバッチそれぞれについて、0が1文目、1が2文目を示す\n",
        "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
        "print(\"入力の文章IDのテンソルサイズ：\", token_type_ids.shape)\n",
        "\n",
        "\n",
        "# BERTの各モジュールを用意\n",
        "embeddings = BertEmbeddings(config)\n",
        "encoder = BertEncoder(config)\n",
        "pooler = BertPooler(config)\n",
        "\n",
        "# マスクの変形　[batch_size, 1, 1, seq_length]にする\n",
        "# Attentionをかけない部分はマイナス無限にしたいので、代わりに-10000をかけ算しています\n",
        "extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "extended_attention_mask = extended_attention_mask.to(dtype=torch.float32)\n",
        "extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "print(\"拡張したマスクのテンソルサイズ：\", extended_attention_mask.shape)\n",
        "\n",
        "# 順伝搬する\n",
        "out1 = embeddings(input_ids, token_type_ids)\n",
        "print(\"BertEmbeddingsの出力テンソルサイズ：\", out1.shape)\n",
        "\n",
        "out2 = encoder(out1, extended_attention_mask)\n",
        "# out2は、[minibatch, seq_length, embedding_dim]が12個のリスト\n",
        "print(\"BertEncoderの最終層の出力テンソルサイズ：\", out2[0].shape)\n",
        "\n",
        "out3 = pooler(out2[-1])  # out2は12層の特徴量のリストになっているので一番最後を使用\n",
        "print(\"BertPoolerの出力テンソルサイズ：\", out3.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8o4WFnris7H"
      },
      "source": [
        "## 全部をつなげてBERTモデルにする"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTQJoOygis7H"
      },
      "source": [
        "class BertModel(nn.Module):\n",
        "    '''モジュールを全部つなげたBERTモデル'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__()\n",
        "\n",
        "        # 3つのモジュールを作成\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True, attention_show_flg=False):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "\n",
        "        # Attentionのマスクと文の1文目、2文目のidが無ければ作成する\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # マスクの変形　[minibatch, 1, 1, seq_length]にする\n",
        "        # 後ほどmulti-head Attentionで使用できる形にしたいので\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # マスクは0、1だがソフトマックスを計算したときにマスクになるように、0と-infにする\n",
        "        # -infの代わりに-10000にしておく\n",
        "        extended_attention_mask = extended_attention_mask.to(\n",
        "            dtype=torch.float32)\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        # 順伝搬させる\n",
        "        # BertEmbeddinsモジュール\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "\n",
        "        # BertLayerモジュール（Transformer）を繰り返すBertEncoderモジュール\n",
        "        if attention_show_flg == True:\n",
        "            '''attention_showのときは、attention_probsもリターンする'''\n",
        "\n",
        "            encoded_layers, attention_probs = self.encoder(embedding_output,\n",
        "                                                           extended_attention_mask,\n",
        "                                                           output_all_encoded_layers, attention_show_flg)\n",
        "\n",
        "        elif attention_show_flg == False:\n",
        "            encoded_layers = self.encoder(embedding_output,\n",
        "                                          extended_attention_mask,\n",
        "                                          output_all_encoded_layers, attention_show_flg)\n",
        "\n",
        "        # BertPoolerモジュール\n",
        "        # encoderの一番最後のBertLayerから出力された特徴量を使う\n",
        "        pooled_output = self.pooler(encoded_layers[-1])\n",
        "\n",
        "        # output_all_encoded_layersがFalseの場合はリストではなく、テンソルを返す\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "\n",
        "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
        "        if attention_show_flg == True:\n",
        "            return encoded_layers, pooled_output, attention_probs\n",
        "        elif attention_show_flg == False:\n",
        "            return encoded_layers, pooled_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpr3VBf1is7H"
      },
      "source": [
        "# 動作確認\n",
        "# 入力の用意\n",
        "input_ids = torch.LongTensor([[31, 51, 12, 23, 99], [15, 5, 1, 0, 0]])\n",
        "attention_mask = torch.LongTensor([[1, 1, 1, 1, 1], [1, 1, 1, 0, 0]])\n",
        "token_type_ids = torch.LongTensor([[0, 0, 1, 1, 1], [0, 1, 1, 1, 1]])\n",
        "\n",
        "# BERTモデルを作る\n",
        "net = BertModel(config)\n",
        "\n",
        "# 順伝搬させる\n",
        "encoded_layers, pooled_output, attention_probs = net(\n",
        "    input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=True)\n",
        "\n",
        "print(\"encoded_layersのテンソルサイズ：\", encoded_layers.shape)\n",
        "print(\"pooled_outputのテンソルサイズ：\", pooled_output.shape)\n",
        "print(\"attention_probsのテンソルサイズ：\", attention_probs.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "496m2yTzis7H"
      },
      "source": [
        "---\n",
        "# 8.3節 BERTを用いたbank（銀行）とbank（土手）の単語ベクトル表現の比較"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a77aVW03is7H"
      },
      "source": [
        "## 学習済みモデルのロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHNOFtclis7H"
      },
      "source": [
        "# 学習済みモデルのロード\n",
        "weights_path = \"./weights/pytorch_model.bin\"\n",
        "loaded_state_dict = torch.load(weights_path)\n",
        "\n",
        "for s in loaded_state_dict.keys():\n",
        "    print(s)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM_1xAgWis7I"
      },
      "source": [
        "# モデルの用意\n",
        "net = BertModel(config)\n",
        "net.eval()\n",
        "\n",
        "# 現在のネットワークモデルのパラメータ名\n",
        "param_names = []  # パラメータの名前を格納していく\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    print(name)\n",
        "    param_names.append(name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpYcZSpwis7I"
      },
      "source": [
        "# state_dictの名前が違うので前から順番に代入する\n",
        "# 今回、パラメータの名前は違っていても、対応するものは同じ順番になっています\n",
        "\n",
        "# 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
        "new_state_dict = net.state_dict().copy()\n",
        "\n",
        "# 新たなstate_dictに学習済みの値を代入\n",
        "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
        "    name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
        "    new_state_dict[name] = value  # 値を入れる\n",
        "    print(str(key_name)+\"→\"+str(name))  # 何から何に入ったかを表示\n",
        "\n",
        "    # 現在のネットワークのパラメータを全部ロードしたら終える\n",
        "    if index+1 >= len(param_names):\n",
        "        break\n",
        "\n",
        "# 新たなstate_dictを実装したBERTモデルに与える\n",
        "net.load_state_dict(new_state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jytb-V8uis7I"
      },
      "source": [
        "## BERT用のTokenizerの実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-w5ol2tis7I"
      },
      "source": [
        "# vocabファイルを読み込み、\n",
        "import collections\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"text形式のvocabファイルの内容を辞書に格納します\"\"\"\n",
        "    vocab = collections.OrderedDict()  # (単語, id)の順番の辞書変数\n",
        "    ids_to_tokens = collections.OrderedDict()  # (id, 単語)の順番の辞書変数\n",
        "    index = 0\n",
        "\n",
        "    with open(vocab_file, \"r\", encoding=\"utf-8\") as reader:\n",
        "        while True:\n",
        "            token = reader.readline()\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "\n",
        "            # 格納\n",
        "            vocab[token] = index\n",
        "            ids_to_tokens[index] = token\n",
        "            index += 1\n",
        "\n",
        "    return vocab, ids_to_tokens\n",
        "\n",
        "\n",
        "# 実行\n",
        "vocab_file = \"./vocab/bert-base-uncased-vocab.txt\"\n",
        "vocab, ids_to_tokens = load_vocab(vocab_file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSSqQYd_is7I"
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6sCFbbfis7J"
      },
      "source": [
        "ids_to_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0-GCRF6is7J"
      },
      "source": [
        "from utils.tokenizer import BasicTokenizer, WordpieceTokenizer\n",
        "\n",
        "# BasicTokenizer, WordpieceTokenizerは、引用文献[2]そのままです\n",
        "# https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/tokenization.py\n",
        "# これらはsub-wordで単語分割を行うクラスになります。\n",
        "\n",
        "\n",
        "class BertTokenizer(object):\n",
        "    '''BERT用の文章の単語分割クラスを実装'''\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        '''\n",
        "        vocab_file：ボキャブラリーへのパス\n",
        "        do_lower_case：前処理で単語を小文字化するかどうか\n",
        "        '''\n",
        "\n",
        "        # ボキャブラリーのロード\n",
        "        self.vocab, self.ids_to_tokens = load_vocab(vocab_file)\n",
        "\n",
        "        # 分割処理の関数をフォルダ「utils」からimoprt、sub-wordで単語分割を行う\n",
        "        never_split = (\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")\n",
        "        # (注釈)上記の単語は途中で分割させない。これで一つの単語とみなす\n",
        "\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case,\n",
        "                                              never_split=never_split)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        '''文章を単語に分割する関数'''\n",
        "        split_tokens = []  # 分割後の単語たち\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        \"\"\"分割された単語リストをIDに変換する関数\"\"\"\n",
        "        ids = []\n",
        "        for token in tokens:\n",
        "            ids.append(self.vocab[token])\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def convert_ids_to_tokens(self, ids):\n",
        "        \"\"\"IDを単語に変換する関数\"\"\"\n",
        "        tokens = []\n",
        "        for i in ids:\n",
        "            tokens.append(self.ids_to_tokens[i])\n",
        "        return tokens\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWK25yeRis7J"
      },
      "source": [
        "## Bankの文脈による意味変化を単語ベクトルとして求める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpU3pQepis7J"
      },
      "source": [
        "# 文章1：銀行口座にアクセスしました。\n",
        "text_1 = \"[CLS] I accessed the bank account. [SEP]\"\n",
        "\n",
        "# 文章2：彼は敷金を銀行口座に振り込みました。\n",
        "text_2 = \"[CLS] He transferred the deposit money into the bank account. [SEP]\"\n",
        "\n",
        "# 文章3：川岸でサッカーをします。\n",
        "text_3 = \"[CLS] We play soccer at the bank of the river. [SEP]\"\n",
        "\n",
        "# 単語分割Tokenizerを用意\n",
        "tokenizer = BertTokenizer(\n",
        "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
        "\n",
        "# 文章を単語分割\n",
        "tokenized_text_1 = tokenizer.tokenize(text_1)\n",
        "tokenized_text_2 = tokenizer.tokenize(text_2)\n",
        "tokenized_text_3 = tokenizer.tokenize(text_3)\n",
        "\n",
        "# 確認\n",
        "print(tokenized_text_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVhD-g5-is7K"
      },
      "source": [
        "# 単語をIDに変換する\n",
        "indexed_tokens_1 = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
        "indexed_tokens_2 = tokenizer.convert_tokens_to_ids(tokenized_text_2)\n",
        "indexed_tokens_3 = tokenizer.convert_tokens_to_ids(tokenized_text_3)\n",
        "\n",
        "# 各文章のbankの位置\n",
        "bank_posi_1 = np.where(np.array(tokenized_text_1) == \"bank\")[0][0]  # 4\n",
        "bank_posi_2 = np.where(np.array(tokenized_text_2) == \"bank\")[0][0]  # 8\n",
        "bank_posi_3 = np.where(np.array(tokenized_text_3) == \"bank\")[0][0]  # 6\n",
        "\n",
        "# seqId（1文目か2文目かは今回は必要ない）\n",
        "\n",
        "# リストをPyTorchのテンソルに\n",
        "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
        "tokens_tensor_2 = torch.tensor([indexed_tokens_2])\n",
        "tokens_tensor_3 = torch.tensor([indexed_tokens_3])\n",
        "\n",
        "# bankの単語id\n",
        "bank_word_id = tokenizer.convert_tokens_to_ids([\"bank\"])[0]\n",
        "\n",
        "# 確認\n",
        "print(tokens_tensor_1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W39XbDK_is7K"
      },
      "source": [
        "# 文章をBERTで処理\n",
        "with torch.no_grad():\n",
        "    encoded_layers_1, _ = net(tokens_tensor_1, output_all_encoded_layers=True)\n",
        "    encoded_layers_2, _ = net(tokens_tensor_2, output_all_encoded_layers=True)\n",
        "    encoded_layers_3, _ = net(tokens_tensor_3, output_all_encoded_layers=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q02yEj-7is7K"
      },
      "source": [
        "# bankの初期の単語ベクトル表現\n",
        "# これはEmbeddingsモジュールから取り出し、単語bankのidに応じた単語ベクトルなので3文で共通している\n",
        "bank_vector_0 = net.embeddings.word_embeddings.weight[bank_word_id]\n",
        "\n",
        "# 文章1のBertLayerモジュール1段目から出力されるbankの特徴量ベクトル\n",
        "bank_vector_1_1 = encoded_layers_1[0][0, bank_posi_1]\n",
        "\n",
        "# 文章1のBertLayerモジュール最終12段目から出力されるのbankの特徴量ベクトル\n",
        "bank_vector_1_12 = encoded_layers_1[11][0, bank_posi_1]\n",
        "\n",
        "# 文章2、3も同様に\n",
        "bank_vector_2_1 = encoded_layers_2[0][0, bank_posi_2]\n",
        "bank_vector_2_12 = encoded_layers_2[11][0, bank_posi_2]\n",
        "bank_vector_3_1 = encoded_layers_3[0][0, bank_posi_3]\n",
        "bank_vector_3_12 = encoded_layers_3[11][0, bank_posi_3]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqi6OQC9is7K"
      },
      "source": [
        "# コサイン類似度を計算\n",
        "import torch.nn.functional as F\n",
        "\n",
        "print(\"bankの初期ベクトル と 文章1の1段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_0, bank_vector_1_1, dim=0))\n",
        "print(\"bankの初期ベクトル と 文章1の12段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_0, bank_vector_1_12, dim=0))\n",
        "\n",
        "print(\"文章1の1層目のbank と 文章2の1段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_1_1, bank_vector_2_1, dim=0))\n",
        "print(\"文章1の1層目のbank と 文章3の1段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_1_1, bank_vector_3_1, dim=0))\n",
        "\n",
        "print(\"文章1の12層目のbank と 文章2の12段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_1_12, bank_vector_2_12, dim=0))\n",
        "print(\"文章1の12層目のbank と 文章3の12段目のbankの類似度：\",\n",
        "      F.cosine_similarity(bank_vector_1_12, bank_vector_3_12, dim=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HodBeOfis7K"
      },
      "source": [
        "ここまでの内容をフォルダ「utils」のbert.pyに別途保存しておき、次節からはこちらから読み込むようにします"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NthInPGais7K"
      },
      "source": [
        "---\n",
        "# 付録"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fQcDAeWis7L"
      },
      "source": [
        "## 事前学習課題用のモジュールを実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UApit0wis7L"
      },
      "source": [
        "class BertPreTrainingHeads(nn.Module):\n",
        "    '''BERTの事前学習課題を行うアダプターモジュール'''\n",
        "\n",
        "    def __init__(self, config, bert_model_embedding_weights):\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "\n",
        "        # 事前学習課題：Masked Language Model用のモジュール\n",
        "        self.predictions = MaskedWordPredictions(config)\n",
        "\n",
        "        # 事前学習課題：Next Sentence Prediction用のモジュール\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        '''入力情報\n",
        "        sequence_output:[batch_size, seq_len, hidden_size]\n",
        "        pooled_output:[batch_size, hidden_size]\n",
        "        '''\n",
        "        # 入力のマスクされた各単語がどの単語かを判定\n",
        "        # 出力 [minibatch, seq_len, vocab_size]\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "\n",
        "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定\n",
        "        seq_relationship_score = self.seq_relationship(\n",
        "            pooled_output)  # 出力 [minibatch, 2]\n",
        "\n",
        "        return prediction_scores, seq_relationship_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pqmy_Iris7L"
      },
      "source": [
        "class BertPreTrainingHeads(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''BERTの事前学習課題を行うアダプターモジュール'''\n",
        "        super(BertPreTrainingHeads, self).__init__()\n",
        "\n",
        "        # 事前学習課題：Masked Language Model用のモジュール\n",
        "        self.predictions = MaskedWordPredictions(config)\n",
        "\n",
        "        # 事前学習課題：Next Sentence Prediction用のモジュール\n",
        "        self.seq_relationship = SeqRelationship(config, out_features=2)\n",
        "\n",
        "    def forward(self, sequence_output, pooled_output):\n",
        "        '''入力情報\n",
        "        sequence_output:[batch_size, seq_len, hidden_size]\n",
        "        pooled_output:[batch_size, hidden_size]\n",
        "        '''\n",
        "        # 入力のマスクされた各単語がどの単語かを判定\n",
        "        # 出力 [batch_size, seq_len, hidden_size]\n",
        "        prediction_scores = self.predictions(sequence_output)\n",
        "\n",
        "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定\n",
        "        seq_relationship_score = self.seq_relationship(\n",
        "            pooled_output)  # 出力 [batch_size, 2]\n",
        "\n",
        "        return prediction_scores, seq_relationship_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wezpf90ris7L"
      },
      "source": [
        "# 事前学習課題：Masked Language Model用のモジュール\n",
        "\n",
        "\n",
        "class MaskedWordPredictions(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        '''事前学習課題：Masked Language Model用のモジュール\n",
        "        元の[2]の実装では、BertLMPredictionHeadという名前です。\n",
        "        '''\n",
        "        super(MaskedWordPredictions, self).__init__()\n",
        "\n",
        "        # BERTから出力された特徴量を変換するモジュール（入出力のサイズは同じ）\n",
        "        self.transform = BertPredictionHeadTransform(config)\n",
        "\n",
        "        # self.transformの出力から、各位置の単語がどれかを当てる全結合層\n",
        "        self.decoder = nn.Linear(in_features=config.hidden_size,  # 'hidden_size': 768\n",
        "                                 out_features=config.vocab_size,  # 'vocab_size': 30522\n",
        "                                 bias=False)\n",
        "        # バイアス項\n",
        "        self.bias = nn.Parameter(torch.zeros(\n",
        "            config.vocab_size))  # 'vocab_size': 30522\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        '''\n",
        "        hidden_states：BERTからの出力[batch_size, seq_len, hidden_size]\n",
        "        '''\n",
        "        # BERTから出力された特徴量を変換\n",
        "        # 出力サイズ：[batch_size, seq_len, hidden_size]\n",
        "        hidden_states = self.transform(hidden_states)\n",
        "\n",
        "        # 各位置の単語がボキャブラリーのどの単語なのかをクラス分類で予測\n",
        "        # 出力サイズ：[batch_size, seq_len, vocab_size]\n",
        "        hidden_states = self.decoder(hidden_states) + self.bias\n",
        "\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertPredictionHeadTransform(nn.Module):\n",
        "    '''MaskedWordPredictionsにて、BERTからの特徴量を変換するモジュール（入出力のサイズは同じ）'''\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertPredictionHeadTransform, self).__init__()\n",
        "\n",
        "        # 全結合層 'hidden_size': 768\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "\n",
        "        # 活性化関数gelu\n",
        "        self.transform_act_fn = gelu\n",
        "\n",
        "        # LayerNormalization\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        '''hidden_statesはsequence_output:[minibatch, seq_len, hidden_size]'''\n",
        "        # 全結合層で特徴量変換し、活性化関数geluを計算したあと、LayerNormalizationする\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.transform_act_fn(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states)\n",
        "        return hidden_states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgK3Rwktis7L"
      },
      "source": [
        "# 事前学習課題：Next Sentence Prediction用のモジュール\n",
        "class SeqRelationship(nn.Module):\n",
        "    def __init__(self, config, out_features):\n",
        "        '''事前学習課題：Next Sentence Prediction用のモジュール\n",
        "        元の引用[2]の実装では、とくにクラスとして用意はしていない。\n",
        "        ただの全結合層に、わざわざ名前をつけた。\n",
        "        '''\n",
        "        super(SeqRelationship, self).__init__()\n",
        "\n",
        "        # 先頭単語の特徴量から1文目と2文目がつながっているかを判定するクラス分類の全結合層\n",
        "        self.seq_relationship = nn.Linear(config.hidden_size, out_features)\n",
        "\n",
        "    def forward(self, pooled_output):\n",
        "        return self.seq_relationship(pooled_output)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mrgqw1jis7M"
      },
      "source": [
        "class BertForMaskedLM(nn.Module):\n",
        "    '''BERTモデルに、事前学習課題用のアダプターモジュール\n",
        "    BertPreTrainingHeadsをつなげたモデル'''\n",
        "\n",
        "    def __init__(self, config, net_bert):\n",
        "        super(BertForMaskedLM, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = net_bert  # BERTモデル\n",
        "\n",
        "        # 事前学習課題用のアダプターモジュール\n",
        "        self.cls = BertPreTrainingHeads(config)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        encoded_layers, pooled_output = self.bert(\n",
        "            input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False, attention_show_flg=False)\n",
        "\n",
        "        # 事前学習課題の推論を実施\n",
        "        prediction_scores, seq_relationship_score = self.cls(\n",
        "            encoded_layers, pooled_output)\n",
        "\n",
        "        return prediction_scores, seq_relationship_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDysMqMfis7M"
      },
      "source": [
        "## 学習済みモデルのロード部分を実装します"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-69t8Vxis7M"
      },
      "source": [
        "# BERTの基本モデル\n",
        "net_bert = BertModel(config)\n",
        "net_bert.eval()\n",
        "\n",
        "# 事前学習課題のアダプターモジュールを搭載したBERT\n",
        "net = BertForMaskedLM(config, net_bert)\n",
        "net.eval()\n",
        "\n",
        "# 学習済みの重みをロード\n",
        "weights_path = \"./weights/pytorch_model.bin\"\n",
        "loaded_state_dict = torch.load(weights_path)\n",
        "\n",
        "\n",
        "# 現在のネットワークモデルのパラメータ名\n",
        "param_names = []  # パラメータの名前を格納していく\n",
        "\n",
        "for name, param in net.named_parameters():\n",
        "    param_names.append(name)\n",
        "\n",
        "\n",
        "# 現在のネットワークの情報をコピーして新たなstate_dictを作成\n",
        "new_state_dict = net.state_dict().copy()\n",
        "\n",
        "# 新たなstate_dictに学習済みの値を代入\n",
        "for index, (key_name, value) in enumerate(loaded_state_dict.items()):\n",
        "    name = param_names[index]  # 現在のネットワークでのパラメータ名を取得\n",
        "    new_state_dict[name] = value  # 値を入れる\n",
        "    print(str(key_name)+\"→\"+str(name))  # 何から何に入ったかを表示\n",
        "\n",
        "    # 現在のネットワークのパラメータを全部ロードしたら終える\n",
        "    if index+1 >= len(param_names):\n",
        "        break\n",
        "\n",
        "# 新たなstate_dictを構築したBERTモデルに与える\n",
        "net.load_state_dict(new_state_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuO9slGHis7M"
      },
      "source": [
        "## 事前学習課題Masked Language Modelを試す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQeCswncis7M"
      },
      "source": [
        "# 入力する文章を用意\n",
        "text = \"[CLS] I accessed the bank account. [SEP] We play soccer at the bank of the river. [SEP]\"\n",
        "\n",
        "# 単語分割Tokenizerを用意\n",
        "tokenizer = BertTokenizer(\n",
        "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
        "\n",
        "# 文章を単語分割\n",
        "tokenized_text = tokenizer.tokenize(text)\n",
        "\n",
        "print(tokenized_text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-bKN6cDis7N"
      },
      "source": [
        "# 単語をマスクする。今回は13単語目のbankをマスクして当てさせる\n",
        "masked_index = 13\n",
        "tokenized_text[masked_index] = '[MASK]'\n",
        "\n",
        "print(tokenized_text)  # 13単語目が[MASK]になっている\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLnhnkF6is7N"
      },
      "source": [
        "# 単語をIDに変換する\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "print(indexed_tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzOTeekmis7N"
      },
      "source": [
        "# 1文目に0を2文目に1を入れた文章IDを用意\n",
        "\n",
        "\n",
        "def seq2id(indexed_tokens):\n",
        "    '''分かち書きされた単語ID列を文章IDに。[SEP]で分ける'''\n",
        "\n",
        "    segments_ids = []\n",
        "    seq_id = 0\n",
        "\n",
        "    for word_id in indexed_tokens:\n",
        "        segments_ids.append(seq_id)  # seq_id=o or 1を追加\n",
        "\n",
        "        # [SEP]を発見したら2文目になるので以降idを1に\n",
        "        if word_id == 102:  # IDの102が[SEP]である\n",
        "            seq_id = 1\n",
        "\n",
        "    return segments_ids\n",
        "\n",
        "\n",
        "# 実行\n",
        "segments_ids = seq2id(indexed_tokens)\n",
        "print(segments_ids)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sErwqSRlis7N"
      },
      "source": [
        "# モデルで推論\n",
        "\n",
        "# リストをPyTorchのテンソルにしてモデルに入力\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "# 推論\n",
        "with torch.no_grad():\n",
        "    prediction_scores, seq_relationship_score = net(\n",
        "        tokens_tensor, segments_tensors)\n",
        "\n",
        "# 推論したIDを単語に戻す\n",
        "predicted_index = torch.argmax(prediction_scores[0, masked_index]).item()\n",
        "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "print(predicted_token)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_iXRNKPYis7N"
      },
      "source": [
        "## 事前学習課題Next Sentence Predictionを試す"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJi4MVUgis7N"
      },
      "source": [
        "print(seq_relationship_score)\n",
        "print(torch.sigmoid(seq_relationship_score))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5ZbVYwkis7O"
      },
      "source": [
        "# 上記の出力を見ると、クラス0：2つの文章が意味のあるれんぞくしたもの、クラス1：2つの文章は無関係\n",
        "# のうち、クラス1の無関係と判定。\n",
        "\n",
        "# 入力文章は\n",
        "# text = \"[CLS] I accessed the bank account. [SEP] We play soccer at the bank of the river. [SEP]\"\n",
        "# と無関係だったので、きちんとNext Sentence Predictionができていることが分かる。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTMLURjRis7O"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iOoweizTngx"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/05. "
      ]
    }
  ]
}