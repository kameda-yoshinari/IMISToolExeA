{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "GC8-4_bert_IMDb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/8_nlp_sentiment_bert/GC8_4_bert_IMDb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoycL_XoT5tk"
      },
      "source": [
        "# 8.4 BERTを用いたレビュー文章に対する感情分析モデルの実装と学習・推論\n",
        "\n",
        "本ファイルでは、BERTを使用し、IMDbデータのポジ・ネガを分類するモデルを学習させ、推論します。また推論時のSelf-Attentionを可視化します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcWZ5xD-T5tm"
      },
      "source": [
        "※　本章のファイルはすべて Google Colaboratory (Ubuntu) での動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC-LAXWET5tn"
      },
      "source": [
        "# 8.4 学習目標\n",
        "\n",
        "1.\tBERTのボキャブラリーをtorchtextで使用する実装方法を理解する\n",
        "2.\tBERTに分類タスク用のアダプターモジュールを追加し、感情分析を実施するモデルを実装できる\n",
        "3.\tBERTをファインチューニングして、モデルを学習できる\n",
        "4.  BERTのSelf-Attentionの重みを可視化し、推論の説明を試みることができる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"8_nlp_sentiment_bert\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0mbX2L9YqiQ"
      },
      "source": [
        "---\n",
        "# 事前準備"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmhkHYmhYyDk"
      },
      "source": [
        "# porting from 7_nlp_sentiment_transformer\n",
        "!ls -l ../7_nlp_sentiment_transformer/data/IMDb_train.tsv ../7_nlp_sentiment_transformer/data/IMDb_test.tsv\n",
        "!rm -rf   /root/data/\n",
        "!mkdir -p /root/data/\n",
        "!cp ../7_nlp_sentiment_transformer/data/IMDb_train.tsv ../7_nlp_sentiment_transformer/data/IMDb_test.tsv /root/data/\n",
        "!rm -f ./data/IMDb_train.tsv ./data/IMDb_test.tsv\n",
        "!ln -s /root/data/IMDb_train.tsv ./data/\n",
        "!ln -s /root/data/IMDb_test.tsv ./data/\n",
        "!ls -l ./data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV6o65ZiT5tn"
      },
      "source": [
        "---\n",
        "# 8.4 節\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO55-TCiV2U1"
      },
      "source": [
        "!pip install attrdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdYNqp9cT5to"
      },
      "source": [
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch \n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import torchtext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRsM0wJ7T5to"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOwCOqDhT5to"
      },
      "source": [
        "# IMDbデータを読み込み\n",
        "\n",
        "* Very similar to 「2.前処理と単語分割の関数を定義」 in [GC7-5_IMDb_Dataset_DataLoader.ipynb](https://github.com/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/7_nlp_sentiment_transformer/GC7_5_IMDb_Dataset_DataLoader.ipynb)\n",
        "* BERTのTokenizerを使用\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkY6kQm5T5tp"
      },
      "source": [
        "# 前処理と単語分割をまとめた関数を作成\n",
        "import re\n",
        "import string\n",
        "from utils.bert import BertTokenizer\n",
        "# フォルダ「utils」のbert.pyより\n",
        "\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    '''IMDbの前処理'''\n",
        "    # 改行コードを消去\n",
        "    text = re.sub('<br />', '', text)\n",
        "\n",
        "    # カンマ、ピリオド以外の記号をスペースに置換\n",
        "    for p in string.punctuation:\n",
        "        if (p == \".\") or (p == \",\"):\n",
        "            continue\n",
        "        else:\n",
        "            text = text.replace(p, \" \")\n",
        "\n",
        "    # ピリオドなどの前後にはスペースを入れておく\n",
        "    text = text.replace(\".\", \" . \")\n",
        "    text = text.replace(\",\", \" , \")\n",
        "    return text\n",
        "\n",
        "\n",
        "# 単語分割用のTokenizerを用意\n",
        "tokenizer_bert = BertTokenizer(\n",
        "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\", do_lower_case=True)\n",
        "\n",
        "\n",
        "# 前処理と単語分割をまとめた関数を定義\n",
        "# 単語分割の関数を渡すので、tokenizer_bertではなく、tokenizer_bert.tokenizeを渡す点に注意\n",
        "def tokenizer_with_preprocessing(text, tokenizer=tokenizer_bert.tokenize):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer(text)  # tokenizer_bert\n",
        "    return ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP4uRgqtXCyl"
      },
      "source": [
        "# DataLoaderの作成\n",
        "\n",
        "* Very similar to 「DataLoaderの作成」 in [GC7-5_IMDb_Dataset_DataLoader.ipynb](https://github.com/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/7_nlp_sentiment_transformer/GC7_5_IMDb_Dataset_DataLoader.ipynb)\n",
        "* BERTのTokenizerを使用"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Syb7wXxiT5tp"
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/148\n",
        "max_length = 256\n",
        "\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
        "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"[CLS]\", eos_token=\"[SEP]\", pad_token='[PAD]', unk_token='[UNK]')\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# (注釈)：各引数を再確認\n",
        "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
        "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
        "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
        "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
        "# include_length: 文章の単語数のデータを保持するか\n",
        "# batch_first：ミニバッチの次元を先頭に用意するかどうか\n",
        "# fix_length：全部の文章を指定した長さと同じになるように、paddingします\n",
        "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyYzn5waT5tq"
      },
      "source": [
        "# フォルダ「data」から各tsvファイルを読み込みます\n",
        "# BERT用で処理するので、時間がかかります\n",
        "# I takes 3 minutes or os on GC\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/148\n",
        "\n",
        "train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='./data/', train='IMDb_train.tsv',\n",
        "    test='IMDb_test.tsv', format='tsv',\n",
        "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n",
        "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
        "train_ds, val_ds = train_val_ds.split(\n",
        "    split_ratio=0.8, random_state=random.seed(1234))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY_j_Q7tX1Q8"
      },
      "source": [
        "# BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zARRiwoOT5tq"
      },
      "source": [
        "# BERTはBERTが持つ全単語でBertEmbeddingモジュールを作成しているので、ボキャブラリーとしては全単語を使用します\n",
        "# そのため訓練データからボキャブラリーは作成しません\n",
        "\n",
        "# まずBERT用の単語辞書を辞書型変数に用意します\n",
        "from utils.bert import BertTokenizer, load_vocab\n",
        "\n",
        "vocab_bert, ids_to_tokens_bert = load_vocab(\n",
        "    vocab_file=\"./vocab/bert-base-uncased-vocab.txt\")\n",
        "\n",
        "\n",
        "# このまま、TEXT.vocab.stoi= vocab_bert (stoiはstring_to_IDで、単語からIDへの辞書)としたいですが、\n",
        "# 一度bulild_vocabを実行しないとTEXTオブジェクトがvocabのメンバ変数をもってくれないです。\n",
        "# （'Field' object has no attribute 'vocab' というエラーをはきます）\n",
        "\n",
        "# 1度適当にbuild_vocabでボキャブラリーを作成してから、BERTのボキャブラリーを上書きします\n",
        "TEXT.build_vocab(train_ds, min_freq=1)\n",
        "TEXT.vocab.stoi = vocab_bert\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bTdgyTzT5tq"
      },
      "source": [
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "batch_size = 32  # BERTでは16、32あたりを使用する\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/148\n",
        "\n",
        "train_dl = torchtext.legacy.data.Iterator(\n",
        "    train_ds, batch_size=batch_size, train=True)\n",
        "\n",
        "val_dl = torchtext.legacy.data.Iterator(\n",
        "    val_ds, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "test_dl = torchtext.legacy.data.Iterator(\n",
        "    test_ds, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip1hTmD4T5tr"
      },
      "source": [
        "# 動作確認 検証データのデータセットで確認\n",
        "batch = next(iter(val_dl))\n",
        "print(batch.Text)\n",
        "print(batch.Label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvM588pwT5tr"
      },
      "source": [
        "# ミニバッチの1文目を確認してみる\n",
        "text_minibatch_1 = (batch.Text[0][1]).numpy()\n",
        "\n",
        "# IDを単語に戻す\n",
        "text = tokenizer_bert.convert_ids_to_tokens(text_minibatch_1)\n",
        "\n",
        "print(text)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqAy6qiOT5ts"
      },
      "source": [
        "# 感情分析用のBERTモデルを構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlGcl6wFT5ts"
      },
      "source": [
        "from utils.bert import get_config, BertModel, set_learned_params\n",
        "\n",
        "# モデル設定のJOSNファイルをオブジェクト変数として読み込みます\n",
        "config = get_config(file_path=\"./weights/bert_config.json\")\n",
        "\n",
        "# BERTモデルを作成します\n",
        "net_bert = BertModel(config)\n",
        "\n",
        "# BERTモデルに学習済みパラメータセットします\n",
        "net_bert = set_learned_params(\n",
        "    net_bert, weights_path=\"./weights/pytorch_model.bin\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYRzZZ5iT5ts"
      },
      "source": [
        "class BertForIMDb(nn.Module):\n",
        "    '''BERTモデルにIMDbのポジ・ネガを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self, net_bert):\n",
        "        super(BertForIMDb, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = net_bert  # BERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元、出力はポジ・ネガの2つ\n",
        "        self.cls = nn.Linear(in_features=768, out_features=2)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=False, attention_show_flg=False):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        token_type_ids： [batch_size, sequence_length]の、各単語が1文目なのか、2文目なのかを示すid\n",
        "        attention_mask：Transformerのマスクと同じ働きのマスキングです\n",
        "        output_all_encoded_layers：最終出力に12段のTransformerの全部をリストで返すか、最後だけかを指定\n",
        "        attention_show_flg：Self-Attentionの重みを返すかのフラグ\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        if attention_show_flg == True:\n",
        "            '''attention_showのときは、attention_probsもリターンする'''\n",
        "            encoded_layers, pooled_output, attention_probs = self.bert(\n",
        "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
        "        elif attention_show_flg == False:\n",
        "            encoded_layers, pooled_output = self.bert(\n",
        "                input_ids, token_type_ids, attention_mask, output_all_encoded_layers, attention_show_flg)\n",
        "\n",
        "        # 入力文章の1単語目[CLS]の特徴量を使用して、ポジ・ネガを分類します\n",
        "        vec_0 = encoded_layers[:, 0, :]\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_sizeに変換\n",
        "        out = self.cls(vec_0)\n",
        "\n",
        "        # attention_showのときは、attention_probs（1番最後の）もリターンする\n",
        "        if attention_show_flg == True:\n",
        "            return out, attention_probs\n",
        "        elif attention_show_flg == False:\n",
        "            return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmTzj7l0T5tt"
      },
      "source": [
        "# モデル構築\n",
        "net = BertForIMDb(net_bert)\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_lA-uJGT5tt"
      },
      "source": [
        "# BERTのファインチューニングに向けた設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCXBVg4GT5tt"
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for name, param in net.named_parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. 最後のBertLayerモジュールを勾配計算ありに変更\n",
        "for name, param in net.bert.encoder.layer[-1].named_parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for name, param in net.cls.named_parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EUgfs-wT5tt"
      },
      "source": [
        "# 最適化手法の設定\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 5e-5}\n",
        "], betas=(0.9, 0.999))\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOyKW7ZLT5tu"
      },
      "source": [
        "# 学習・検証を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQiGcYvYT5tu"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # 開始時刻を保存\n",
        "            t_epoch_start = time.time()\n",
        "            t_iter_start = time.time()\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BertForIMDbに入力\n",
        "                    outputs = net(inputs, token_type_ids=None, attention_mask=None,\n",
        "                                  output_all_encoded_layers=False, attention_show_flg=False)\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(), duration, acc))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            t_epoch_finish = time.time()\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "            t_epoch_start = time.time()\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qZ-pXTET5tu"
      },
      "source": [
        "# 学習・検証を実行する。 \n",
        "# 8 minutes per epoch, 16 minutes in total on GC with GPU\n",
        "\n",
        "num_epochs = 2\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWTS_gSOT5tv"
      },
      "source": [
        "# 学習したネットワークパラメータを保存します\n",
        "save_path = './weights/bert_fine_tuning_IMDb.pth'\n",
        "torch.save(net_trained.state_dict(), save_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36JYaufeT5tv"
      },
      "source": [
        "# テストデータでの正解率を求める\n",
        "# it takes 8 minutes or so (GC with GPU)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(test_dl):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForIMDbに入力\n",
        "        outputs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
        "                              output_all_encoded_layers=False, attention_show_flg=False)\n",
        "\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset), epoch_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "168YPhmvT5tv"
      },
      "source": [
        "# Attentionの可視化"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuWBCGxKT5tv"
      },
      "source": [
        "# batch_sizeを64にしたテストデータでDataLoaderを作成\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/148\n",
        "\n",
        "batch_size = 64\n",
        "test_dl = torchtext.legacy.data.Iterator(\n",
        "    test_ds, batch_size=batch_size, train=False, sort=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdf04xCTT5tv"
      },
      "source": [
        "# BertForIMDbで処理\n",
        "\n",
        "# ミニバッチの用意\n",
        "batch = next(iter(test_dl))\n",
        "\n",
        "# GPUが使えるならGPUにデータを送る\n",
        "inputs = batch.Text[0].to(device)  # 文章\n",
        "labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "outputs, attention_probs = net_trained(inputs, token_type_ids=None, attention_mask=None,\n",
        "                                       output_all_encoded_layers=False, attention_show_flg=True)\n",
        "\n",
        "_, preds = torch.max(outputs, 1)  # ラベルを予測\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svGK4qb8T5tw"
      },
      "source": [
        "# HTMLを作成する関数を実装\n",
        "\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
        "\n",
        "\n",
        "def mk_html(index, batch, preds, normlized_weights, TEXT):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    sentence = batch.Text[0][index]  # 文章\n",
        "    label = batch.Label[index]  # ラベル\n",
        "    pred = preds[index]  # 予測\n",
        "\n",
        "    # ラベルと予測結果を文字に置き換え\n",
        "    if label == 0:\n",
        "        label_str = \"Negative\"\n",
        "    else:\n",
        "        label_str = \"Positive\"\n",
        "\n",
        "    if pred == 0:\n",
        "        pred_str = \"Negative\"\n",
        "    else:\n",
        "        pred_str = \"Positive\"\n",
        "\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
        "\n",
        "    # Self-Attentionの重みを可視化。Multi-Headが12個なので、12種類のアテンションが存在\n",
        "    for i in range(12):\n",
        "\n",
        "        # indexのAttentionを抽出と規格化\n",
        "        # 0単語目[CLS]の、i番目のMulti-Head Attentionを取り出す\n",
        "        # indexはミニバッチの何個目のデータかをしめす\n",
        "        attens = normlized_weights[index, i, 0, :]\n",
        "        attens /= attens.max()\n",
        "\n",
        "        html += '[BERTのAttentionを可視化_' + str(i+1) + ']<br>'\n",
        "        for word, attn in zip(sentence, attens):\n",
        "\n",
        "            # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
        "            if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
        "                break\n",
        "\n",
        "            # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
        "            html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
        "                [word.numpy().tolist()])[0], attn)\n",
        "        html += \"<br><br>\"\n",
        "\n",
        "    # 12種類のAttentionの平均を求める。最大値で規格化\n",
        "    all_attens = attens*0  # all_attensという変数を作成する\n",
        "    for i in range(12):\n",
        "        attens += normlized_weights[index, i, 0, :]\n",
        "    attens /= attens.max()\n",
        "\n",
        "    html += '[BERTのAttentionを可視化_ALL]<br>'\n",
        "    for word, attn in zip(sentence, attens):\n",
        "\n",
        "        # 単語が[SEP]の場合は文章が終わりなのでbreak\n",
        "        if tokenizer_bert.convert_ids_to_tokens([word.numpy().tolist()])[0] == \"[SEP]\":\n",
        "            break\n",
        "\n",
        "        # 関数highlightで色をつける、関数tokenizer_bert.convert_ids_to_tokensでIDを単語に戻す\n",
        "        html += highlight(tokenizer_bert.convert_ids_to_tokens(\n",
        "            [word.numpy().tolist()])[0], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_zmplqST5tw"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "index = 3  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, attention_probs, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvve5kcOT5tw"
      },
      "source": [
        "index = 61  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, attention_probs, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_IHTT6VT5tw"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nNYDnDTcNYm"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/05. "
      ]
    }
  ]
}