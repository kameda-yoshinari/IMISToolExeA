{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC1-1_load_vgg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/1_image_classification/GC1_1_load_vgg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBGEws_BNH2P"
      },
      "source": [
        "# 1.1「学習済みVGGモデル」を使用する方法\n",
        "\n",
        "- 本ipynbでは、学習済みのVGGモデルを使用し、未知の画像（ゴールデンレトリバー）を分類します\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBfSv2DyNH2W"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "1.\tPyTorchでImangeNetデータセットでの学習済みモデルをロードできるようになる\n",
        "2.\tVGGモデルについて理解する\n",
        "3.\t入力画像のサイズや色を変換できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nG4o2ueryyi3"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWa96dW8yyi_"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUtgaSlfyyjA"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nsa8fL1yyjA"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFiA1xJ0yyjA"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK1piLdryyjB"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYmin35HyyjB"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDclndJayyjB"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ULzOBxyyjC"
      },
      "source": [
        "%cd \"1_image_classification\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sbZ63j7yyjC"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFizVjJC2ru5"
      },
      "source": [
        "---\n",
        "# データ準備\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhufbJ0e56ye"
      },
      "source": [
        "1-1節の内容に入る準備として make_folders_and_data_downloads.ipynbの中身を実行"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KefQTosN3G9_"
      },
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "url = \"https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\"\n",
        "save_path = os.path.join(data_dir, \"imagenet_class_index.json\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "\n",
        "url = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\"\n",
        "save_path = os.path.join(data_dir, \"hymenoptera_data.zip\")\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    urllib.request.urlretrieve(url, save_path)\n",
        "\n",
        "    # ZIPファイルを読み込み\n",
        "    zip = zipfile.ZipFile(save_path)\n",
        "    zip.extractall(data_dir)  # ZIPを解凍\n",
        "    zip.close()  # ZIPファイルをクローズ\n",
        "\n",
        "    # ZIPファイルを消去\n",
        "    os.remove(save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpQa-OCQNH2X"
      },
      "source": [
        "---\n",
        "# パッケージのimportとPyTorchのバージョンを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTYHPeyGNH2Y"
      },
      "source": [
        "# パッケージのimport\n",
        "import numpy as np\n",
        "import json\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models, transforms\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWFzRfYwNH2Z"
      },
      "source": [
        "# PyTorchのバージョン確認\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQj18-KqNH2Z"
      },
      "source": [
        "# VGG-16の学習済みモデルをロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugoB5bhBNH2a"
      },
      "source": [
        "# 学習済みのVGG-16モデルをロード\n",
        "# 初めて実行する際は、学習済みパラメータをダウンロードするため、実行に時間がかかります\n",
        "\n",
        "# VGG-16モデルのインスタンスを生成\n",
        "use_pretrained = True  # 学習済みのパラメータを使用\n",
        "net = models.vgg16(pretrained=use_pretrained)\n",
        "net.eval()  # 推論モードに設定\n",
        "\n",
        "# モデルのネットワーク構成を出力\n",
        "print(net)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbnucsIQNH2a"
      },
      "source": [
        "# 入力画像の前処理クラスを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wegLsyJKNH2a"
      },
      "source": [
        "# 入力画像の前処理のクラス\n",
        "class BaseTransform():\n",
        "    \"\"\"\n",
        "    画像のサイズをリサイズし、色を標準化する。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    resize : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    mean : (R, G, B)\n",
        "        各色チャネルの平均値。\n",
        "    std : (R, G, B)\n",
        "        各色チャネルの標準偏差。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, resize, mean, std):\n",
        "        self.base_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),  # 短い辺の長さがresizeの大きさになる\n",
        "            transforms.CenterCrop(resize),  # 画像中央をresize × resizeで切り取り\n",
        "            transforms.ToTensor(),  # Torchテンソルに変換\n",
        "            transforms.Normalize(mean, std)  # 色情報の標準化\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        return self.base_transform(img)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyPkp-v9NH2b"
      },
      "source": [
        "# 画像前処理の動作を確認\n",
        "\n",
        "# 1. 画像読み込み\n",
        "image_file_path = './data/goldenretriever-3724972_640.jpg'\n",
        "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "# 2. 元の画像の表示\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# 3. 画像の前処理と処理済み画像の表示\n",
        "resize = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "transform = BaseTransform(resize, mean, std)\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "\n",
        "# (色、高さ、幅)を (高さ、幅、色)に変換し、0-1に値を制限して表示\n",
        "img_transformed = img_transformed.numpy().transpose((1, 2, 0))\n",
        "img_transformed = np.clip(img_transformed, 0, 1)\n",
        "plt.imshow(img_transformed)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z2ZWxWnNH2b"
      },
      "source": [
        "# 出力結果からラベルを予測する後処理クラスを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uWvtv09NH2c"
      },
      "source": [
        "# ILSVRCのラベル情報をロードし辞書型変数を生成します\n",
        "ILSVRC_class_index = json.load(open('./data/imagenet_class_index.json', 'r'))\n",
        "ILSVRC_class_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fGbk8QpNH2c"
      },
      "source": [
        "# 出力結果からラベルを予測する後処理クラス\n",
        "class ILSVRCPredictor():\n",
        "    \"\"\"\n",
        "    ILSVRCデータに対するモデルの出力からラベルを求める。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    class_index : dictionary\n",
        "            クラスindexとラベル名を対応させた辞書型変数。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_index):\n",
        "        self.class_index = class_index\n",
        "\n",
        "    def predict_max(self, out):\n",
        "        \"\"\"\n",
        "        確率最大のILSVRCのラベル名を取得する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        out : torch.Size([1, 1000])\n",
        "            Netからの出力。\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        predicted_label_name : str\n",
        "            最も予測確率が高いラベルの名前\n",
        "        \"\"\"\n",
        "        maxid = np.argmax(out.detach().numpy())\n",
        "        predicted_label_name = self.class_index[str(maxid)][1]\n",
        "\n",
        "        return predicted_label_name\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMbCCgJvNH2d"
      },
      "source": [
        "# 学習済みVGGモデルで手元の画像を予測"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EhD_ZQYNH2d"
      },
      "source": [
        "# Just to surpress UserWarning\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "# ILSVRCのラベル情報をロードし辞意書型変数を生成します\n",
        "ILSVRC_class_index = json.load(open('./data/imagenet_class_index.json', 'r'))\n",
        "\n",
        "# ILSVRCPredictorのインスタンスを生成します\n",
        "predictor = ILSVRCPredictor(ILSVRC_class_index)\n",
        "\n",
        "# 入力画像を読み込む\n",
        "image_file_path = './data/goldenretriever-3724972_640.jpg'\n",
        "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "# 前処理の後、バッチサイズの次元を追加する\n",
        "transform = BaseTransform(resize, mean, std)  # 前処理クラス作成\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
        "\n",
        "# モデルに入力し、モデル出力をラベルに変換する\n",
        "out = net(inputs)  # torch.Size([1, 1000])\n",
        "result = predictor.predict_max(out)\n",
        "\n",
        "# 予測結果を出力する\n",
        "print(\"入力画像の予測結果：\", result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Cs8IdOrNH2d"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpmIpS9tBG8z"
      },
      "source": [
        "---\n",
        "# 持ち込み画像に対する認識\n",
        "\n",
        "[U-Tsukuba] Extra remarks\n",
        "\n",
        "Change the image of ./data/goldenretriever-3724972_640.jpg in the previous script and see the result.  \n",
        "For example, obtain akita-inu image and see what will happen.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQgrRseBBxZ-"
      },
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/7/78/Akita_inu.jpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukmBuQ4BCbiG"
      },
      "source": [
        "# 1. 画像読み込み\n",
        "image_file_path = './Akita_inu.jpeg'\n",
        "img = Image.open(image_file_path)  # [高さ][幅][色RGB]\n",
        "\n",
        "# 2. 元の画像の表示\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8KRshQyBZNa"
      },
      "source": [
        "# 前処理の後、バッチサイズの次元を追加する\n",
        "transform = BaseTransform(resize, mean, std)  # 前処理クラス作成\n",
        "img_transformed = transform(img)  # torch.Size([3, 224, 224])\n",
        "inputs = img_transformed.unsqueeze_(0)  # torch.Size([1, 3, 224, 224])\n",
        "\n",
        "# モデルに入力し、モデル出力をラベルに変換する\n",
        "out = net(inputs)  # torch.Size([1, 1000])\n",
        "result = predictor.predict_max(out)\n",
        "\n",
        "# 予測結果を出力する\n",
        "print(\"入力画像の予測結果：\", result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBJFkga2D_YS"
      },
      "source": [
        "Since the Akita-inu is prepared as a class, their answer is Eskimo dog :-)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L3-spL57ges"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/02. \n"
      ]
    }
  ]
}