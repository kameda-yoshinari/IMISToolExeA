{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC2-8_SSD_inference_appendix.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vbtHk7ktuezu"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/2_objectdetection/GC2_8_SSD_inference_appendix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgGVSTedjf6W"
      },
      "source": [
        "# 2.8 推論の実施の付録：検証の実施\n",
        "\n",
        "本ipynbでは、学習させたSSDで物体検出を行います。\n",
        "\n",
        "\n",
        "VOC2012の訓練データセットと検証データセットに対して、学習済みSSDの推論を実施し、推論結果と正しい答えであるアノテーションデータの両方を表示させるファイルです。\n",
        "\n",
        "学習させたSSDモデルが正しいアノテーションデータとどれくらい近いのかなどを確認したいケースでは、こちらもご使用ください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRUqU4tnxmZl"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVasAdJYxmZp"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMAZ5ELyxmZp"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvNuqvC4xmZq"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b2qZLfoxmZq"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGkLljtdxmZq"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcgSKlMxxmZr"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um0EwrGSxmZr"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqEb59O9xmZr"
      },
      "source": [
        "%cd \"2_objectdetection\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNqNiyHFxmZr"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ormBHZ_Kjf6Z"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "\n",
        "- フォルダ「utils」に2.3～2.7までで実装した内容をまとめたssd_model.pyがあることを確認してください\n",
        "- 学習させた重みパラメータを用意\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbtHk7ktuezu"
      },
      "source": [
        "# utils/ssd_model.py\n",
        "\n",
        "[U-Tsukuba] utils/ssd_model.py should be a new one to cope with new pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ada9ZeHZuEiQ"
      },
      "source": [
        "%%writefile utils/ssd_model.py\n",
        "\"\"\"\n",
        "第2章SSDで実装した内容をまとめたファイル\n",
        "PyTorch1.5以降で発生するエラーへの対応済 (KAMEDA Yoshinari)\n",
        "https://github.com/YutaroOgawa/pytorch_advanced/issues/71\n",
        "\"\"\"\n",
        "\n",
        "# パッケージのimport\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Function\n",
        "import torch.utils.data as data\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os.path as osp\n",
        "from itertools import product as product\n",
        "from math import sqrt as sqrt\n",
        "\n",
        "# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# フォルダ「utils」のdata_augumentation.pyからimport。入力画像の前処理をするクラス\n",
        "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "# フォルダ「utils」にある関数matchを記述したmatch.pyからimport\n",
        "from utils.match import match\n",
        "\n",
        "\n",
        "# 学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    データへのパスを格納したリストを作成する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        データフォルダへのパス\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        データへのパスを格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "\n",
        "    # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "\n",
        "    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        train_img_list.append(img_path)  # リストに追加\n",
        "        train_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        val_img_list.append(img_path)  # リストに追加\n",
        "        val_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "\n",
        "\n",
        "# 「XML形式のアノテーション」を、リスト形式に変換するクラス\n",
        "\n",
        "\n",
        "class Anno_xml2list(object):\n",
        "    \"\"\"\n",
        "    1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    classes : リスト\n",
        "        VOCのクラス名を格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        \"\"\"\n",
        "        1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xml_path : str\n",
        "            xmlファイルへのパス。\n",
        "        width : int\n",
        "            対象画像の幅。\n",
        "        height : int\n",
        "            対象画像の高さ。\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "            物体のアノテーションデータを格納したリスト。画像内に存在する物体数分のだけ要素を持つ。\n",
        "        \"\"\"\n",
        "\n",
        "        # 画像内の全ての物体のアノテーションをこのリストに格納します\n",
        "        ret = []\n",
        "\n",
        "        # xmlファイルを読み込む\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        # 画像内にある物体（object）の数だけループする\n",
        "        for obj in xml.iter('object'):\n",
        "\n",
        "            # アノテーションで検知がdifficultに設定されているものは除外\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "\n",
        "            # 1つの物体に対するアノテーションを格納するリスト\n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()  # 物体名\n",
        "            bbox = obj.find('bndbox')  # バウンディングボックスの情報\n",
        "\n",
        "            # アノテーションの xmin, ymin, xmax, ymaxを取得し、0～1に規格化\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                # VOCは原点が(1,1)なので1を引き算して（0, 0）に\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1\n",
        "\n",
        "                # 幅、高さで規格化\n",
        "                if pt == 'xmin' or pt == 'xmax':  # x方向のときは幅で割算\n",
        "                    cur_pixel /= width\n",
        "                else:  # y方向のときは高さで割算\n",
        "                    cur_pixel /= height\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            # アノテーションのクラス名のindexを取得して追加\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            # resに[xmin, ymin, xmax, ymax, label_ind]を足す\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "\n",
        "\n",
        "# 入力画像の前処理をするクラス\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とアノテーションの前処理クラス。訓練と推論で異なる動作をする。\n",
        "    画像のサイズを300x300にする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_size : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    color_mean : (B, G, R)\n",
        "        各色チャネルの平均値。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # intをfloat32に変換\n",
        "                ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n",
        "                PhotometricDistort(),  # 画像の色調などをランダムに変化\n",
        "                Expand(color_mean),  # 画像のキャンバスを広げる\n",
        "                RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n",
        "                RandomMirror(),  # 画像を反転させる\n",
        "                ToPercentCoords(),  # アノテーションデータを0-1に規格化\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # intをfloatに変換\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img, boxes, labels)\n",
        "\n",
        "\n",
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    anno_list : リスト\n",
        "        アノテーションへのパスを格納したリスト\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "    transform_anno : object\n",
        "        xmlのアノテーションをリストに変換するインスタンス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  # train もしくは valを指定\n",
        "        self.transform = transform  # 画像の変形\n",
        "        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のテンソル形式のデータとアノテーションを取得\n",
        "        '''\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "        height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "        # 2. xml形式のアノテーション情報をリストに\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "\n",
        "        # 3. 前処理を実施\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "\n",
        "        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n",
        "        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width\n",
        "\n",
        "\n",
        "def od_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。\n",
        "    画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
        "    この変化に対応したDataLoaderを作成するために、\n",
        "    カスタイマイズした、collate_fnを作成します。\n",
        "    collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
        "    ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
        "    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
        "    \"\"\"\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0] は画像imgです\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n",
        "\n",
        "    # imgsはミニバッチサイズのリストになっています\n",
        "    # リストの要素はtorch.Size([3, 300, 300])です。\n",
        "    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targetsはアノテーションデータの正解であるgtのリストです。\n",
        "    # リストのサイズはミニバッチサイズです。\n",
        "    # リストtargetsの要素は [n, 5] となっています。\n",
        "    # nは画像ごとに異なり、画像内にある物体の数となります。\n",
        "    # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
        "\n",
        "    return imgs, targets\n",
        "\n",
        "\n",
        "# 35層にわたる、vggモジュールを作成\n",
        "def make_vgg():\n",
        "    layers = []\n",
        "    in_channels = 3  # 色チャネル数\n",
        "\n",
        "    # vggモジュールで使用する畳み込み層やマックスプーリングのチャネル数\n",
        "    cfg = [64, 64, 'M', 128, 128, 'M', 256, 256,\n",
        "           256, 'MC', 512, 512, 512, 'M', 512, 512, 512]\n",
        "\n",
        "    for v in cfg:\n",
        "        if v == 'M':\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "        elif v == 'MC':\n",
        "            # ceilは出力サイズを、計算結果（float）に対して、切り上げで整数にするモード\n",
        "            # デフォルトでは出力サイズを計算結果（float）に対して、切り下げで整数にするfloorモード\n",
        "            layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]\n",
        "        else:\n",
        "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "            layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "            in_channels = v\n",
        "\n",
        "    pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "    conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)\n",
        "    conv7 = nn.Conv2d(1024, 1024, kernel_size=1)\n",
        "    layers += [pool5, conv6,\n",
        "               nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "# 8層にわたる、extrasモジュールを作成\n",
        "def make_extras():\n",
        "    layers = []\n",
        "    in_channels = 1024  # vggモジュールから出力された、extraに入力される画像チャネル数\n",
        "\n",
        "    # extraモジュールの畳み込み層のチャネル数を設定するコンフィギュレーション\n",
        "    cfg = [256, 512, 128, 256, 128, 256, 128, 256]\n",
        "\n",
        "    layers += [nn.Conv2d(in_channels, cfg[0], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[0], cfg[1], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[1], cfg[2], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[2], cfg[3], kernel_size=(3), stride=2, padding=1)]\n",
        "    layers += [nn.Conv2d(cfg[3], cfg[4], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[4], cfg[5], kernel_size=(3))]\n",
        "    layers += [nn.Conv2d(cfg[5], cfg[6], kernel_size=(1))]\n",
        "    layers += [nn.Conv2d(cfg[6], cfg[7], kernel_size=(3))]\n",
        "\n",
        "    return nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "# デフォルトボックスのオフセットを出力するloc_layers、\n",
        "# デフォルトボックスに対する各クラスの確率を出力するconf_layersを作成\n",
        "\n",
        "\n",
        "def make_loc_conf(num_classes=21, bbox_aspect_num=[4, 6, 6, 6, 4, 4]):\n",
        "\n",
        "    loc_layers = []\n",
        "    conf_layers = []\n",
        "\n",
        "    # VGGの22層目、conv4_3（source1）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[0]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # VGGの最終層（source2）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(1024, bbox_aspect_num[1]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source3）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(512, bbox_aspect_num[2]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source4）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[3]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source5）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[4]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    # extraの（source6）に対する畳み込み層\n",
        "    loc_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                             * 4, kernel_size=3, padding=1)]\n",
        "    conf_layers += [nn.Conv2d(256, bbox_aspect_num[5]\n",
        "                              * num_classes, kernel_size=3, padding=1)]\n",
        "\n",
        "    return nn.ModuleList(loc_layers), nn.ModuleList(conf_layers)\n",
        "\n",
        "\n",
        "# convC4_3からの出力をscale=20のL2Normで正規化する層\n",
        "class L2Norm(nn.Module):\n",
        "    def __init__(self, input_channels=512, scale=20):\n",
        "        super(L2Norm, self).__init__()  # 親クラスのコンストラクタ実行\n",
        "        self.weight = nn.Parameter(torch.Tensor(input_channels))\n",
        "        self.scale = scale  # 係数weightの初期値として設定する値\n",
        "        self.reset_parameters()  # パラメータの初期化\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        '''結合パラメータを大きさscaleの値にする初期化を実行'''\n",
        "        init.constant_(self.weight, self.scale)  # weightの値がすべてscale（=20）になる\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''38×38の特徴量に対して、512チャネルにわたって2乗和のルートを求めた\n",
        "        38×38個の値を使用し、各特徴量を正規化してから係数をかけ算する層'''\n",
        "\n",
        "        # 各チャネルにおける38×38個の特徴量のチャネル方向の2乗和を計算し、\n",
        "        # さらにルートを求め、割り算して正規化する\n",
        "        # normのテンソルサイズはtorch.Size([batch_num, 1, 38, 38])になります\n",
        "        norm = x.pow(2).sum(dim=1, keepdim=True).sqrt()+self.eps\n",
        "        x = torch.div(x, norm)\n",
        "\n",
        "        # 係数をかける。係数はチャネルごとに1つで、512個の係数を持つ\n",
        "        # self.weightのテンソルサイズはtorch.Size([512])なので\n",
        "        # torch.Size([batch_num, 512, 38, 38])まで変形します\n",
        "        weights = self.weight.unsqueeze(\n",
        "            0).unsqueeze(2).unsqueeze(3).expand_as(x)\n",
        "        out = weights * x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# デフォルトボックスを出力するクラス\n",
        "class DBox(object):\n",
        "    def __init__(self, cfg):\n",
        "        super(DBox, self).__init__()\n",
        "\n",
        "        # 初期設定\n",
        "        self.image_size = cfg['input_size']  # 画像サイズの300\n",
        "        # [38, 19, …] 各sourceの特徴量マップのサイズ\n",
        "        self.feature_maps = cfg['feature_maps']\n",
        "        self.num_priors = len(cfg[\"feature_maps\"])  # sourceの個数=6\n",
        "        self.steps = cfg['steps']  # [8, 16, …] DBoxのピクセルサイズ\n",
        "\n",
        "        self.min_sizes = cfg['min_sizes']\n",
        "        # [30, 60, …] 小さい正方形のDBoxのピクセルサイズ(正確には面積)\n",
        "\n",
        "        self.max_sizes = cfg['max_sizes']\n",
        "        # [60, 111, …] 大きい正方形のDBoxのピクセルサイズ(正確には面積)\n",
        "\n",
        "        self.aspect_ratios = cfg['aspect_ratios']  # 長方形のDBoxのアスペクト比\n",
        "\n",
        "    def make_dbox_list(self):\n",
        "        '''DBoxを作成する'''\n",
        "        mean = []\n",
        "        # 'feature_maps': [38, 19, 10, 5, 3, 1]\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            for i, j in product(range(f), repeat=2):  # fまでの数で2ペアの組み合わせを作る　f_P_2 個\n",
        "                # 特徴量の画像サイズ\n",
        "                # 300 / 'steps': [8, 16, 32, 64, 100, 300],\n",
        "                f_k = self.image_size / self.steps[k]\n",
        "\n",
        "                # DBoxの中心座標 x,y　ただし、0～1で規格化している\n",
        "                cx = (j + 0.5) / f_k\n",
        "                cy = (i + 0.5) / f_k\n",
        "\n",
        "                # アスペクト比1の小さいDBox [cx,cy, width, height]\n",
        "                # 'min_sizes': [30, 60, 111, 162, 213, 264]\n",
        "                s_k = self.min_sizes[k]/self.image_size\n",
        "                mean += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # アスペクト比1の大きいDBox [cx,cy, width, height]\n",
        "                # 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
        "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
        "\n",
        "                # その他のアスペクト比のdefBox [cx,cy, width, height]\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
        "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
        "\n",
        "        # DBoxをテンソルに変換 torch.Size([8732, 4])\n",
        "        output = torch.Tensor(mean).view(-1, 4)\n",
        "\n",
        "        # DBoxの大きさが1を超えている場合は1にする\n",
        "        output.clamp_(max=1, min=0)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "# オフセット情報を使い、DBoxをBBoxに変換する関数\n",
        "def decode(loc, dbox_list):\n",
        "    \"\"\"\n",
        "    オフセット情報を使い、DBoxをBBoxに変換する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    loc:  [8732,4]\n",
        "        SSDモデルで推論するオフセット情報。\n",
        "    dbox_list: [8732,4]\n",
        "        DBoxの情報\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    boxes : [xmin, ymin, xmax, ymax]\n",
        "        BBoxの情報\n",
        "    \"\"\"\n",
        "\n",
        "    # DBoxは[cx, cy, width, height]で格納されている\n",
        "    # locも[Δcx, Δcy, Δwidth, Δheight]で格納されている\n",
        "\n",
        "    # オフセット情報からBBoxを求める\n",
        "    boxes = torch.cat((\n",
        "        dbox_list[:, :2] + loc[:, :2] * 0.1 * dbox_list[:, 2:],\n",
        "        dbox_list[:, 2:] * torch.exp(loc[:, 2:] * 0.2)), dim=1)\n",
        "    # boxesのサイズはtorch.Size([8732, 4])となります\n",
        "\n",
        "    # BBoxの座標情報を[cx, cy, width, height]から[xmin, ymin, xmax, ymax] に\n",
        "    boxes[:, :2] -= boxes[:, 2:] / 2  # 座標(xmin,ymin)へ変換\n",
        "    boxes[:, 2:] += boxes[:, :2]  # 座標(xmax,ymax)へ変換\n",
        "\n",
        "    return boxes\n",
        "\n",
        "# Non-Maximum Suppressionを行う関数\n",
        "\n",
        "\n",
        "def nm_suppression(boxes, scores, overlap=0.45, top_k=200):\n",
        "    \"\"\"\n",
        "    Non-Maximum Suppressionを行う関数。\n",
        "    boxesのうち被り過ぎ（overlap以上）のBBoxを削除する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    boxes : [確信度閾値（0.01）を超えたBBox数,4]\n",
        "        BBox情報。\n",
        "    scores :[確信度閾値（0.01）を超えたBBox数]\n",
        "        confの情報\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    keep : リスト\n",
        "        confの降順にnmsを通過したindexが格納\n",
        "    count：int\n",
        "        nmsを通過したBBoxの数\n",
        "    \"\"\"\n",
        "\n",
        "    # returnのひな形を作成\n",
        "    count = 0\n",
        "    keep = scores.new(scores.size(0)).zero_().long()\n",
        "    # keep：torch.Size([確信度閾値を超えたBBox数])、要素は全部0\n",
        "\n",
        "    # 各BBoxの面積areaを計算\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    area = torch.mul(x2 - x1, y2 - y1)\n",
        "\n",
        "    # boxesをコピーする。後で、BBoxの被り度合いIOUの計算に使用する際のひな形として用意\n",
        "    tmp_x1 = boxes.new()\n",
        "    tmp_y1 = boxes.new()\n",
        "    tmp_x2 = boxes.new()\n",
        "    tmp_y2 = boxes.new()\n",
        "    tmp_w = boxes.new()\n",
        "    tmp_h = boxes.new()\n",
        "\n",
        "    # socreを昇順に並び変える\n",
        "    v, idx = scores.sort(0)\n",
        "\n",
        "    # 上位top_k個（200個）のBBoxのindexを取り出す（200個存在しない場合もある）\n",
        "    idx = idx[-top_k:]\n",
        "\n",
        "    # idxの要素数が0でない限りループする\n",
        "    while idx.numel() > 0:\n",
        "        i = idx[-1]  # 現在のconf最大のindexをiに\n",
        "\n",
        "        # keepの現在の最後にconf最大のindexを格納する\n",
        "        # このindexのBBoxと被りが大きいBBoxをこれから消去する\n",
        "        keep[count] = i\n",
        "        count += 1\n",
        "\n",
        "        # 最後のBBoxになった場合は、ループを抜ける\n",
        "        if idx.size(0) == 1:\n",
        "            break\n",
        "\n",
        "        # 現在のconf最大のindexをkeepに格納したので、idxをひとつ減らす\n",
        "        idx = idx[:-1]\n",
        "\n",
        "        # -------------------\n",
        "        # これからkeepに格納したBBoxと被りの大きいBBoxを抽出して除去する\n",
        "        # -------------------\n",
        "        # ひとつ減らしたidxまでのBBoxを、outに指定した変数として作成する\n",
        "        torch.index_select(x1, 0, idx, out=tmp_x1)\n",
        "        torch.index_select(y1, 0, idx, out=tmp_y1)\n",
        "        torch.index_select(x2, 0, idx, out=tmp_x2)\n",
        "        torch.index_select(y2, 0, idx, out=tmp_y2)\n",
        "\n",
        "        # すべてのBBoxに対して、現在のBBox=indexがiと被っている値までに設定(clamp)\n",
        "        tmp_x1 = torch.clamp(tmp_x1, min=x1[i])\n",
        "        tmp_y1 = torch.clamp(tmp_y1, min=y1[i])\n",
        "        tmp_x2 = torch.clamp(tmp_x2, max=x2[i])\n",
        "        tmp_y2 = torch.clamp(tmp_y2, max=y2[i])\n",
        "\n",
        "        # wとhのテンソルサイズをindexを1つ減らしたものにする\n",
        "        tmp_w.resize_as_(tmp_x2)\n",
        "        tmp_h.resize_as_(tmp_y2)\n",
        "\n",
        "        # clampした状態でのBBoxの幅と高さを求める\n",
        "        tmp_w = tmp_x2 - tmp_x1\n",
        "        tmp_h = tmp_y2 - tmp_y1\n",
        "\n",
        "        # 幅や高さが負になっているものは0にする\n",
        "        tmp_w = torch.clamp(tmp_w, min=0.0)\n",
        "        tmp_h = torch.clamp(tmp_h, min=0.0)\n",
        "\n",
        "        # clampされた状態での面積を求める\n",
        "        inter = tmp_w*tmp_h\n",
        "\n",
        "        # IoU = intersect部分 / (area(a) + area(b) - intersect部分)の計算\n",
        "        rem_areas = torch.index_select(area, 0, idx)  # 各BBoxの元の面積\n",
        "        union = (rem_areas - inter) + area[i]  # 2つのエリアのANDの面積\n",
        "        IoU = inter/union\n",
        "\n",
        "        # IoUがoverlapより小さいidxのみを残す\n",
        "        idx = idx[IoU.le(overlap)]  # leはLess than or Equal toの処理をする演算です\n",
        "        # IoUがoverlapより大きいidxは、最初に選んでkeepに格納したidxと同じ物体に対してBBoxを囲んでいるため消去\n",
        "\n",
        "    # whileのループが抜けたら終了\n",
        "\n",
        "    return keep, count\n",
        "\n",
        "\n",
        "# SSDの推論時にconfとlocの出力から、被りを除去したBBoxを出力する\n",
        "\n",
        "\n",
        "# 2021/07/31 修正 KAMEDA Yoshinari\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/71\n",
        "class Detect():\n",
        "\n",
        "    def __init__(self, conf_thresh=0.01, top_k=200, nms_thresh=0.45):\n",
        "        self.softmax = nn.Softmax(dim=-1)  # confをソフトマックス関数で正規化するために用意\n",
        "        self.conf_thresh = conf_thresh  # confがconf_thresh=0.01より高いDBoxのみを扱う\n",
        "        self.top_k = top_k  # nm_supressionでconfの高いtop_k個を計算に使用する, top_k = 200\n",
        "        self.nms_thresh = nms_thresh  # nm_supressionでIOUがnms_thresh=0.45より大きいと、同一物体へのBBoxとみなす\n",
        "\n",
        "    def __call__(self, loc_data, conf_data, dbox_list):\n",
        "        \"\"\"\n",
        "        順伝搬の計算を実行する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        loc_data:  [batch_num,8732,4]\n",
        "            オフセット情報。\n",
        "        conf_data: [batch_num, 8732,num_classes]\n",
        "            検出の確信度。\n",
        "        dbox_list: [8732,4]\n",
        "            DBoxの情報\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Size([batch_num, 21, 200, 5])\n",
        "            （batch_num、クラス、confのtop200、BBoxの情報）\n",
        "        \"\"\"\n",
        "\n",
        "        # 各サイズを取得\n",
        "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
        "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
        "        num_classes = conf_data.size(2)  # クラス数 = 21\n",
        "\n",
        "        # confはソフトマックスを適用して正規化する\n",
        "        conf_data = self.softmax(conf_data)\n",
        "\n",
        "        # 出力の型を作成する。テンソルサイズは[minibatch数, 21, 200, 5]\n",
        "        output = torch.zeros(num_batch, num_classes, self.top_k, 5)\n",
        "\n",
        "        # cof_dataを[batch_num,8732,num_classes]から[batch_num, num_classes,8732]に順番変更\n",
        "        conf_preds = conf_data.transpose(2, 1)\n",
        "\n",
        "        # ミニバッチごとのループ\n",
        "        for i in range(num_batch):\n",
        "\n",
        "            # 1. locとDBoxから修正したBBox [xmin, ymin, xmax, ymax] を求める\n",
        "            decoded_boxes = decode(loc_data[i], dbox_list)\n",
        "\n",
        "            # confのコピーを作成\n",
        "            conf_scores = conf_preds[i].clone()\n",
        "\n",
        "            # 画像クラスごとのループ（背景クラスのindexである0は計算せず、index=1から）\n",
        "            for cl in range(1, num_classes):\n",
        "\n",
        "                # 2.confの閾値を超えたBBoxを取り出す\n",
        "                # confの閾値を超えているかのマスクを作成し、\n",
        "                # 閾値を超えたconfのインデックスをc_maskとして取得\n",
        "                c_mask = conf_scores[cl].gt(self.conf_thresh)\n",
        "                # gtはGreater thanのこと。gtにより閾値を超えたものが1に、以下が0になる\n",
        "                # conf_scores:torch.Size([21, 8732])\n",
        "                # c_mask:torch.Size([8732])\n",
        "\n",
        "                # scoresはtorch.Size([閾値を超えたBBox数])\n",
        "                scores = conf_scores[cl][c_mask]\n",
        "\n",
        "                # 閾値を超えたconfがない場合、つまりscores=[]のときは、何もしない\n",
        "                if scores.nelement() == 0:  # nelementで要素数の合計を求める\n",
        "                    continue\n",
        "\n",
        "                # c_maskを、decoded_boxesに適用できるようにサイズを変更します\n",
        "                l_mask = c_mask.unsqueeze(1).expand_as(decoded_boxes)\n",
        "                # l_mask:torch.Size([8732, 4])\n",
        "\n",
        "                # l_maskをdecoded_boxesに適応します\n",
        "                boxes = decoded_boxes[l_mask].view(-1, 4)\n",
        "                # decoded_boxes[l_mask]で1次元になってしまうので、\n",
        "                # viewで（閾値を超えたBBox数, 4）サイズに変形しなおす\n",
        "\n",
        "                # 3. Non-Maximum Suppressionを実施し、被っているBBoxを取り除く\n",
        "                ids, count = nm_suppression(\n",
        "                    boxes, scores, self.nms_thresh, self.top_k)\n",
        "                # ids：confの降順にNon-Maximum Suppressionを通過したindexが格納\n",
        "                # count：Non-Maximum Suppressionを通過したBBoxの数\n",
        "\n",
        "                # outputにNon-Maximum Suppressionを抜けた結果を格納\n",
        "                output[i, cl, :count] = torch.cat((scores[ids[:count]].unsqueeze(1),\n",
        "                                                   boxes[ids[:count]]), 1)\n",
        "\n",
        "        return output  # torch.Size([1, 21, 200, 5])\n",
        "\n",
        "# SSDクラスを作成する\n",
        "\n",
        "\n",
        "class SSD(nn.Module):\n",
        "\n",
        "    def __init__(self, phase, cfg):\n",
        "        super(SSD, self).__init__()\n",
        "\n",
        "        self.phase = phase  # train or inferenceを指定\n",
        "        self.num_classes = cfg[\"num_classes\"]  # クラス数=21\n",
        "\n",
        "        # SSDのネットワークを作る\n",
        "        self.vgg = make_vgg()\n",
        "        self.extras = make_extras()\n",
        "        self.L2Norm = L2Norm()\n",
        "        self.loc, self.conf = make_loc_conf(\n",
        "            cfg[\"num_classes\"], cfg[\"bbox_aspect_num\"])\n",
        "\n",
        "        # DBox作成\n",
        "        dbox = DBox(cfg)\n",
        "        self.dbox_list = dbox.make_dbox_list()\n",
        "\n",
        "        # 推論時はクラス「Detect」を用意します\n",
        "        if phase == 'inference':\n",
        "            self.detect = Detect()\n",
        "\n",
        "    def forward(self, x):\n",
        "        sources = list()  # locとconfへの入力source1～6を格納\n",
        "        loc = list()  # locの出力を格納\n",
        "        conf = list()  # confの出力を格納\n",
        "\n",
        "        # vggのconv4_3まで計算する\n",
        "        for k in range(23):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        # conv4_3の出力をL2Normに入力し、source1を作成、sourcesに追加\n",
        "        source1 = self.L2Norm(x)\n",
        "        sources.append(source1)\n",
        "\n",
        "        # vggを最後まで計算し、source2を作成、sourcesに追加\n",
        "        for k in range(23, len(self.vgg)):\n",
        "            x = self.vgg[k](x)\n",
        "\n",
        "        sources.append(x)\n",
        "\n",
        "        # extrasのconvとReLUを計算\n",
        "        # source3～6を、sourcesに追加\n",
        "        for k, v in enumerate(self.extras):\n",
        "            x = F.relu(v(x), inplace=True)\n",
        "            if k % 2 == 1:  # conv→ReLU→cov→ReLUをしたらsourceに入れる\n",
        "                sources.append(x)\n",
        "\n",
        "        # source1～6に、それぞれ対応する畳み込みを1回ずつ適用する\n",
        "        # zipでforループの複数のリストの要素を取得\n",
        "        # source1～6まであるので、6回ループが回る\n",
        "        for (x, l, c) in zip(sources, self.loc, self.conf):\n",
        "            # Permuteは要素の順番を入れ替え\n",
        "            loc.append(l(x).permute(0, 2, 3, 1).contiguous())\n",
        "            conf.append(c(x).permute(0, 2, 3, 1).contiguous())\n",
        "            # l(x)とc(x)で畳み込みを実行\n",
        "            # l(x)とc(x)の出力サイズは[batch_num, 4*アスペクト比の種類数, featuremapの高さ, featuremap幅]\n",
        "            # sourceによって、アスペクト比の種類数が異なり、面倒なので順番入れ替えて整える\n",
        "            # permuteで要素の順番を入れ替え、\n",
        "            # [minibatch数, featuremap数, featuremap数,4*アスペクト比の種類数]へ\n",
        "            # （注釈）\n",
        "            # torch.contiguous()はメモリ上で要素を連続的に配置し直す命令です。\n",
        "            # あとでview関数を使用します。\n",
        "            # このviewを行うためには、対象の変数がメモリ上で連続配置されている必要があります。\n",
        "\n",
        "        # さらにlocとconfの形を変形\n",
        "        # locのサイズは、torch.Size([batch_num, 34928])\n",
        "        # confのサイズはtorch.Size([batch_num, 183372])になる\n",
        "        loc = torch.cat([o.view(o.size(0), -1) for o in loc], 1)\n",
        "        conf = torch.cat([o.view(o.size(0), -1) for o in conf], 1)\n",
        "\n",
        "        # さらにlocとconfの形を整える\n",
        "        # locのサイズは、torch.Size([batch_num, 8732, 4])\n",
        "        # confのサイズは、torch.Size([batch_num, 8732, 21])\n",
        "        loc = loc.view(loc.size(0), -1, 4)\n",
        "        conf = conf.view(conf.size(0), -1, self.num_classes)\n",
        "\n",
        "        # 最後に出力する\n",
        "        output = (loc, conf, self.dbox_list)\n",
        "\n",
        "        # 2021/07/31 修正 KAMEDA Yoshinari\n",
        "        # https://github.com/YutaroOgawa/pytorch_advanced/issues/71\n",
        "        if self.phase == \"inference\":  # 推論時\n",
        "            # クラス「Detect」のforwardを実行\n",
        "            # 返り値のサイズは torch.Size([batch_num, 21, 200, 5])\n",
        "            with torch.no_grad():\n",
        "                return self.detect(output[0], output[1], output[2])\n",
        "\n",
        "        else:  # 学習時\n",
        "            return output\n",
        "            # 返り値は(loc, conf, dbox_list)のタプル\n",
        "\n",
        "\n",
        "class MultiBoxLoss(nn.Module):\n",
        "    \"\"\"SSDの損失関数のクラスです。\"\"\"\n",
        "\n",
        "    def __init__(self, jaccard_thresh=0.5, neg_pos=3, device='cpu'):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.jaccard_thresh = jaccard_thresh  # 0.5 関数matchのjaccard係数の閾値\n",
        "        self.negpos_ratio = neg_pos  # 3:1 Hard Negative Miningの負と正の比率\n",
        "        self.device = device  # CPUとGPUのいずれで計算するのか\n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "        \"\"\"\n",
        "        損失関数の計算。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        predictions : SSD netの訓練時の出力(tuple)\n",
        "            (loc=torch.Size([num_batch, 8732, 4]), conf=torch.Size([num_batch, 8732, 21]), dbox_list=torch.Size [8732,4])。\n",
        "\n",
        "        targets : [num_batch, num_objs, 5]\n",
        "            5は正解のアノテーション情報[xmin, ymin, xmax, ymax, label_ind]を示す\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        loss_l : テンソル\n",
        "            locの損失の値\n",
        "        loss_c : テンソル\n",
        "            confの損失の値\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        # SSDモデルの出力がタプルになっているので、個々にばらす\n",
        "        loc_data, conf_data, dbox_list = predictions\n",
        "\n",
        "        # 要素数を把握\n",
        "        num_batch = loc_data.size(0)  # ミニバッチのサイズ\n",
        "        num_dbox = loc_data.size(1)  # DBoxの数 = 8732\n",
        "        num_classes = conf_data.size(2)  # クラス数 = 21\n",
        "\n",
        "        # 損失の計算に使用するものを格納する変数を作成\n",
        "        # conf_t_label：各DBoxに一番近い正解のBBoxのラベルを格納させる\n",
        "        # loc_t:各DBoxに一番近い正解のBBoxの位置情報を格納させる\n",
        "        conf_t_label = torch.LongTensor(num_batch, num_dbox).to(self.device)\n",
        "        loc_t = torch.Tensor(num_batch, num_dbox, 4).to(self.device)\n",
        "\n",
        "        # loc_tとconf_t_labelに、\n",
        "        # DBoxと正解アノテーションtargetsをmatchさせた結果を上書きする\n",
        "        for idx in range(num_batch):  # ミニバッチでループ\n",
        "\n",
        "            # 現在のミニバッチの正解アノテーションのBBoxとラベルを取得\n",
        "            truths = targets[idx][:, :-1].to(self.device)  # BBox\n",
        "            # ラベル [物体1のラベル, 物体2のラベル, …]\n",
        "            labels = targets[idx][:, -1].to(self.device)\n",
        "\n",
        "            # デフォルトボックスを新たな変数で用意\n",
        "            dbox = dbox_list.to(self.device)\n",
        "\n",
        "            # 関数matchを実行し、loc_tとconf_t_labelの内容を更新する\n",
        "            # （詳細）\n",
        "            # loc_t:各DBoxに一番近い正解のBBoxの位置情報が上書きされる\n",
        "            # conf_t_label：各DBoxに一番近いBBoxのラベルが上書きされる\n",
        "            # ただし、一番近いBBoxとのjaccard overlapが0.5より小さい場合は\n",
        "            # 正解BBoxのラベルconf_t_labelは背景クラスの0とする\n",
        "            variance = [0.1, 0.2]\n",
        "            # このvarianceはDBoxからBBoxに補正計算する際に使用する式の係数です\n",
        "            match(self.jaccard_thresh, truths, dbox,\n",
        "                  variance, labels, loc_t, conf_t_label, idx)\n",
        "\n",
        "        # ----------\n",
        "        # 位置の損失：loss_lを計算\n",
        "        # Smooth L1関数で損失を計算する。ただし、物体を発見したDBoxのオフセットのみを計算する\n",
        "        # ----------\n",
        "        # 物体を検出したBBoxを取り出すマスクを作成\n",
        "        pos_mask = conf_t_label > 0  # torch.Size([num_batch, 8732])\n",
        "\n",
        "        # pos_maskをloc_dataのサイズに変形\n",
        "        pos_idx = pos_mask.unsqueeze(pos_mask.dim()).expand_as(loc_data)\n",
        "\n",
        "        # Positive DBoxのloc_dataと、教師データloc_tを取得\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "\n",
        "        # 物体を発見したPositive DBoxのオフセット情報loc_tの損失（誤差）を計算\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, reduction='sum')\n",
        "\n",
        "        # ----------\n",
        "        # クラス予測の損失：loss_cを計算\n",
        "        # 交差エントロピー誤差関数で損失を計算する。ただし、背景クラスが正解であるDBoxが圧倒的に多いので、\n",
        "        # Hard Negative Miningを実施し、物体発見DBoxと背景クラスDBoxの比が1:3になるようにする。\n",
        "        # そこで背景クラスDBoxと予想したもののうち、損失が小さいものは、クラス予測の損失から除く\n",
        "        # ----------\n",
        "        batch_conf = conf_data.view(-1, num_classes)\n",
        "\n",
        "        # クラス予測の損失を関数を計算(reduction='none'にして、和をとらず、次元をつぶさない)\n",
        "        loss_c = F.cross_entropy(\n",
        "            batch_conf, conf_t_label.view(-1), reduction='none')\n",
        "\n",
        "        # -----------------\n",
        "        # これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
        "        # -----------------\n",
        "\n",
        "        # 物体発見したPositive DBoxの損失を0にする\n",
        "        # （注意）物体はlabelが1以上になっている。ラベル0は背景。\n",
        "        num_pos = pos_mask.long().sum(1, keepdim=True)  # ミニバッチごとの物体クラス予測の数\n",
        "        loss_c = loss_c.view(num_batch, -1)  # torch.Size([num_batch, 8732])\n",
        "        loss_c[pos_mask] = 0  # 物体を発見したDBoxは損失0とする\n",
        "\n",
        "        # Hard Negative Miningを実施する\n",
        "        # 各DBoxの損失の大きさloss_cの順位であるidx_rankを求める\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "\n",
        "        # （注釈）\n",
        "        # 実装コードがかなり特殊で直感的ではないです。\n",
        "        # 上記2行は、要は各DBoxに対して、損失の大きさが何番目なのかの情報を\n",
        "        # 変数idx_rankとして高速に取得したいというコードです。\n",
        "        #\n",
        "        # DBOXの損失値の大きい方から降順に並べ、DBoxの降順のindexをloss_idxに格納。\n",
        "        # 損失の大きさloss_cの順位であるidx_rankを求める。\n",
        "        # ここで、\n",
        "        # 降順になった配列indexであるloss_idxを、0から8732まで昇順に並べ直すためには、\n",
        "        # 何番目のloss_idxのインデックスをとってきたら良いのかを示すのが、idx_rankである。\n",
        "        # 例えば、\n",
        "        # idx_rankの要素0番目 = idx_rank[0]を求めるには、loss_idxの値が0の要素、\n",
        "        # つまりloss_idx[?}=0 の、?は何番かを求めることになる。ここで、? = idx_rank[0]である。\n",
        "        # いま、loss_idx[?]=0の0は、元のloss_cの要素の0番目という意味である。\n",
        "        # つまり?は、元のloss_cの要素0番目は、降順に並び替えられたloss_idxの何番目ですか\n",
        "        # を求めていることになり、 結果、\n",
        "        # ? = idx_rank[0] はloss_cの要素0番目が、降順の何番目かを示すことになる。\n",
        "\n",
        "        # 背景のDBoxの数num_negを決める。HardNegative Miningにより、\n",
        "        # 物体発見のDBoxの数num_posの3倍（self.negpos_ratio倍）とする。\n",
        "        # ただし、万が一、DBoxの数を超える場合は、DBoxの数を上限とする\n",
        "        num_neg = torch.clamp(num_pos*self.negpos_ratio, max=num_dbox)\n",
        "\n",
        "        # idx_rankは各DBoxの損失の大きさが上から何番目なのかが入っている\n",
        "        # 背景のDBoxの数num_negよりも、順位が低い（すなわち損失が大きい）DBoxを取るマスク作成\n",
        "        # torch.Size([num_batch, 8732])\n",
        "        neg_mask = idx_rank < (num_neg).expand_as(idx_rank)\n",
        "\n",
        "        # -----------------\n",
        "        # （終了）これからNegative DBoxのうち、Hard Negative Miningで抽出するものを求めるマスクを作成します\n",
        "        # -----------------\n",
        "\n",
        "        # マスクの形を整形し、conf_dataに合わせる\n",
        "        # pos_idx_maskはPositive DBoxのconfを取り出すマスクです\n",
        "        # neg_idx_maskはHard Negative Miningで抽出したNegative DBoxのconfを取り出すマスクです\n",
        "        # pos_mask：torch.Size([num_batch, 8732])→pos_idx_mask：torch.Size([num_batch, 8732, 21])\n",
        "        pos_idx_mask = pos_mask.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx_mask = neg_mask.unsqueeze(2).expand_as(conf_data)\n",
        "\n",
        "        # conf_dataからposとnegだけを取り出してconf_hnmにする。形はtorch.Size([num_pos+num_neg, 21])\n",
        "        conf_hnm = conf_data[(pos_idx_mask+neg_idx_mask).gt(0)\n",
        "                             ].view(-1, num_classes)\n",
        "        # （注釈）gtは greater than (>)の略称。これでmaskが1のindexを取り出す。\n",
        "        # pos_idx_mask+neg_idx_maskは足し算だが、indexへのmaskをまとめているだけである。\n",
        "        # つまり、posであろうがnegであろうが、マスクが1のものを足し算で一つのリストにし、それをgtで取得\n",
        "\n",
        "        # 同様に教師データであるconf_t_labelからposとnegだけを取り出してconf_t_label_hnmに\n",
        "        # 形はtorch.Size([pos+neg])になる\n",
        "        conf_t_label_hnm = conf_t_label[(pos_mask+neg_mask).gt(0)]\n",
        "\n",
        "        # confidenceの損失関数を計算（要素の合計=sumを求める）\n",
        "        loss_c = F.cross_entropy(conf_hnm, conf_t_label_hnm, reduction='sum')\n",
        "\n",
        "        # 物体を発見したBBoxの数N（全ミニバッチの合計）で損失を割り算\n",
        "        N = num_pos.sum()\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "\n",
        "        return loss_l, loss_c"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_elu01WLuo4f"
      },
      "source": [
        "---\n",
        "# 2.8節"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ykwu0kmHjf6a"
      },
      "source": [
        "import cv2  # OpenCVライブラリ\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TKTcCEIjf6a"
      },
      "source": [
        "# 推論用の関数とクラスを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA__FqNljf6b"
      },
      "source": [
        "def ssd_predict(img_index, img_list, dataset, net=None, dataconfidence_level=0.5):\n",
        "    \"\"\"\n",
        "    SSDで予測させる関数。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_index:  int\n",
        "        データセット内の予測対象画像のインデックス。\n",
        "    img_list: list\n",
        "        画像のファイルパスのリスト\n",
        "    dataset: PyTorchのDataset\n",
        "        画像のDataset\n",
        "    net: PyTorchのNetwork\n",
        "        学習させたSSDネットワーク\n",
        "    dataconfidence_level: float\n",
        "        予測で発見とする確信度の閾値\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\n",
        "    \"\"\"\n",
        "\n",
        "    # rgbの画像データを取得\n",
        "    image_file_path = img_list[img_index]\n",
        "    img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "    height, width, channels = img.shape  # 画像のサイズを取得\n",
        "    rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # 正解のBBoxを取得\n",
        "    im, gt = dataset.__getitem__(img_index)\n",
        "    true_bbox = gt[:, 0:4] * [width, height, width, height]\n",
        "    true_label_index = gt[:, 4].astype(int)\n",
        "\n",
        "    # SSDで予測\n",
        "    net.eval()  # ネットワークを推論モードへ\n",
        "    x = im.unsqueeze(0)  # ミニバッチ化：torch.Size([1, 3, 300, 300])\n",
        "    detections = net(x)\n",
        "    # detectionsの形は、torch.Size([1, 21, 200, 5])  ※200はtop_kの値\n",
        "\n",
        "    # confidence_levelが基準以上を取り出す\n",
        "    predict_bbox = []\n",
        "    pre_dict_label_index = []\n",
        "    scores = []\n",
        "    detections = detections.cpu().detach().numpy()\n",
        "\n",
        "    # 条件以上の値を抽出\n",
        "    find_index = np.where(detections[:, 0:, :, 0] >= dataconfidence_level)\n",
        "    detections = detections[find_index]\n",
        "    for i in range(len(find_index[1])):  # 抽出した物体数分ループを回す\n",
        "        if (find_index[1][i]) > 0:  # 背景クラスでないもの\n",
        "            sc = detections[i][0]  # 確信度\n",
        "            bbox = detections[i][1:] * [width, height, width, height]\n",
        "            lable_ind = find_index[1][i]-1  # find_indexはミニバッチ数、クラス、topのtuple\n",
        "            # （注釈）\n",
        "            # 背景クラスが0なので1を引く\n",
        "\n",
        "            # 返り値のリストに追加\n",
        "            predict_bbox.append(bbox)\n",
        "            pre_dict_label_index.append(lable_ind)\n",
        "            scores.append(sc)\n",
        "\n",
        "    return rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxuhxRF9jf6b"
      },
      "source": [
        "def vis_bbox(rgb_img, bbox, label_index, scores, label_names):\n",
        "    \"\"\"\n",
        "    物体検出の予測結果を画像で表示させる関数。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rgb_img:rgbの画像\n",
        "        対象の画像データ\n",
        "    bbox: list\n",
        "        物体のBBoxのリスト\n",
        "    label_index: list\n",
        "        物体のラベルへのインデックス\n",
        "    scores: list\n",
        "        物体の確信度。\n",
        "    label_names: list\n",
        "        ラベル名の配列\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    なし。rgb_imgに物体検出結果が加わった画像が表示される。\n",
        "    \"\"\"\n",
        "\n",
        "    # 枠の色の設定\n",
        "    num_classes = len(label_names)  # クラス数（背景のぞく）\n",
        "    colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
        "\n",
        "    # 画像の表示\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(rgb_img)\n",
        "    currentAxis = plt.gca()\n",
        "\n",
        "    # BBox分のループ\n",
        "    for i, bb in enumerate(bbox):\n",
        "\n",
        "        # ラベル名\n",
        "        label_name = label_names[label_index[i]]\n",
        "        color = colors[label_index[i]]  # クラスごとに別の色の枠を与える\n",
        "\n",
        "        # 枠につけるラベル　例：person;0.72　\n",
        "        if scores is not None:\n",
        "            sc = scores[i]\n",
        "            display_txt = '%s: %.2f' % (label_name, sc)\n",
        "        else:\n",
        "            display_txt = '%s: ans' % (label_name)\n",
        "\n",
        "        # 枠の座標\n",
        "        xy = (bb[0], bb[1])\n",
        "        width = bb[2] - bb[0]\n",
        "        height = bb[3] - bb[1]\n",
        "\n",
        "        # 長方形を描画する\n",
        "        currentAxis.add_patch(plt.Rectangle(\n",
        "            xy, width, height, fill=False, edgecolor=color, linewidth=2))\n",
        "\n",
        "        # 長方形の枠の左上にラベルを描画する\n",
        "        currentAxis.text(xy[0], xy[1], display_txt, bbox={\n",
        "                         'facecolor': color, 'alpha': 0.5})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTq2pAvljf6c"
      },
      "source": [
        "class SSDPredictShow():\n",
        "    \"\"\"SSDでの予測と画像の表示をまとめて行うクラス\"\"\"\n",
        "\n",
        "    def __init__(self, img_list, dataset,  eval_categories, net=None, dataconfidence_level=0.6):\n",
        "        self.img_list = img_list\n",
        "        self.dataset = dataset\n",
        "        self.net = net\n",
        "        self.dataconfidence_level = dataconfidence_level\n",
        "        self.eval_categories = eval_categories\n",
        "\n",
        "    def show(self, img_index, predict_or_ans):\n",
        "        \"\"\"\n",
        "        物体検出の予測と表示をする関数。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        img_index:  int\n",
        "            データセット内の予測対象画像のインデックス。\n",
        "        predict_or_ans: text\n",
        "            'precit'もしくは'ans'でBBoxの予測と正解のどちらを表示させるか指定する\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        なし。rgb_imgに物体検出結果が加わった画像が表示される。\n",
        "        \"\"\"\n",
        "        rgb_img, true_bbox, true_label_index, predict_bbox, pre_dict_label_index, scores = ssd_predict(img_index, self.img_list,\n",
        "                                                                 self.dataset,\n",
        "                                                                 self.net,\n",
        "                                                                 self.dataconfidence_level)\n",
        "\n",
        "        if predict_or_ans == \"predict\":\n",
        "            vis_bbox(rgb_img, bbox=predict_bbox, label_index=pre_dict_label_index,\n",
        "                     scores=scores, label_names=self.eval_categories)\n",
        "\n",
        "        elif predict_or_ans == \"ans\":\n",
        "            vis_bbox(rgb_img, bbox=true_bbox, label_index=true_label_index,\n",
        "                     scores=None, label_names=self.eval_categories)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNd9fZhPjf6d"
      },
      "source": [
        "# 推論を実行する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX7azjKojf6d"
      },
      "source": [
        "from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\n",
        "\n",
        "\n",
        "# ファイルパスのリストを取得\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath)\n",
        "\n",
        "# Datasetを作成\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN84uyNjjf6d"
      },
      "source": [
        "from utils.ssd_model import SSD\n",
        "\n",
        "# SSD300の設定\n",
        "ssd_cfg = {\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
        "    'input_size': 300,  # 画像の入力サイズ\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "}\n",
        "\n",
        "# SSDネットワークモデル\n",
        "net = SSD(phase=\"inference\", cfg=ssd_cfg)\n",
        "net.eval()\n",
        "\n",
        "# SSDの学習済みの重みを設定\n",
        "net_weights = torch.load('./weights/ssd300_50.pth',\n",
        "                         map_location={'cuda:0': 'cpu'})\n",
        "\n",
        "#net_weights = torch.load('./weights/ssd300_mAP_77.43_v2.pth',\n",
        "#                         map_location={'cuda:0': 'cpu'})\n",
        "\n",
        "net.load_state_dict(net_weights)\n",
        "\n",
        "# GPUが使えるかを確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie72sxqjf6e"
      },
      "source": [
        "# 結果の描画 (It may sometimes take 2-3 minutes)\n",
        "\n",
        "# Just to surpress UserWarning\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "\n",
        "ssd = SSDPredictShow(img_list=train_img_list, dataset=train_dataset, eval_categories=voc_classes,\n",
        "                     net=net, dataconfidence_level=0.6)\n",
        "img_index = 0\n",
        "ssd.show(img_index, \"predict\")\n",
        "ssd.show(img_index, \"ans\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5N4bzeHmjf6e"
      },
      "source": [
        "# 結果の描画\n",
        "ssd = SSDPredictShow(img_list=val_img_list, dataset=val_dataset, eval_categories=voc_classes,\n",
        "                     net=net, dataconfidence_level=0.6)\n",
        "img_index = 0\n",
        "ssd.show(img_index, \"predict\")\n",
        "ssd.show(img_index, \"ans\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsCxUZizjf6e"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QTzmfkfNiqz"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. revised version of utils/ssd_model.py  \n",
        "2021/08/02. "
      ]
    }
  ]
}