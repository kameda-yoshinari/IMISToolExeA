{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC2-2-3_Dataset_DataLoader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/2_objectdetection/GC2_2_3_Dataset_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFaNmehxUZ79"
      },
      "source": [
        "# 2.2 Datasetの実装、 2.3 DataLoaderの実装\n",
        "\n",
        "本ファイルでは、SSDなど物体検出アルゴリズム用のDatasetとDataLoaderを作成します。\n",
        "\n",
        "VOC2012データセットを対象とします。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFH04uHeUZ7_"
      },
      "source": [
        "# 2.2 学習目標\n",
        "\n",
        "1.\t物体検出で使用するDatasetクラスを作成できるようになる\n",
        "2.\tSSDの学習時のデータオーギュメンテーションで、何をしているのかを理解する\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXt4WWhhUZ8A"
      },
      "source": [
        "# 2.3 学習目標\n",
        "\n",
        "1.\t物体検出で使用するDataLoaderクラスを作成できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsRS2mo_Y8AO"
      },
      "source": [
        "---\n",
        "\n",
        "# [U-Tsukuba] Extra remarks\n",
        "\n",
        "Before we start, we need to set up VOC2012 in advance.  \n",
        "Follow the instruction of GC2-2-0_make_folders_and_data_downloads.ipynb and set up VOC2012 on your google drive.\n",
        "\n",
        "Note that it will use 4GB in total (2GB for the tar file, 2GB for the extracted files).\n",
        "\n",
        "\n",
        "Reference of VOC2012:  \n",
        "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PP4YVTTLkiG"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i00BLJjmPM4X"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG8Cu0UxXcQ0"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFizVjJC2ru5"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0_8gZRh8-op"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLva_hyUlxlf"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dLafiH3l0v2"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gw4aia4Dl3F2"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6LE69pul4zW"
      },
      "source": [
        "%cd \"2_objectdetection\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YIxC8nkmLgB"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rux859mS5dGO"
      },
      "source": [
        "---\n",
        "# VOC2012準備\n",
        "\n",
        "VOCdevkit/ from VOCtrainval_11-May-2012.tar is placed at /root .  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDDBrUtU54KX"
      },
      "source": [
        "# It takes one minutes or so.\n",
        "\n",
        "%cd /root\n",
        "!tar xf \"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/2_objectdetection/data/VOCtrainval_11-May-2012.tar\"\n",
        "!ls /root/VOCdevkit\n",
        "%cd -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ei_3-v7UZ8A"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "\n",
        "\n",
        "OpenCVのインストールはGoogle Colaboratoryでは済んでいる。\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m8MmYlGeYsBx"
      },
      "source": [
        "!pip list | grep opencv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ps5Q07IUZ8A"
      },
      "source": [
        "# パッケージのimport\n",
        "import os.path as osp\n",
        "import random\n",
        "# XMLをファイルやテキストから読み込んだり、加工したり、保存したりするためのライブラリ\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKs6IaI_UZ8B"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMfsym0RUZ8B"
      },
      "source": [
        "# 画像データ、アノテーションデータへのファイルパスのリストを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjSJKaEYUZ8C"
      },
      "source": [
        "# 学習、検証の画像データとアノテーションデータへのファイルパスリストを作成する\n",
        "\n",
        "\n",
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    データへのパスを格納したリストを作成する。\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    rootpath : str\n",
        "        データフォルダへのパス\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    ret : train_img_list, train_anno_list, val_img_list, val_anno_list\n",
        "        データへのパスを格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    # 画像ファイルとアノテーションファイルへのパスのテンプレートを作成\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "\n",
        "    # 訓練と検証、それぞれのファイルのID（ファイル名）を取得する\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "\n",
        "    # 訓練データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    train_img_list = list()\n",
        "    train_anno_list = list()\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        train_img_list.append(img_path)  # リストに追加\n",
        "        train_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    # 検証データの画像ファイルとアノテーションファイルへのパスリストを作成\n",
        "    val_img_list = list()\n",
        "    val_anno_list = list()\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip()  # 空白スペースと改行を除去\n",
        "        img_path = (imgpath_template % file_id)  # 画像のパス\n",
        "        anno_path = (annopath_template % file_id)  # アノテーションのパス\n",
        "        val_img_list.append(img_path)  # リストに追加\n",
        "        val_anno_list.append(anno_path)  # リストに追加\n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg7J3WLjUZ8C"
      },
      "source": [
        "# ファイルパスのリストを作成\n",
        "rootpath = \"/root/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath)\n",
        "\n",
        "# 動作確認\n",
        "print(train_img_list[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fFp2_U2UZ8D"
      },
      "source": [
        "# xml形式のアノテーションデータをリストに変換する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZUGLdpyUZ8D"
      },
      "source": [
        "# 「XML形式のアノテーション」を、リスト形式に変換するクラス\n",
        "\n",
        "\n",
        "class Anno_xml2list(object):\n",
        "    \"\"\"\n",
        "    1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    classes : リスト\n",
        "        VOCのクラス名を格納したリスト\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, classes):\n",
        "\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        \"\"\"\n",
        "        1枚の画像に対する「XML形式のアノテーションデータ」を、画像サイズで規格化してからリスト形式に変換する。\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        xml_path : str\n",
        "            xmlファイルへのパス。\n",
        "        width : int\n",
        "            対象画像の幅。\n",
        "        height : int\n",
        "            対象画像の高さ。\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        ret : [[xmin, ymin, xmax, ymax, label_ind], ... ]\n",
        "            物体のアノテーションデータを格納したリスト。画像内に存在する物体数分のだけ要素を持つ。\n",
        "        \"\"\"\n",
        "\n",
        "        # 画像内の全ての物体のアノテーションをこのリストに格納します\n",
        "        ret = []\n",
        "\n",
        "        # xmlファイルを読み込む\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "\n",
        "        # 画像内にある物体（object）の数だけループする\n",
        "        for obj in xml.iter('object'):\n",
        "\n",
        "            # アノテーションで検知がdifficultに設定されているものは除外\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "\n",
        "            # 1つの物体に対するアノテーションを格納するリスト\n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()  # 物体名\n",
        "            bbox = obj.find('bndbox')  # バウンディングボックスの情報\n",
        "\n",
        "            # アノテーションの xmin, ymin, xmax, ymaxを取得し、0～1に規格化\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                # VOCは原点が(1,1)なので1を引き算して（0, 0）に\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1\n",
        "\n",
        "                # 幅、高さで規格化\n",
        "                if pt == 'xmin' or pt == 'xmax':  # x方向のときは幅で割算\n",
        "                    cur_pixel /= width\n",
        "                else:  # y方向のときは高さで割算\n",
        "                    cur_pixel /= height\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            # アノテーションのクラス名のindexを取得して追加\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            # resに[xmin, ymin, xmax, ymax, label_ind]を足す\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-zb7qGkUZ8E"
      },
      "source": [
        "# 動作確認　\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "\n",
        "# 画像の読み込み OpenCVを使用\n",
        "ind = 1\n",
        "image_file_path = val_img_list[ind]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "# アノテーションをリストで表示\n",
        "transform_anno(val_anno_list[ind], width, height)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx9GQLwfUZ8E"
      },
      "source": [
        "# 画像とアノテーションの前処理を行うクラスDataTransformを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3B9dd-vOUZ8E"
      },
      "source": [
        "# フォルダ「utils」にあるdata_augumentation.pyからimport。\n",
        "# 入力画像の前処理をするクラス\n",
        "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とアノテーションの前処理クラス。訓練と推論で異なる動作をする。\n",
        "    画像のサイズを300x300にする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    input_size : int\n",
        "        リサイズ先の画像の大きさ。\n",
        "    color_mean : (B, G, R)\n",
        "        各色チャネルの平均値。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # intをfloat32に変換\n",
        "                ToAbsoluteCoords(),  # アノテーションデータの規格化を戻す\n",
        "                PhotometricDistort(),  # 画像の色調などをランダムに変化\n",
        "                Expand(color_mean),  # 画像のキャンバスを広げる\n",
        "                RandomSampleCrop(),  # 画像内の部分をランダムに抜き出す\n",
        "                RandomMirror(),  # 画像を反転させる\n",
        "                ToPercentCoords(),  # アノテーションデータを0-1に規格化\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # intをfloatに変換\n",
        "                Resize(input_size),  # 画像サイズをinput_size×input_sizeに変形\n",
        "                SubtractMeans(color_mean)  # BGRの色の平均値を引き算\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        return self.data_transform[phase](img, boxes, labels)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0CT3h57UZ8F"
      },
      "source": [
        "# 動作の確認\n",
        "\n",
        "# 1. 画像読み込み\n",
        "image_file_path = train_img_list[0]\n",
        "img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "# 2. アノテーションをリストに\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "anno_list = transform_anno(train_anno_list[0], width, height)\n",
        "\n",
        "# 3. 元画像の表示\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 4. 前処理クラスの作成\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 5. train画像の表示\n",
        "phase = \"train\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. val画像の表示\n",
        "phase = \"val\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYV7387SUZ8F"
      },
      "source": [
        "# Datasetを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_KFhPFFUZ8G"
      },
      "source": [
        "# VOC2012のDatasetを作成する\n",
        "\n",
        "\n",
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    VOC2012のDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    anno_list : リスト\n",
        "        アノテーションへのパスを格納したリスト\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "    transform_anno : object\n",
        "        xmlのアノテーションをリストに変換するインスタンス\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  # train もしくは valを指定\n",
        "        self.transform = transform  # 画像の変形\n",
        "        self.transform_anno = transform_anno  # アノテーションデータをxmlからリストへ\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        '''\n",
        "        前処理をした画像のテンソル形式のデータとアノテーションを取得\n",
        "        '''\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''前処理をした画像のテンソル形式のデータ、アノテーション、画像の高さ、幅を取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "        height, width, channels = img.shape  # 画像のサイズを取得\n",
        "\n",
        "        # 2. xml形式のアノテーション情報をリストに\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "\n",
        "        # 3. 前処理を実施\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "\n",
        "        # 色チャネルの順番がBGRになっているので、RGBに順番変更\n",
        "        # さらに（高さ、幅、色チャネル）の順を（色チャネル、高さ、幅）に変換\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        # BBoxとラベルをセットにしたnp.arrayを作成、変数名「gt」はground truth（答え）の略称\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73pzj0GBUZ8G"
      },
      "source": [
        "# 動作確認\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "\n",
        "# データの取り出し例\n",
        "val_dataset.__getitem__(1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eggwYKQgUZ8G"
      },
      "source": [
        "# DataLoaderを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJoDTGLgUZ8G"
      },
      "source": [
        "def od_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Datasetから取り出すアノテーションデータのサイズが画像ごとに異なります。\n",
        "    画像内の物体数が2個であれば(2, 5)というサイズですが、3個であれば（3, 5）など変化します。\n",
        "    この変化に対応したDataLoaderを作成するために、\n",
        "    カスタイマイズした、collate_fnを作成します。\n",
        "    collate_fnは、PyTorchでリストからmini-batchを作成する関数です。\n",
        "    ミニバッチ分の画像が並んでいるリスト変数batchに、\n",
        "    ミニバッチ番号を指定する次元を先頭に1つ追加して、リストの形を変形します。\n",
        "    \"\"\"\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0] は画像imgです\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1] はアノテーションgtです\n",
        "\n",
        "    # imgsはミニバッチサイズのリストになっています\n",
        "    # リストの要素はtorch.Size([3, 300, 300])です。\n",
        "    # このリストをtorch.Size([batch_num, 3, 300, 300])のテンソルに変換します\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targetsはアノテーションデータの正解であるgtのリストです。\n",
        "    # リストのサイズはミニバッチサイズです。\n",
        "    # リストtargetsの要素は [n, 5] となっています。\n",
        "    # nは画像ごとに異なり、画像内にある物体の数となります。\n",
        "    # 5は [xmin, ymin, xmax, ymax, class_index] です\n",
        "\n",
        "    return imgs, targets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EyuZZVMUZ8H"
      },
      "source": [
        "# データローダーの作成\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(dataloaders_dict[\"val\"])  # イタレータに変換\n",
        "images, targets = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
        "print(len(targets))\n",
        "print(targets[1].size())  # ミニバッチのサイズのリスト、各要素は[n, 5]、nは物体数\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VlL2YTZUZ8H"
      },
      "source": [
        "print(train_dataset.__len__())\n",
        "print(val_dataset.__len__())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7JK64UDUZ8H"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1AYMTjSfhpY"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/02. "
      ]
    }
  ]
}