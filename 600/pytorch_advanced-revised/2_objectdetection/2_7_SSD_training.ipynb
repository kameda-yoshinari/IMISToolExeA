{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "2-7_SSD_training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/2_objectdetection/2_7_SSD_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRDrOClOJvlA"
      },
      "source": [
        "# 2.7 学習と検証の実施\n",
        "\n",
        "- 本ファイルでは、SSDの学習と検証の実施を行います。手元のマシンで動作を確認後、AWSのGPUマシンで計算します。\n",
        "- p2.xlargeで約6時間かかります。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQkSZ3f7JvlH"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "1.\tSSDの学習を実装できるようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-pUmMgPJvlI"
      },
      "source": [
        "# 事前準備\n",
        "\n",
        "- AWS EC2 のGPUインスタンスを使用します\n",
        "- フォルダ「utils」のssd_model.pyをします"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wFXuAjsJvlI"
      },
      "source": [
        "# パッケージのimport\n",
        "import os.path as osp\n",
        "import random\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKIiLcdZJvlJ"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNcYx2L0JvlK"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2hwPVpFJvlK"
      },
      "source": [
        "# DatasetとDataLoaderを作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9scicyiOJvlL"
      },
      "source": [
        "from utils.ssd_model import make_datapath_list, VOCDataset, DataTransform, Anno_xml2list, od_collate_fn\n",
        "\n",
        "\n",
        "# ファイルパスのリストを取得\n",
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(\n",
        "    rootpath)\n",
        "\n",
        "# Datasetを作成\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
        "input_size = 300  # 画像のinputサイズを300×300にする\n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "\n",
        "# DataLoaderを作成する\n",
        "batch_size = 32\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbyl0KAwJvlL"
      },
      "source": [
        "# ネットワークモデルの作成する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dFZ6oVjJvlM"
      },
      "source": [
        "from utils.ssd_model import SSD\n",
        "\n",
        "# SSD300の設定\n",
        "ssd_cfg = {\n",
        "    'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
        "    'input_size': 300,  # 画像の入力サイズ\n",
        "    'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],  # 各sourceの画像サイズ\n",
        "    'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "}\n",
        "\n",
        "# SSDネットワークモデル\n",
        "net = SSD(phase=\"train\", cfg=ssd_cfg)\n",
        "\n",
        "# SSDの初期の重みを設定\n",
        "# ssdのvgg部分に重みをロードする\n",
        "vgg_weights = torch.load('./weights/vgg16_reducedfc.pth')\n",
        "net.vgg.load_state_dict(vgg_weights)\n",
        "\n",
        "# ssdのその他のネットワークの重みはHeの初期値で初期化\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    if isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight.data)\n",
        "        if m.bias is not None:  # バイアス項がある場合\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "# Heの初期値を適用\n",
        "net.extras.apply(weights_init)\n",
        "net.loc.apply(weights_init)\n",
        "net.conf.apply(weights_init)\n",
        "\n",
        "# GPUが使えるかを確認\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"使用デバイス：\", device)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5tGwXegJvlM"
      },
      "source": [
        "# 損失関数と最適化手法を定義する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkZq2BPeJvlM"
      },
      "source": [
        "from utils.ssd_model import MultiBoxLoss\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = MultiBoxLoss(jaccard_thresh=0.5, neg_pos=3, device=device)\n",
        "\n",
        "# 最適化手法の設定\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-3,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXpicFdQJvlN"
      },
      "source": [
        "# 学習・検証を実施する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5BNcGMKJvlN"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # イテレーションカウンタをセット\n",
        "    iteration = 1\n",
        "    epoch_train_loss = 0.0  # epochの損失和\n",
        "    epoch_val_loss = 0.0  # epochの損失和\n",
        "    logs = []\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs+1):\n",
        "\n",
        "        # 開始時刻を保存\n",
        "        t_epoch_start = time.time()\n",
        "        t_iter_start = time.time()\n",
        "\n",
        "        print('-------------')\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-------------')\n",
        "\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "                print('（train）')\n",
        "            else:\n",
        "                if((epoch+1) % 10 == 0):\n",
        "                    net.eval()   # モデルを検証モードに\n",
        "                    print('-------------')\n",
        "                    print('（val）')\n",
        "                else:\n",
        "                    # 検証は10回に1回だけ行う\n",
        "                    continue\n",
        "\n",
        "            # データローダーからminibatchずつ取り出すループ\n",
        "            for images, targets in dataloaders_dict[phase]:\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                images = images.to(device)\n",
        "                targets = [ann.to(device)\n",
        "                           for ann in targets]  # リストの各要素のテンソルをGPUへ\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # 順伝搬（forward）計算\n",
        "                    outputs = net(images)\n",
        "\n",
        "                    # 損失の計算\n",
        "                    loss_l, loss_c = criterion(outputs, targets)\n",
        "                    loss = loss_l + loss_c\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()  # 勾配の計算\n",
        "\n",
        "                        # 勾配が大きくなりすぎると計算が不安定になるので、clipで最大でも勾配2.0に留める\n",
        "                        nn.utils.clip_grad_value_(\n",
        "                            net.parameters(), clip_value=2.0)\n",
        "\n",
        "                        optimizer.step()  # パラメータ更新\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            t_iter_finish = time.time()\n",
        "                            duration = t_iter_finish - t_iter_start\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n",
        "                                iteration, loss.item(), duration))\n",
        "                            t_iter_start = time.time()\n",
        "\n",
        "                        epoch_train_loss += loss.item()\n",
        "                        iteration += 1\n",
        "\n",
        "                    # 検証時\n",
        "                    else:\n",
        "                        epoch_val_loss += loss.item()\n",
        "\n",
        "        # epochのphaseごとのloss （Issue158での誤植修正）\n",
        "        t_epoch_finish = time.time()\n",
        "        print('-------------')\n",
        "        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n",
        "            epoch+1, epoch_train_loss, epoch_val_loss))\n",
        "        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n",
        "        t_epoch_start = time.time()\n",
        "\n",
        "        # ログを保存\n",
        "        log_epoch = {'epoch': epoch+1,\n",
        "                     'train_loss': epoch_train_loss, 'val_loss': epoch_val_loss}\n",
        "        logs.append(log_epoch)\n",
        "        df = pd.DataFrame(logs)\n",
        "        df.to_csv(\"log_output.csv\")\n",
        "\n",
        "        epoch_train_loss = 0.0  # epochの損失和\n",
        "        epoch_val_loss = 0.0  # epochの損失和\n",
        "\n",
        "        # ネットワークを保存する\n",
        "        if ((epoch+1) % 10 == 0):\n",
        "            torch.save(net.state_dict(), 'weights/ssd300_' +\n",
        "                       str(epoch+1) + '.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuOV3vkiJvlO"
      },
      "source": [
        "# 学習・検証を実行する\n",
        "num_epochs= 50  \n",
        "train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAXU63BxJvlP"
      },
      "source": [
        "以上"
      ]
    }
  ]
}