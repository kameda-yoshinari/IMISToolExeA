{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC4-2_DataLoader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/4_pose_estimation/GC4_2_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tb6rzfMPRZU6"
      },
      "source": [
        "# 4.2 DataLoaderの作成\n",
        "\n",
        "- 本ファイルでは、OpenPosetなど姿勢推定で使用するDatasetとDataLoaderを作成します。MS COCOデータセットを対象とします。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaAIo1XfRZVC"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "1.\tマスクデータについて理解する\n",
        "2.\tOpenPoseで使用するDatasetクラス、DataLoaderクラスを実装できるようになる\n",
        "3.\tOpenPoseの前処理およびデータオーギュメンテーションで、何をしているのか理解する\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"4_pose_estimation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PkHjWeV-cpi"
      },
      "source": [
        "---\n",
        "# Data extraction\n",
        "\n",
        "val2014.zip and mask.tar.gz are expanded here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OHg_wVA-cpl"
      },
      "source": [
        "# it takes 6 minutes or so.\n",
        "\n",
        "!mkdir -p /root/data\n",
        "%cd       /root/data\n",
        "!tar xfz '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/data/mask.tar.gz'\n",
        "# 40504 image files, 6GB or over in total\n",
        "!unzip -q -u '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/data/val2014.zip'\n",
        "\n",
        "%cd '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/'\n",
        "!rm -f data/val2014 data/mask\n",
        "!ln -s /root/data/val2014 data/val2014\n",
        "!ln -s /root/data/mask data/mask\n",
        "\n",
        "!ls -ld data/val2014/ data/mask/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w9HghHN-cpm"
      },
      "source": [
        "!ls -l utils data weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1KR7QmiRZVD"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "書籍の指示に従い、本章で使用するデータを用意します\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnAmwE6iRZVD"
      },
      "source": [
        "# 必要なパッケージのimport\n",
        "import json\n",
        "import os\n",
        "import os.path as osp\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch.utils.data as data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-hpGyxSRZVD"
      },
      "source": [
        "# 画像、マスク画像、アノテーションデータへのファイルパスリストを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRT36RH_RZVE"
      },
      "source": [
        "def make_datapath_list(rootpath):\n",
        "    \"\"\"\n",
        "    学習、検証の画像データとアノテーションデータ、マスクデータへのファイルパスリストを作成する。\n",
        "    \"\"\"\n",
        "\n",
        "    # アノテーションのJSONファイルを読み込む\n",
        "    json_path = osp.join(rootpath, 'COCO.json')\n",
        "    with open(json_path) as data_file:\n",
        "        data_this = json.load(data_file)\n",
        "        data_json = data_this['root']\n",
        "\n",
        "    # indexを格納\n",
        "    num_samples = len(data_json)\n",
        "    train_indexes = []\n",
        "    val_indexes = []\n",
        "    for count in range(num_samples):\n",
        "        if data_json[count]['isValidation'] != 0.:\n",
        "            val_indexes.append(count)\n",
        "        else:\n",
        "            train_indexes.append(count)\n",
        "\n",
        "    # 画像ファイルパスを格納\n",
        "    train_img_list = list()\n",
        "    val_img_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        train_img_list.append(img_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_path = os.path.join(rootpath, data_json[idx]['img_paths'])\n",
        "        val_img_list.append(img_path)\n",
        "\n",
        "    # マスクデータのパスを格納\n",
        "    train_mask_list = []\n",
        "    val_mask_list = []\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/train2014/mask_COCO_train2014_\" + img_idx+'.jpg'\n",
        "        train_mask_list.append(anno_path)\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        img_idx = data_json[idx]['img_paths'][-16:-4]\n",
        "        anno_path = \"./data/mask/val2014/mask_COCO_val2014_\" + img_idx+'.jpg'\n",
        "        val_mask_list.append(anno_path)\n",
        "\n",
        "    # アノテーションデータを格納\n",
        "    train_meta_list = list()\n",
        "    val_meta_list = list()\n",
        "\n",
        "    for idx in train_indexes:\n",
        "        train_meta_list.append(data_json[idx])\n",
        "\n",
        "    for idx in val_indexes:\n",
        "        val_meta_list.append(data_json[idx])\n",
        "\n",
        "    return train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sWLoTFjRZVE"
      },
      "source": [
        "# 動作確認（実行には10秒ほど時間がかかります）\n",
        "train_img_list, train_mask_list, val_img_list, val_mask_list, train_meta_list, val_meta_list = make_datapath_list(rootpath=\"./data/\")\n",
        "\n",
        "val_meta_list[24]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd7evimQRZVG"
      },
      "source": [
        "# マスクデータの働きを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOZDxkXDRZVG"
      },
      "source": [
        "index = 24\n",
        "\n",
        "# 画像\n",
        "img = cv2.imread(val_img_list[index])\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# マスク\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "mask_miss = cv2.cvtColor(mask_miss, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(mask_miss)\n",
        "plt.show()\n",
        "\n",
        "# 合成\n",
        "blend_img = cv2.addWeighted(img, 0.4, mask_miss, 0.6, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DoQovgnmRZVH"
      },
      "source": [
        "# 画像の前処理作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNikSSaxRZVH"
      },
      "source": [
        "# データ処理のクラスとデータオーギュメンテーションのクラスをimportする\n",
        "from utils.data_augumentation import Compose, get_anno, add_neck, aug_scale, aug_rotate, aug_croppad, aug_flip, remove_illegal_joint, Normalize_Tensor, no_Normalize_Tensor\n",
        "\n",
        "\n",
        "class DataTransform():\n",
        "    \"\"\"\n",
        "    画像とマスク、アノテーションの前処理クラス。\n",
        "    学習時と推論時で異なる動作をする。\n",
        "    学習時はデータオーギュメンテーションする。\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                get_anno(),  # JSONからアノテーションを辞書に格納\n",
        "                add_neck(),  # アノテーションデータの順番を変更し、さらに首のアノテーションデータを追加\n",
        "                aug_scale(),  # 拡大縮小\n",
        "                aug_rotate(),  # 回転\n",
        "                aug_croppad(),  # 切り出し\n",
        "                aug_flip(),  # 左右反転\n",
        "                remove_illegal_joint(),  # 画像からはみ出たアノテーションを除去\n",
        "                # Normalize_Tensor()  # 色情報の標準化とテンソル化\n",
        "                no_Normalize_Tensor()  # 本節のみ、色情報の標準化をなくす\n",
        "            ]),\n",
        "            'val': Compose([\n",
        "                # 本書では検証は省略\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, phase, meta_data, img, mask_miss):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        phase : 'train' or 'val'\n",
        "            前処理のモードを指定。\n",
        "        \"\"\"\n",
        "        meta_data, img, mask_miss = self.data_transform[phase](\n",
        "            meta_data, img, mask_miss)\n",
        "\n",
        "        return meta_data, img, mask_miss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvZT6uiFRZVI"
      },
      "source": [
        "# 動作確認\n",
        "# 画像読み込み\n",
        "index = 24\n",
        "img = cv2.imread(val_img_list[index])\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "meta_data = val_meta_list[index]\n",
        "\n",
        "# 画像前処理\n",
        "transform = DataTransform()\n",
        "meta_data, img, mask_miss = transform(\"train\", meta_data, img, mask_miss)\n",
        "\n",
        "# 画像表示\n",
        "img = img.numpy().transpose((1, 2, 0))\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "# マスク表示\n",
        "mask_miss = mask_miss.numpy().transpose((1, 2, 0))\n",
        "plt.imshow(mask_miss)\n",
        "plt.show()\n",
        "\n",
        "# 合成 RGBにそろえてから\n",
        "img = Image.fromarray(np.uint8(img*255))\n",
        "img = np.asarray(img.convert('RGB'))\n",
        "mask_miss = Image.fromarray(np.uint8((mask_miss)))\n",
        "mask_miss = np.asarray(mask_miss.convert('RGB'))\n",
        "blend_img = cv2.addWeighted(img, 0.4, mask_miss, 0.6, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3J_H4u7cRZVI"
      },
      "source": [
        "# 訓練データの正解情報として使うアノテーションデータの作成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUJDYZvPRhMr"
      },
      "source": [
        "※ Issue [#142] (https://github.com/YutaroOgawa/pytorch_advanced/issues/142) 対策済\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d7jvIvXL4O4"
      },
      "source": [
        "!grep \"heat_map\" *.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP33wQTORZVJ"
      },
      "source": [
        "from utils.dataloader import get_ground_truth\n",
        "\n",
        "# 画像読み込み\n",
        "index = 24\n",
        "img = cv2.imread(val_img_list[index])\n",
        "mask_miss = cv2.imread(val_mask_list[index])\n",
        "meta_data = val_meta_list[index]\n",
        "\n",
        "# 画像前処理\n",
        "meta_data, img, mask_miss = transform(\"train\", meta_data, img, mask_miss)\n",
        "\n",
        "img = img.numpy().transpose((1, 2, 0))\n",
        "mask_miss = mask_miss.numpy().transpose((1, 2, 0))\n",
        "\n",
        "# OpenPoseのアノテーションデータ生成\n",
        "heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(meta_data, mask_miss)\n",
        "\n",
        "# 画像表示\n",
        "plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIA6Y0GnRZVJ"
      },
      "source": [
        "# 左肘のheatmapを確認\n",
        "\n",
        "# 元画像\n",
        "img = Image.fromarray(np.uint8(img*255))\n",
        "img = np.asarray(img.convert('RGB'))\n",
        "\n",
        "# 左肘\n",
        "heat_map = heatmaps[:, :, 6]  # 6は左肘\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map.numpy())*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "heat_map = cv2.resize(\n",
        "    heat_map, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "# 注意：heatmapは画像サイズが1/8になっているので拡大する\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(img, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRdis8vNRZVJ"
      },
      "source": [
        "# 左手首\n",
        "heat_map = heatmaps[:, :, 7]  # 7は左手首\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map.numpy())*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "heat_map = cv2.resize(\n",
        "    heat_map, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(img, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXC2iiLHRZVK"
      },
      "source": [
        "# 左肘と左手首へのPAFを確認\n",
        "paf = pafs[:, :, 24]  # 24は左肘と左手首をつなぐxベクトルのPAF\n",
        "\n",
        "paf = Image.fromarray(np.uint8((paf)*255))\n",
        "paf = np.asarray(paf.convert('RGB'))\n",
        "paf = cv2.resize(\n",
        "    paf, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(img, 0.3, paf, 0.7, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcIn-n5VRZVK"
      },
      "source": [
        "# PAFのみを表示\n",
        "paf = pafs[:, :, 24]  # 24は左肘と左手首をつなぐxベクトルのPAF\n",
        "paf = Image.fromarray(np.uint8((paf)*255))\n",
        "paf = np.asarray(paf.convert('RGB'))\n",
        "paf = cv2.resize(\n",
        "    paf, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "plt.imshow(paf)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaBvPnqGRZVK"
      },
      "source": [
        "# Datasetの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpSyrYAkRZVK"
      },
      "source": [
        "from utils.dataloader import get_ground_truth\n",
        "\n",
        "\n",
        "class COCOkeypointsDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    MSCOCOのCocokeypointsのDatasetを作成するクラス。PyTorchのDatasetクラスを継承。\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : リスト\n",
        "        画像のパスを格納したリスト\n",
        "    anno_list : リスト\n",
        "        アノテーションへのパスを格納したリスト\n",
        "    phase : 'train' or 'test'\n",
        "        学習か訓練かを設定する。\n",
        "    transform : object\n",
        "        前処理クラスのインスタンス\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, mask_list, meta_list, phase, transform):\n",
        "        self.img_list = img_list\n",
        "        self.mask_list = mask_list\n",
        "        self.meta_list = meta_list\n",
        "        self.phase = phase\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        '''画像の枚数を返す'''\n",
        "        return len(self.img_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img, heatmaps, heat_mask, pafs, paf_mask = self.pull_item(index)\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask\n",
        "\n",
        "    def pull_item(self, index):\n",
        "        '''画像のTensor形式のデータ、アノテーション、マスクを取得する'''\n",
        "\n",
        "        # 1. 画像読み込み\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [高さ][幅][色BGR]\n",
        "\n",
        "        # 2. マスクとアノテーション読み込み\n",
        "        mask_miss = cv2.imread(self.mask_list[index])\n",
        "        meta_data = self.meta_list[index]\n",
        "\n",
        "        # 3. 画像前処理\n",
        "        meta_data, img, mask_miss = self.transform(\n",
        "            self.phase, meta_data, img, mask_miss)\n",
        "\n",
        "        # 4. 正解アノテーションデータの取得\n",
        "        mask_miss_numpy = mask_miss.numpy().transpose((1, 2, 0))\n",
        "        heat_mask, heatmaps, paf_mask, pafs = get_ground_truth(\n",
        "            meta_data, mask_miss_numpy)\n",
        "\n",
        "        # 5. マスクデータはRGBが(1,1,1)か(0,0,0)なので、次元を落とす\n",
        "        # マスクデータはマスクされている場所は値が0、それ以外は値が1です\n",
        "        heat_mask = heat_mask[:, :, :, 0]\n",
        "        paf_mask = paf_mask[:, :, :, 0]\n",
        "\n",
        "        # 6. チャネルが最後尾にあるので順番を変える\n",
        "        # 例：paf_mask：torch.Size([46, 46, 38])\n",
        "        # → torch.Size([38, 46, 46])\n",
        "        paf_mask = paf_mask.permute(2, 0, 1)\n",
        "        heat_mask = heat_mask.permute(2, 0, 1)\n",
        "        pafs = pafs.permute(2, 0, 1)\n",
        "        heatmaps = heatmaps.permute(2, 0, 1)\n",
        "\n",
        "        return img, heatmaps, heat_mask, pafs, paf_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCQw-iEyRZVL"
      },
      "source": [
        "# 動作確認\n",
        "train_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"train\", transform=DataTransform())\n",
        "val_dataset = COCOkeypointsDataset(\n",
        "    val_img_list, val_mask_list, val_meta_list, phase=\"val\", transform=DataTransform())\n",
        "\n",
        "\n",
        "# データの取り出し例\n",
        "item = train_dataset.__getitem__(0)\n",
        "print(item[0].shape)  # img\n",
        "print(item[1].shape)  # heatmaps,\n",
        "print(item[2].shape)  # heat_mask\n",
        "print(item[3].shape)  # pafs \n",
        "print(item[4].shape)  # paf_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mZPttvgRZVL"
      },
      "source": [
        "# DataLoaderの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjPfFF1ARZVM"
      },
      "source": [
        "# データローダーの作成\n",
        "batch_size = 8\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 辞書型変数にまとめる\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 動作の確認\n",
        "batch_iterator = iter(dataloaders_dict[\"train\"])  # イタレータに変換\n",
        "item = next(batch_iterator)  # 1番目の要素を取り出す\n",
        "print(item[0].shape)  # img\n",
        "print(item[1].shape)  # heatmaps,\n",
        "print(item[2].shape)  # heat_mask\n",
        "print(item[3].shape)  # pafs \n",
        "print(item[4].shape)  # paf_mask\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HSc2nbyTRZVM"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmTpfRbjOQ_8"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. Use of symbolic link on google drive.  \n",
        "2021/08/03. [issue170対応](https://github.com/YutaroOgawa/pytorch_advanced/issues/170)  \n",
        "2021/08/02.  \n"
      ]
    }
  ]
}