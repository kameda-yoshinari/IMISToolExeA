{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC4-7_OpenPose_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/4_pose_estimation/GC4_7_OpenPose_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMMvyHhxGgtt"
      },
      "source": [
        "# 4.7 推論の実施\n",
        "\n",
        "- 本ファイルでは、学習させたOpenPoseで姿勢推定を行います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxd4CMjKGgt1"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "\n",
        "1.\tOpenPoseの学習済みモデルをロードできるようになる\n",
        "2.\tOpenPoseの推論を実装できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"4_pose_estimation\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PkHjWeV-cpi"
      },
      "source": [
        "---\n",
        "# Data extraction\n",
        "\n",
        "val2014.zip and mask.tar.gz are expanded here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OHg_wVA-cpl"
      },
      "source": [
        "# it takes 6 minutes or so.\n",
        "\n",
        "!mkdir -p /root/data\n",
        "%cd       /root/data\n",
        "!tar xfz '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/data/mask.tar.gz'\n",
        "# 40504 image files, 6GB or over in total\n",
        "!unzip -q -u '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/data/val2014.zip'\n",
        "\n",
        "%cd '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/'\n",
        "!rm -f data/val2014 data/mask\n",
        "!ln -s /root/data/val2014 data/val2014\n",
        "!ln -s /root/data/mask data/mask\n",
        "\n",
        "!ls -ld data/val2014/ data/mask/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-w9HghHN-cpm"
      },
      "source": [
        "!ls -l utils data weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWPaCzjEGgt2"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "\n",
        "- 学習済みの重みパラメータ「pose_model_scratch.pth」をフォルダ「weights」に用意済のはずです。\n",
        "- ※ Issue [#142] (https://github.com/YutaroOgawa/pytorch_advanced/issues/142) 対策不要？\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMXPJsSGgt2"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg_MjI-OGgt3"
      },
      "source": [
        "from utils.openpose_net import OpenPoseNet\n",
        "\n",
        "# 学習済みモデルと本章のモデルでネットワークの層の名前が違うので、対応させてロードする\n",
        "# モデルの定義\n",
        "net = OpenPoseNet()\n",
        "\n",
        "# 学習済みパラメータをロードする\n",
        "net_weights = torch.load(\n",
        "    './weights/pose_model_scratch.pth', map_location={'cuda:0': 'cpu'})\n",
        "keys = list(net_weights.keys())\n",
        "\n",
        "weights_load = {}\n",
        "\n",
        "# ロードした内容を、本書で構築したモデルの\n",
        "# パラメータ名net.state_dict().keys()にコピーする\n",
        "for i in range(len(keys)):\n",
        "    weights_load[list(net.state_dict().keys())[i]\n",
        "                 ] = net_weights[list(keys)[i]]\n",
        "\n",
        "# コピーした内容をモデルに与える\n",
        "state = net.state_dict()\n",
        "state.update(weights_load)\n",
        "net.load_state_dict(state)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xX2RJt_Ggt4"
      },
      "source": [
        "# 草野球の画像を読み込み、前処理します\n",
        "\n",
        "test_image = './data/hit-1407826_640.jpg'\n",
        "oriImg = cv2.imread(test_image)  # B,G,Rの順番\n",
        "\n",
        "# BGRをRGBにして表示\n",
        "oriImg = cv2.cvtColor(oriImg, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "# 画像のリサイズ\n",
        "size = (368, 368)\n",
        "img = cv2.resize(oriImg, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# 画像の前処理\n",
        "img = img.astype(np.float32) / 255.\n",
        "\n",
        "# 色情報の標準化\n",
        "color_mean = [0.485, 0.456, 0.406]\n",
        "color_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# 21/03/07 Issue147 https://github.com/YutaroOgawa/pytorch_advanced/issues/147\n",
        "# 色チャネルの順番を誤っています\n",
        "# preprocessed_img = img.copy()[:, :, ::-1]  # RGB→BGR\n",
        "preprocessed_img = img.copy()  # RGB\n",
        "\n",
        "for i in range(3):\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - color_mean[i]\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / color_std[i]\n",
        "\n",
        "# （高さ、幅、色）→（色、高さ、幅）\n",
        "img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "# 画像をTensorに\n",
        "img = torch.from_numpy(img)\n",
        "\n",
        "# ミニバッチ化：torch.Size([1, 3, 368, 368])\n",
        "x = img.unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwwD0jTGgt4"
      },
      "source": [
        "# OpenPoseでheatmapsとPAFsを求めます\n",
        "net.eval()\n",
        "predicted_outputs, _ = net(x)\n",
        "\n",
        "# 画像をテンソルからNumPyに変化し、サイズを戻します\n",
        "pafs = predicted_outputs[0][0].detach().numpy().transpose(1, 2, 0)\n",
        "heatmaps = predicted_outputs[1][0].detach().numpy().transpose(1, 2, 0)\n",
        "\n",
        "pafs = cv2.resize(pafs, size, interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(heatmaps, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "pafs = cv2.resize(\n",
        "    pafs, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(\n",
        "    heatmaps, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-YecEqGgt5"
      },
      "source": [
        "# 左肘と左手首のheatmap、そして左肘と左手首をつなぐPAFのxベクトルを可視化する\n",
        "# 左肘\n",
        "heat_map = heatmaps[:, :, 6]  # 6は左肘\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 左手首\n",
        "heat_map = heatmaps[:, :, 7]  # 7は左手首\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map)*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 左肘と左手首をつなぐPAFのxベクトル\n",
        "paf = pafs[:, :, 24]\n",
        "paf = Image.fromarray(np.uint8(cm.jet(paf)*255))\n",
        "paf = np.asarray(paf.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, paf, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhR2IvFEGgt5"
      },
      "source": [
        "from utils.decode_pose import decode_pose\n",
        "_, result_img, _, _ = decode_pose(oriImg, heatmaps, pafs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFC-m6szGgt6"
      },
      "source": [
        "# 結果を描画\n",
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(result_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5nnUpdLGgt6"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0H5F48xWDSN"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. Use of symbolic link on google drive.  \n",
        "2021/08/02. "
      ]
    }
  ]
}