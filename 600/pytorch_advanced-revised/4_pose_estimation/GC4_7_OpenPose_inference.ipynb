{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC4-7_OpenPose_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/4_pose_estimation/GC4_7_OpenPose_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMMvyHhxGgtt"
      },
      "source": [
        "# 4.7 推論の実施\n",
        "\n",
        "- 本ファイルでは、学習させたOpenPoseで姿勢推定を行います。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxd4CMjKGgt1"
      },
      "source": [
        "# 学習目標\n",
        "\n",
        "\n",
        "1.\tOpenPoseの学習済みモデルをロードできるようになる\n",
        "2.\tOpenPoseの推論を実装できるようになる\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfd73959-3d4a-422b-865d-0ba419b66d85"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Change to the JST notation.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a01307-0593-488f-d327-a62521a8f513"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start mounting your Google Drive.\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive\n",
            "Move to the working directory.\n",
            "/content/drive/My Drive/202107_Tool-A/Work600\n",
            "total 4\n",
            "drwx------ 13 root root 4096 Jul 31 09:41 pytorch_advanced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfe5070b-0913-4613-d20c-e6cea85c1181"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OK. Alreadly git cloned. You can go.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9adc9c-b05d-44f8-9807-f7b94f03df56"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch_advanced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02af2f34-6bae-45b7-cebb-50fa31b70b07"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1deee81b-b1bd-4632-f521-51ba8c563f7d"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1_image_classification\t 7_nlp_sentiment_transformer\n",
            "2_objectdetection\t 8_nlp_sentiment_bert\n",
            "3_semantic_segmentation  9_video_classification_eco\n",
            "4_pose_estimation\t etc\n",
            "5_gan_generation\t LICENSE\n",
            "6_gan_anomaly_detection  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27dda35e-3057-42cd-d837-3b20a5caefe3"
      },
      "source": [
        "%cd \"4_pose_estimation\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb616179-ae0d-4f22-884c-5340125987b8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4-2_DataLoader.ipynb\t     4-7_OpenPose_inference.ipynb\t    utils\n",
            "4-3-4_NetworkModel.ipynb     data\t\t\t\t    weights\n",
            "4-5_TensorBoardX.ipynb\t     make_folders_and_data_downloads.ipynb\n",
            "4-6_OpenPose_training.ipynb  tbX\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCElkS-f3p-v"
      },
      "source": [
        "---\n",
        "# Utils, Data, and Weights\n",
        "\n",
        "Working directories are:\n",
        "\n",
        "* /root/utils  \n",
        "* /root/data  \n",
        "* /root/weights  \n",
        "\n",
        "val2014.zip and mask.tar.gz are expanded there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T1Dk6II1n8-",
        "outputId": "7301d967-3f42-48ff-f496-9f37f830b7be"
      },
      "source": [
        "!ls -l '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2628\n",
            "-rw------- 1 root root 1273983 Jul 31 09:42 4-2_DataLoader.ipynb\n",
            "-rw------- 1 root root  150239 Jul 31 09:42 4-3-4_NetworkModel.ipynb\n",
            "-rw------- 1 root root   15393 Jul 31 09:42 4-5_TensorBoardX.ipynb\n",
            "-rw------- 1 root root   14793 Jul 31 09:42 4-6_OpenPose_training.ipynb\n",
            "-rw------- 1 root root 1213399 Jul 31 09:42 4-7_OpenPose_inference.ipynb\n",
            "drwx------ 2 root root    4096 Jul 31 09:42 data\n",
            "-rw------- 1 root root    5310 Jul 31 09:42 make_folders_and_data_downloads.ipynb\n",
            "drwx------ 2 root root    4096 Aug  1 14:51 tbX\n",
            "drwx------ 3 root root    4096 Jul 31 09:42 utils\n",
            "drwx------ 2 root root    4096 Aug  1 12:49 weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMH-_2g34yS7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9de086a-0155-453c-8a65-7f15a1e9739f"
      },
      "source": [
        "# This will take 8 minutes.\n",
        "\n",
        "%cd ~\n",
        "\n",
        "!cp -r '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/utils/'   .\n",
        "!cp -r '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/data/'    .\n",
        "!cp -r '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/4_pose_estimation/weights/' .\n",
        "\n",
        "%cd data\n",
        "!unzip -q -o  val2014.zip\n",
        "!tar xfz mask.tar.gz\n",
        "\n",
        "%cd ~"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dg4_dOq6Yz-"
      },
      "source": [
        "!ls -l utils data weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWPaCzjEGgt2"
      },
      "source": [
        "---\n",
        "# 事前準備\n",
        "\n",
        "- 学習済みの重みパラメータ「pose_model_scratch.pth」をフォルダ「weights」に用意済のはずです。\n",
        "- ※ Issue [#142] (https://github.com/YutaroOgawa/pytorch_advanced/issues/142) 対策済\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHMXPJsSGgt2"
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fg_MjI-OGgt3"
      },
      "source": [
        "from utils.openpose_net import OpenPoseNet\n",
        "\n",
        "# 学習済みモデルと本章のモデルでネットワークの層の名前が違うので、対応させてロードする\n",
        "# モデルの定義\n",
        "net = OpenPoseNet()\n",
        "\n",
        "# 学習済みパラメータをロードする\n",
        "net_weights = torch.load(\n",
        "    './weights/pose_model_scratch.pth', map_location={'cuda:0': 'cpu'})\n",
        "keys = list(net_weights.keys())\n",
        "\n",
        "weights_load = {}\n",
        "\n",
        "# ロードした内容を、本書で構築したモデルの\n",
        "# パラメータ名net.state_dict().keys()にコピーする\n",
        "for i in range(len(keys)):\n",
        "    weights_load[list(net.state_dict().keys())[i]\n",
        "                 ] = net_weights[list(keys)[i]]\n",
        "\n",
        "# コピーした内容をモデルに与える\n",
        "state = net.state_dict()\n",
        "state.update(weights_load)\n",
        "net.load_state_dict(state)\n",
        "\n",
        "print('ネットワーク設定完了：学習済みの重みをロードしました')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xX2RJt_Ggt4"
      },
      "source": [
        "# 草野球の画像を読み込み、前処理します\n",
        "\n",
        "test_image = './data/hit-1407826_640.jpg'\n",
        "oriImg = cv2.imread(test_image)  # B,G,Rの順番\n",
        "\n",
        "# BGRをRGBにして表示\n",
        "oriImg = cv2.cvtColor(oriImg, cv2.COLOR_BGR2RGB)\n",
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "# 画像のリサイズ\n",
        "size = (368, 368)\n",
        "img = cv2.resize(oriImg, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "# 画像の前処理\n",
        "img = img.astype(np.float32) / 255.\n",
        "\n",
        "# 色情報の標準化\n",
        "color_mean = [0.485, 0.456, 0.406]\n",
        "color_std = [0.229, 0.224, 0.225]\n",
        "\n",
        "# 21/03/07 Issue147 https://github.com/YutaroOgawa/pytorch_advanced/issues/147\n",
        "# 色チャネルの順番を誤っています\n",
        "# preprocessed_img = img.copy()[:, :, ::-1]  # RGB→BGR\n",
        "preprocessed_img = img.copy()  # RGB\n",
        "\n",
        "for i in range(3):\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - color_mean[i]\n",
        "    preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / color_std[i]\n",
        "\n",
        "# （高さ、幅、色）→（色、高さ、幅）\n",
        "img = preprocessed_img.transpose((2, 0, 1)).astype(np.float32)\n",
        "\n",
        "# 画像をTensorに\n",
        "img = torch.from_numpy(img)\n",
        "\n",
        "# ミニバッチ化：torch.Size([1, 3, 368, 368])\n",
        "x = img.unsqueeze(0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRwwD0jTGgt4"
      },
      "source": [
        "# OpenPoseでheatmapsとPAFsを求めます\n",
        "net.eval()\n",
        "predicted_outputs, _ = net(x)\n",
        "\n",
        "# 画像をテンソルからNumPyに変化し、サイズを戻します\n",
        "pafs = predicted_outputs[0][0].detach().numpy().transpose(1, 2, 0)\n",
        "heatmaps = predicted_outputs[1][0].detach().numpy().transpose(1, 2, 0)\n",
        "\n",
        "pafs = cv2.resize(pafs, size, interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(heatmaps, size, interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "pafs = cv2.resize(\n",
        "    pafs, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n",
        "heatmaps = cv2.resize(\n",
        "    heatmaps, (oriImg.shape[1], oriImg.shape[0]), interpolation=cv2.INTER_CUBIC)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd-YecEqGgt5"
      },
      "source": [
        "# 左肘と左手首のheatmap、そして左肘と左手首をつなぐPAFのxベクトルを可視化する\n",
        "# 左肘\n",
        "heat_map = heatmaps[:, :, 6]  # 6は左肘\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map.numpy())*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 左手首\n",
        "heat_map = heatmaps[:, :, 7]  # 7は左手首\n",
        "heat_map = Image.fromarray(np.uint8(cm.jet(heat_map.numpy())*255))\n",
        "heat_map = np.asarray(heat_map.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, heat_map, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 左肘と左手首をつなぐPAFのxベクトル\n",
        "paf = pafs[:, :, 24]\n",
        "paf = Image.fromarray(np.uint8(cm.jet(paf)*255))\n",
        "paf = np.asarray(paf.convert('RGB'))\n",
        "\n",
        "# 合成して表示\n",
        "blend_img = cv2.addWeighted(oriImg, 0.5, paf, 0.5, 0)\n",
        "plt.imshow(blend_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhR2IvFEGgt5"
      },
      "source": [
        "from utils.decode_pose import decode_pose\n",
        "_, result_img, _, _ = decode_pose(oriImg, heatmaps, pafs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFC-m6szGgt6"
      },
      "source": [
        "# 結果を描画\n",
        "plt.imshow(oriImg)\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(result_img)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5nnUpdLGgt6"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0H5F48xWDSN"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/02. "
      ]
    }
  ]
}