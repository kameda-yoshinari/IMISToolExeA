{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC7-4_vectorize.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/7_nlp_sentiment_transformer/GC7_4_vectorize.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh6pYLx-IB9r"
      },
      "source": [
        "# 7.4 word2vec、fastTextを用いた日本語単語のベクトル表現の実装\n",
        "\n",
        "- 本ファイルでは、日本語の単語をword2vecもしくはfastTextを使用してベクトル化する手法を解説します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8un2N2hIB9t"
      },
      "source": [
        "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k276Mgs3IB9u"
      },
      "source": [
        "# 7.4 学習目標\n",
        "\n",
        "1.\t学習済みの日本語word2vecモデルで単語をベクトル表現に変換する実装ができるようになる\n",
        "2.\t学習済みの日本語fastText モデルで単語をベクトル表現に変換する実装ができるようになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"7_nlp_sentiment_transformer\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIBhIU7cj1wb"
      },
      "source": [
        "---\n",
        "# Preparation for word separation (GC7-1)\n",
        "\n",
        "This is the whole procedure of preparation of word separation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e1nNRpmj5ZZ"
      },
      "source": [
        "# janome, mecab, ipadic-neologd, and mecab-pytorch\n",
        "\n",
        "!pip install janome > /dev/null\n",
        "\n",
        "# https://qiita.com/jun40vn/items/78e33e29dce3d50c2df1\n",
        "\n",
        "%cd ~\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!test -d mecab-ipadic-neologd || git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "!echo yes | ./mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "\n",
        "!test -f /usr/local/etc/mecabrc || ln -s /etc/mecabrc /usr/local/etc/mecabrc \n",
        "\n",
        "!echo \"Dictionary path is : \"\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "\n",
        "%cd -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B67SEcQOIB9u"
      },
      "source": [
        "---\n",
        "# 1. 文書を読み込んで、分かち書き、データセット作成まで（8.2と同じです）\n",
        "\n",
        "前処理と分かち書きをし、最後にデータセットを作成する部分を実装します\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaf_n3fiIB9v"
      },
      "source": [
        "# 単語分割にはMecab＋NEologdを使用\n",
        "import MeCab\n",
        "\n",
        "#m_t = MeCab.Tagger('-Owakati -d /usr/lib/mecab/dic/mecab-ipadic-neologd')\n",
        "m_t = MeCab.Tagger('-Owakati -d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd')\n",
        "\n",
        "def tokenizer_mecab(text):\n",
        "    text = m_t.parse(text)  # これでスペースで単語が区切られる\n",
        "    ret = text.strip().split()  # スペース部分で区切ったリストに変換\n",
        "    return ret\n",
        "\n",
        "\n",
        "\n",
        "# 前処理として正規化をする関数を定義\n",
        "import re\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    # 改行、半角スペース、全角スペースを削除\n",
        "    text = re.sub('\\r', '', text)\n",
        "    text = re.sub('\\n', '', text)\n",
        "    text = re.sub('　', '', text)\n",
        "    text = re.sub(' ', '', text)\n",
        "\n",
        "    # 数字文字の一律「0」化\n",
        "    text = re.sub(r'[0-9 ０-９]', '0', text)  # 数字\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "# 前処理とJanomeの単語分割を合わせた関数を定義する\n",
        "\n",
        "\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)  # 前処理の正規化\n",
        "    ret = tokenizer_mecab(text)  # Mecabの単語分割\n",
        "\n",
        "    return ret\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9xAaERVIB9v"
      },
      "source": [
        "import torchtext.legacy\n",
        "\n",
        "# tsvやcsvデータを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "# 文章とラベルの両方に用意します\n",
        "\n",
        "max_length = 25\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
        "                            use_vocab=True, lower=True, include_lengths=True, batch_first=True, fix_length=max_length)\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "\n",
        "# フォルダ「data」から各tsvファイルを読み込みます\n",
        "train_ds, val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='./data/', train='text_train.tsv',\n",
        "    validation='text_val.tsv', test='text_test.tsv', format='tsv',\n",
        "    fields=[('Text', TEXT), ('Label', LABEL)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoqLRj3BIB9w"
      },
      "source": [
        "# 2. 単語のベクトル化"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nizLkClMIB9w"
      },
      "source": [
        "## 2.1 word2vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uWFu47CIB9w"
      },
      "source": [
        "単語をベクトル表現に変換します。\n",
        "\n",
        "TorchTextには日本語の学習済みデータがないわけではないですが、精度が微妙なので\n",
        "\n",
        "東北大学 乾・岡崎研究室で公開されているWord2Vecの学習済みのベクトルを使用します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jbXDpD5IB9x"
      },
      "source": [
        "# 以下から、日本語のfasttextの学習済みベクトルをダウンロードします\n",
        "\n",
        "# 東北大学 乾・岡崎研究室：日本語 Wikipedia エンティティベクトル\n",
        "\n",
        "# http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/\n",
        "# http://www.cl.ecei.tohoku.ac.jp/~m-suzuki/jawiki_vector/data/20170201.tar.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rxvSCojIB9x"
      },
      "source": [
        "# そのままではtorchtextで読み込めないので、gensimライブラリを使用して、\n",
        "# Word2Vecのformatで保存し直します\n",
        "\n",
        "# 事前インストール\n",
        "# pip install gensim\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "# 一度gensimライブラリで読み込んで、word2vecのformatで保存する\n",
        "model = KeyedVectors.load_word2vec_format(\n",
        "    './data/entity_vector/entity_vector.model.bin', binary=True)\n",
        "\n",
        "# 保存（時間がかかります、約3分）\n",
        "model.wv.save_word2vec_format('./data/japanese_word2vec_vectors.vec')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZy9s5jfIB9y"
      },
      "source": [
        "# It takes 1 minute or so.\n",
        "\n",
        "# torchtextで単語ベクトルとして読み込みます\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "japanese_word2vec_vectors = Vectors(\n",
        "    name='./data/japanese_word2vec_vectors.vec')\n",
        "\n",
        "# 単語ベクトルの中身を確認します\n",
        "print(\"1単語を表現する次元数：\", japanese_word2vec_vectors.dim)\n",
        "print(\"単語数：\", len(japanese_word2vec_vectors.itos))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5efWo7HIB9y"
      },
      "source": [
        "# ベクトル化したバージョンのボキャブラリーを作成します\n",
        "TEXT.build_vocab(train_ds, vectors=japanese_word2vec_vectors, min_freq=1)\n",
        "\n",
        "# ボキャブラリーのベクトルを確認します\n",
        "print(TEXT.vocab.vectors.shape)  # 49個の単語が200次元のベクトルで表現されている\n",
        "TEXT.vocab.vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRWJ-9QeIB9y"
      },
      "source": [
        "# ボキャブラリーの単語の順番を確認します\n",
        "TEXT.vocab.stoi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSWMnH_BIB9y"
      },
      "source": [
        "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 姫 - 女性 + 男性\n",
        "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
        "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
        "\n",
        "# コサイン類似度を計算\n",
        "# dim=0 は0次元目で計算してくださいという指定\n",
        "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
        "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
        "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
        "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI4qICVgIB9z"
      },
      "source": [
        "姫 - 女性 + 男性　を計算すると狙った通り、王子がもっとも近い結果になりました"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5mPu3PeIB9z"
      },
      "source": [
        "## 2.2 fastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thq7ajARIB9z"
      },
      "source": [
        "word2vecより進歩したベクトル化手法であるfastTextによる単語のベクトル表現を使用します。\n",
        "\n",
        "日本語の学習モデルを以下の記事にて公開してくださっているので、使用させていただきます。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPK2jgWBIB9z"
      },
      "source": [
        "# Qiita：いますぐ使える単語埋め込みベクトルのリスト\n",
        "# https://qiita.com/Hironsan/items/8f7d35f0a36e0f99752c\n",
        "\n",
        "# Download Word Vectors\n",
        "# https://drive.google.com/open?id=0ByFQ96A4DgSPNFdleG1GaHcxQzA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vXqKYivIB9z"
      },
      "source": [
        "# torchtextで単語ベクトルとして読み込みます\n",
        "# word2vecとは異なり、すぐに読み込めます (1 minute or so)\n",
        "\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "japanese_fasttext_vectors = Vectors(name='./data/vector_neologd/model.vec')\n",
        "\n",
        "                                    \n",
        "# 単語ベクトルの中身を確認します\n",
        "print(\"1単語を表現する次元数：\", japanese_fasttext_vectors.dim)\n",
        "print(\"単語数：\", len(japanese_fasttext_vectors.itos))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s08JRD1lIB90"
      },
      "source": [
        "# ベクトル化したバージョンのボキャブラリーを作成します\n",
        "TEXT.build_vocab(train_ds, vectors=japanese_fasttext_vectors, min_freq=1)\n",
        "\n",
        "# ボキャブラリーのベクトルを確認します\n",
        "print(TEXT.vocab.vectors.shape)  # 52個の単語が300次元のベクトルで表現されている\n",
        "TEXT.vocab.vectors\n",
        "\n",
        "# ボキャブラリーの単語の順番を確認します\n",
        "TEXT.vocab.stoi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yG4QS_8IB90"
      },
      "source": [
        "# 姫 - 女性 + 男性 のベクトルがどれと似ているのか確認してみます\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 姫 - 女性 + 男性\n",
        "tensor_calc = TEXT.vocab.vectors[41] - \\\n",
        "    TEXT.vocab.vectors[38] + TEXT.vocab.vectors[46]\n",
        "\n",
        "# コサイン類似度を計算\n",
        "# dim=0 は0次元目で計算してくださいという指定\n",
        "print(\"女王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[39], dim=0))\n",
        "print(\"王\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[44], dim=0))\n",
        "print(\"王子\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[45], dim=0))\n",
        "print(\"機械学習\", F.cosine_similarity(tensor_calc, TEXT.vocab.vectors[43], dim=0))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnjXJrP9IB90"
      },
      "source": [
        "姫 - 女性 + 男性　を計算すると狙った通り、王子がもっとも近い結果になりました"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBSBwmgRIB90"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZN5DxZ8aSPd"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. "
      ]
    }
  ]
}