{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "GC7-5_IMDb_Dataset_DataLoader.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/7_nlp_sentiment_transformer/GC7_5_IMDb_Dataset_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp9DPxVvbpPS"
      },
      "source": [
        "# 7.5 IMDb（Internet Movie Database）からDataLoaderを作成\n",
        "\n",
        "- 本ファイルでは、IMDb（Internet Movie Database）のデータを使用して、感情分析（0：ネガティブ、1：ポジティブ）を2値クラス分類するためのDatasetとDataLoaderを作成します。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ED3mvbE9bpPV"
      },
      "source": [
        "※　本章のファイルはすべてUbuntuでの動作を前提としています。Windowsなど文字コードが違う環境での動作にはご注意下さい。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJZ7foADbpPV"
      },
      "source": [
        "# 7.5 学習目標\n",
        "\n",
        "1.\tテキスト形式のファイルデータからtsvファイルを作成し、torchtext用のDataLoaderを作成できるようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"7_nlp_sentiment_transformer\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxprtbsK4RJw"
      },
      "source": [
        "---\n",
        "# Extraction of aclImdb (GC7-0)\n",
        "\n",
        "The working directory is at /content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/ .\n",
        "\n",
        "aclImdb will be prepared at /root/data/aclImdb and it will be accessible through \n",
        "/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/data/aclImdb ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDD4Tx9b2htz"
      },
      "source": [
        "# local drive on GC is very, very fast.\n",
        "!mkdir -p /root/data/\n",
        "%cd /root/data/\n",
        "!tar xfz '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/data/aclImdb_v1.tar.gz'\n",
        "\n",
        "# make a symbolic link at the working directory on google drive.\n",
        "%cd '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/'\n",
        "!rm -f data/aclImdb\n",
        "!ln -s /root/data/aclImdb data/aclImdb\n",
        "\n",
        "!ls -ld data/aclImdb/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VIBhIU7cj1wb"
      },
      "source": [
        "---\n",
        "# Preparation for word separation (GC7-1)\n",
        "\n",
        "This is the whole procedure of preparation of word separation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e1nNRpmj5ZZ"
      },
      "source": [
        "# janome, mecab, ipadic-neologd, and mecab-pytorch\n",
        "\n",
        "!pip install janome > /dev/null\n",
        "\n",
        "# https://qiita.com/jun40vn/items/78e33e29dce3d50c2df1\n",
        "\n",
        "%cd ~\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!test -d mecab-ipadic-neologd || git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null 2>&1\n",
        "!echo yes | ./mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "\n",
        "!test -f /usr/local/etc/mecabrc || ln -s /etc/mecabrc /usr/local/etc/mecabrc \n",
        "\n",
        "!echo \"Dictionary path is : \"\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "\n",
        "%cd -"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeVplLqbpPV"
      },
      "source": [
        "---\n",
        "# 1. IMDbデータセットをtsv形式に変換\n",
        "\n",
        "Datasetをダウンロードします\n",
        "\n",
        "※torchtextで標準でIMDbが使える関数があるのですが、今回は今後データセットが用意されていない場合でも対応できるように0から作ります。\n",
        "\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "\n",
        "5万件のデータ（train,testともに2.5万件）です。データidとrating（1-10）でファイル名が決まっています。\n",
        "\n",
        "rateは10の方が良いです。4以下がnegative、7以上がpositiveにクラス分けされています。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ_BBRNXbpPW"
      },
      "source": [
        "# tsv形式のファイルにします\n",
        "import glob\n",
        "import os\n",
        "import io\n",
        "import string\n",
        "\n",
        "\n",
        "# 訓練データのtsvファイルを作成します\n",
        "\n",
        "f = open('./data/IMDb_train.tsv', 'w')\n",
        "\n",
        "path = './data/aclImdb/train/pos/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "path = './data/aclImdb/train/neg/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtkRP7_ubpPW"
      },
      "source": [
        "# テストデータの作成\n",
        "\n",
        "f = open('./data/IMDb_test.tsv', 'w')\n",
        "\n",
        "path = './data/aclImdb/test/pos/'\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'1'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "\n",
        "path = './data/aclImdb/test/neg/'\n",
        "\n",
        "for fname in glob.glob(os.path.join(path, '*.txt')):\n",
        "    with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
        "        text = ff.readline()\n",
        "\n",
        "        # タブがあれば消しておきます\n",
        "        text = text.replace('\\t', \" \")\n",
        "\n",
        "        text = text+'\\t'+'0'+'\\t'+'\\n'\n",
        "        f.write(text)\n",
        "\n",
        "f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-yDVQvrbpPX"
      },
      "source": [
        "# 2. 前処理と単語分割の関数を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViJj_-L4bpPX"
      },
      "source": [
        "import string\n",
        "import re\n",
        "\n",
        "# 以下の記号はスペースに置き換えます（カンマ、ピリオドを除く）。\n",
        "# punctuationとは日本語で句点という意味です\n",
        "print(\"区切り文字：\", string.punctuation)\n",
        "# !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "\n",
        "# 前処理\n",
        "\n",
        "\n",
        "def preprocessing_text(text):\n",
        "    # 改行コードを消去\n",
        "    text = re.sub('<br />', '', text)\n",
        "\n",
        "    # カンマ、ピリオド以外の記号をスペースに置換\n",
        "    for p in string.punctuation:\n",
        "        if (p == \".\") or (p == \",\"):\n",
        "            continue\n",
        "        else:\n",
        "            text = text.replace(p, \" \")\n",
        "\n",
        "    # ピリオドなどの前後にはスペースを入れておく\n",
        "    text = text.replace(\".\", \" . \")\n",
        "    text = text.replace(\",\", \" , \")\n",
        "    return text\n",
        "\n",
        "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
        "\n",
        "\n",
        "def tokenizer_punctuation(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "\n",
        "# 前処理と分かち書きをまとめた関数を定義\n",
        "def tokenizer_with_preprocessing(text):\n",
        "    text = preprocessing_text(text)\n",
        "    ret = tokenizer_punctuation(text)\n",
        "    return ret\n",
        "\n",
        "\n",
        "# 動作を確認します\n",
        "print(tokenizer_with_preprocessing('I like cats.'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FMu6pWebpPY"
      },
      "source": [
        "# DataLoaderの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jkvy8v3ZbpPY"
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "# https://github.com/YutaroOgawa/pytorch_advanced/issues/148\n",
        "import torchtext \n",
        "\n",
        "# 文章とラベルの両方に用意します\n",
        "max_length = 256\n",
        "TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
        "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
        "LABEL = torchtext.legacy.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# 引数の意味は次の通り\n",
        "# init_token：全部の文章で、文頭に入れておく単語\n",
        "# eos_token：全部の文章で、文末に入れておく単語\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9cQ40yObpPY"
      },
      "source": [
        "# フォルダ「data」から各tsvファイルを読み込みます\n",
        "train_val_ds, test_ds = torchtext.legacy.data.TabularDataset.splits(\n",
        "    path='./data/', train='IMDb_train.tsv',\n",
        "    test='IMDb_test.tsv', format='tsv',\n",
        "    fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n",
        "# 動作確認\n",
        "print('訓練および検証のデータ数', len(train_val_ds))\n",
        "print('1つ目の訓練および検証のデータ', vars(train_val_ds[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JP1gusslbpPZ"
      },
      "source": [
        "import random\n",
        "# torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
        "\n",
        "train_ds, val_ds = train_val_ds.split(\n",
        "    split_ratio=0.8, random_state=random.seed(1234))\n",
        "\n",
        "# 動作確認\n",
        "print('訓練データの数', len(train_ds))\n",
        "print('検証データの数', len(val_ds))\n",
        "print('1つ目の訓練データ', vars(train_ds[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gP0-stSbbpPZ"
      },
      "source": [
        "# ボキャブラリーを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljg0bDHxbpPZ"
      },
      "source": [
        "# It will take 2 minutes or so.\n",
        "# torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
        "\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "english_fasttext_vectors = Vectors(name='data/wiki-news-300d-1M.vec')\n",
        "\n",
        "\n",
        "# 単語ベクトルの中身を確認します\n",
        "print(\"1単語を表現する次元数：\", english_fasttext_vectors.dim)\n",
        "print(\"単語数：\", len(english_fasttext_vectors.itos))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15ZHOG6HbpPa"
      },
      "source": [
        "# ベクトル化したバージョンのボキャブラリーを作成します\n",
        "TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
        "\n",
        "# ボキャブラリーのベクトルを確認します\n",
        "print(TEXT.vocab.vectors.shape)  # 17916個の単語が300次元のベクトルで表現されている\n",
        "TEXT.vocab.vectors\n",
        "\n",
        "# ボキャブラリーの単語の順番を確認します\n",
        "TEXT.vocab.stoi\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AXt5Dn5bpPa"
      },
      "source": [
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "train_dl = torchtext.legacy.data.Iterator(train_ds, batch_size=24, train=True)\n",
        "\n",
        "val_dl = torchtext.legacy.data.Iterator(\n",
        "    val_ds, batch_size=24, train=False, sort=False)\n",
        "\n",
        "test_dl = torchtext.legacy.data.Iterator(\n",
        "    test_ds, batch_size=24, train=False, sort=False)\n",
        "\n",
        "\n",
        "# 動作確認 検証データのデータセットで確認\n",
        "batch = next(iter(val_dl))\n",
        "print(batch.Text)\n",
        "print(batch.Label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y09IKmDUbpPa"
      },
      "source": [
        "このようにDataLoaderは単語のidを格納しているので、分散表現はディープラーニングモデル側でidに応じて取得してあげる必要があります。\n",
        "\n",
        "ここまでの内容をフォルダ「utils」のdataloader.pyに別途保存しておき、次節からはこちらから読み込むようにします"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8M03hZFzbpPb"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvFAVl3rvlxK"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. "
      ]
    }
  ]
}