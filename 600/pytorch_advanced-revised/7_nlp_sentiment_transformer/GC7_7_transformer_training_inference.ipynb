{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "GC7-7_transformer_training_inference.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/600/pytorch_advanced-revised/7_nlp_sentiment_transformer/GC7_7_transformer_training_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_esvziJvySN"
      },
      "source": [
        "# 7.7 Transformerの学習・推論、判定根拠の可視化を実装\n",
        "\n",
        "- 本ファイルでは、ここまでで作成したTransformerモデルとIMDbのDataLoaderを使用してクラス分類を学習させます。テストデータで推論をし、さらに判断根拠となるAttentionを可視化します"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiS5iFkAvySQ"
      },
      "source": [
        "# 7.7 学習目標\n",
        "\n",
        "1.\tTransformerの学習を実装できるようになる\n",
        "2.\tTransformerの判定時のAttention可視化を実装できるようになる\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z2iYK_ox7s-"
      },
      "source": [
        "---\n",
        "\n",
        "# Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p12QTDHEx7tH"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "usdzzcLbx7tI"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/\n",
        "!echo \"Move to the working directory.\"\n",
        "%cd 202107_Tool-A/Work600/\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDc3dnE_x7tJ"
      },
      "source": [
        "---\n",
        "# 共通準備\n",
        "\n",
        "\"pytorch_advanced\" folder should be ready before you come here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cED0rxM7x7tJ"
      },
      "source": [
        "# Skip this if you have already issued git in advance. \n",
        "# If you come here by way of 600-PyTorchADL.ipynb, \n",
        "# you should skip the git command (as you have already issued in 600).  \n",
        "# If you run git when pytorch_advanced already exists, git tells the error and clone won't be made.\n",
        "\n",
        "#!git clone https://github.com/YutaroOgawa/pytorch_advanced.git\n",
        "\n",
        "import os\n",
        "if os.path.exists(\"/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced\"):\n",
        "    print(\"OK. Alreadly git cloned. You can go.\")\n",
        "else:\n",
        "    print(\"You'd better go back to the first 600-PyTorchADL.ipynb\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRia6DwFx7tJ"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGd5BWZZx7tJ"
      },
      "source": [
        "%cd \"pytorch_advanced\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWF4QMix7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YelRffxx7tK"
      },
      "source": [
        "%cd \"7_nlp_sentiment_transformer\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRZz7hVLx7tK"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxprtbsK4RJw"
      },
      "source": [
        "---\n",
        "# Extraction of aclImdb (GC7-0)\n",
        "\n",
        "The working directory is at /content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/ .\n",
        "\n",
        "aclImdb will be prepared at /root/data/aclImdb and it will be accessible through \n",
        "/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/data/aclImdb ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDD4Tx9b2htz"
      },
      "source": [
        "# local drive on GC is very, very fast.\n",
        "!mkdir -p /root/data/\n",
        "%cd /root/data/\n",
        "!tar xfz '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/data/aclImdb_v1.tar.gz'\n",
        "\n",
        "# make a symbolic link at the working directory on google drive.\n",
        "%cd '/content/drive/My Drive/202107_Tool-A/Work600/pytorch_advanced/7_nlp_sentiment_transformer/'\n",
        "!rm -f data/aclImdb\n",
        "!ln -s /root/data/aclImdb data/aclImdb\n",
        "\n",
        "!ls -ld data/aclImdb/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jxOQXwjwVAj"
      },
      "source": [
        "# utils/dataloader.py\n",
        "\n",
        "utils/dataloader.py should be updated on 7-6_Transformer.ipynb ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0RtN3GGwpIh"
      },
      "source": [
        "!head -5 utils/dataloader.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WylFzPmVvySQ"
      },
      "source": [
        "---\n",
        "# 7.7節\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ksmqfzlvySR"
      },
      "source": [
        "# パッケージのimport\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchtext\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRlRizALvySR"
      },
      "source": [
        "# 乱数のシードを設定\n",
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAHopbxCvySR"
      },
      "source": [
        "# DatasetとDataLoaderを作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_wfNeKVvySR"
      },
      "source": [
        "# it takes about one minute.\n",
        "\n",
        "from utils.dataloader import get_IMDb_DataLoaders_and_TEXT\n",
        "\n",
        "# 読み込み\n",
        "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
        "    max_length=256, batch_size=64)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": train_dl, \"val\": val_dl}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW6M9RARvySS"
      },
      "source": [
        "# ネットワークモデルの作成"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZFL4HccvySS"
      },
      "source": [
        "from utils.transformer import TransformerClassification\n",
        "\n",
        "# モデル構築\n",
        "net = TransformerClassification(\n",
        "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)\n",
        "\n",
        "# ネットワークの初期化を定義\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Linear') != -1:\n",
        "        # Liner層の初期化\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "# TransformerBlockモジュールを初期化実行\n",
        "net.net3_1.apply(weights_init)\n",
        "net.net3_2.apply(weights_init)\n",
        "\n",
        "\n",
        "print('ネットワーク設定完了')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS98VYkyvyST"
      },
      "source": [
        "# 損失関数と最適化手法を定義"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQl95ZWHvyST"
      },
      "source": [
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算\n",
        "\n",
        "# 最適化手法の設定\n",
        "learning_rate = 2e-5\n",
        "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yADrGNH-vyST"
      },
      "source": [
        "# 学習・検証を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGzAWnX8vyST"
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書オブジェクト\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # mask作成\n",
        "                    input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "                    input_mask = (inputs != input_pad)\n",
        "\n",
        "                    # Transformerに入力\n",
        "                    outputs, _, _ = net(inputs, input_mask)\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                    # 結果の計算\n",
        "                    epoch_loss += loss.item() * inputs.size(0)  # lossの合計を更新\n",
        "                    # 正解数の合計を更新\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-UCWPebvySU"
      },
      "source": [
        "# 学習・検証を実行。 (it takes 5 minutes or so on GC with GPU)\n",
        "num_epochs = 10\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQdV_oQVvySU"
      },
      "source": [
        "# テストデータでの正解率を求める"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7U7Yg5MvySV"
      },
      "source": [
        "# device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)\n",
        "\n",
        "epoch_corrects = 0  # epochの正解数\n",
        "\n",
        "for batch in (test_dl):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    \n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # mask作成\n",
        "        input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "        input_mask = (inputs != input_pad)\n",
        "\n",
        "        # Transformerに入力\n",
        "        outputs, _, _ = net_trained(inputs, input_mask)\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "        # 結果の計算\n",
        "        # 正解数の合計を更新\n",
        "        epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(test_dl.dataset),epoch_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ozgt4aawvySV"
      },
      "source": [
        "# Attentionの可視化で判定根拠を探る\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAwpJnczvySV"
      },
      "source": [
        "# HTMLを作成する関数を実装\n",
        "\n",
        "\n",
        "def highlight(word, attn):\n",
        "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
        "\n",
        "    html_color = '#%02X%02X%02X' % (\n",
        "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
        "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
        "\n",
        "\n",
        "def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2, TEXT):\n",
        "    \"HTMLデータを作成する\"\n",
        "\n",
        "    # indexの結果を抽出\n",
        "    sentence = batch.Text[0][index]  # 文章\n",
        "    label = batch.Label[index]  # ラベル\n",
        "    pred = preds[index]  # 予測\n",
        "\n",
        "    # indexのAttentionを抽出と規格化\n",
        "    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens1 /= attens1.max()\n",
        "\n",
        "    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n",
        "    attens2 /= attens2.max()\n",
        "\n",
        "    # ラベルと予測結果を文字に置き換え\n",
        "    if label == 0:\n",
        "        label_str = \"Negative\"\n",
        "    else:\n",
        "        label_str = \"Positive\"\n",
        "\n",
        "    if pred == 0:\n",
        "        pred_str = \"Negative\"\n",
        "    else:\n",
        "        pred_str = \"Positive\"\n",
        "\n",
        "    # 表示用のHTMLを作成する\n",
        "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
        "\n",
        "    # 1段目のAttention\n",
        "    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens1):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    # 2段目のAttention\n",
        "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
        "    for word, attn in zip(sentence, attens2):\n",
        "        html += highlight(TEXT.vocab.itos[word], attn)\n",
        "\n",
        "    html += \"<br><br>\"\n",
        "\n",
        "    return html\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMZhujI9vySW"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "# Transformerで処理\n",
        "\n",
        "# ミニバッチの用意\n",
        "batch = next(iter(test_dl))\n",
        "\n",
        "# GPUが使えるならGPUにデータを送る\n",
        "inputs = batch.Text[0].to(device)  # 文章\n",
        "labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "# mask作成\n",
        "input_pad = 1  # 単語のIDにおいて、'<pad>': 1 なので\n",
        "input_mask = (inputs != input_pad)\n",
        "\n",
        "# Transformerに入力\n",
        "outputs, normlized_weights_1, normlized_weights_2 = net_trained(\n",
        "    inputs, input_mask)\n",
        "_, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "\n",
        "index = 3  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, normlized_weights_1,\n",
        "                      normlized_weights_2, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDkRAErRvySW"
      },
      "source": [
        "index = 61  # 出力させたいデータ\n",
        "html_output = mk_html(index, batch, preds, normlized_weights_1,\n",
        "                      normlized_weights_2, TEXT)  # HTML作成\n",
        "HTML(html_output)  # HTML形式で出力\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaAD_POtvySX"
      },
      "source": [
        "以上"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQd8MRKiy3qX"
      },
      "source": [
        "---\n",
        "Revised by KAMEDA, Yoshinari at University of Tsukuba for lecture purpose.  \n",
        "Original: https://github.com/YutaroOgawa/pytorch_advanced\n",
        "\n",
        "2021/08/03. "
      ]
    }
  ]
}