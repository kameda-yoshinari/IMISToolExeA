{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "440-DLfS-ch04.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/N1L9VJYh4aTjY8d4tXKA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2021/blob/main/400/440_DLfS_ch04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunZb1eqUdur"
      },
      "source": [
        "# ゼロから作るDeep Learning ch04"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoB0f6lbUZP1"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgYXd3C3UhdE"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!echo \"Make a working folder and move to there.\"\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%cd 202107_Tool-A/Work400/deep-learning-from-scratch-master\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk85QIoEUjR-"
      },
      "source": [
        "%cd ch04"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMC7qkSCUlvv"
      },
      "source": [
        "---\n",
        "# 4.3.2 gradient_1d.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGx16RGVVECy"
      },
      "source": [
        "!python gradient_1d.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTacF15kVSS_"
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "\n",
        "def numerical_diff(f, x):\n",
        "    h = 1e-4 # 0.0001\n",
        "    return (f(x+h) - f(x-h)) / (2*h)\n",
        "\n",
        "\n",
        "def function_1(x):\n",
        "    return 0.01*x**2 + 0.1*x \n",
        "\n",
        "def tangent_line(f, x):\n",
        "    d = numerical_diff(f, x)\n",
        "    print(d)\n",
        "    y = f(x) - d*x\n",
        "    return lambda t: d*t + y\n",
        "     \n",
        "x = np.arange(0.0, 20.0, 0.1)\n",
        "y = function_1(x)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "\n",
        "tf = tangent_line(function_1, 5)\n",
        "y2 = tf(x)\n",
        "\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, y2)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_oUTRVaFb"
      },
      "source": [
        "# 4.4 gradient_2d.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlFjr1_VhqV"
      },
      "source": [
        "!python gradient_2d.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVw2WIMwVlkA"
      },
      "source": [
        "# coding: utf-8\n",
        "# cf.http://d.hatena.ne.jp/white_wheels/20100327/p3\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "\n",
        "def _numerical_gradient_no_batch(f, x):\n",
        "    h = 1e-4  # 0.0001\n",
        "    grad = np.zeros_like(x)\n",
        "    \n",
        "    for idx in range(x.size):\n",
        "        tmp_val = x[idx]\n",
        "        x[idx] = float(tmp_val) + h\n",
        "        fxh1 = f(x)  # f(x+h)\n",
        "        \n",
        "        x[idx] = tmp_val - h \n",
        "        fxh2 = f(x)  # f(x-h)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "        \n",
        "        x[idx] = tmp_val  # 値を元に戻す\n",
        "        \n",
        "    return grad\n",
        "\n",
        "\n",
        "def numerical_gradient(f, X):\n",
        "    if X.ndim == 1:\n",
        "        return _numerical_gradient_no_batch(f, X)\n",
        "    else:\n",
        "        grad = np.zeros_like(X)\n",
        "        \n",
        "        for idx, x in enumerate(X):\n",
        "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
        "        \n",
        "        return grad\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    if x.ndim == 1:\n",
        "        return np.sum(x**2)\n",
        "    else:\n",
        "        return np.sum(x**2, axis=1)\n",
        "\n",
        "\n",
        "def tangent_line(f, x):\n",
        "    d = numerical_gradient(f, x)\n",
        "    print(d)\n",
        "    y = f(x) - d*x\n",
        "    return lambda t: d*t + y\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x0 = np.arange(-2, 2.5, 0.25)\n",
        "    x1 = np.arange(-2, 2.5, 0.25)\n",
        "    X, Y = np.meshgrid(x0, x1)\n",
        "    \n",
        "    X = X.flatten()\n",
        "    Y = Y.flatten()\n",
        "\n",
        "    grad = numerical_gradient(function_2, np.array([X, Y]).T).T\n",
        "\n",
        "    plt.figure()\n",
        "    plt.quiver(X, Y, -grad[0], -grad[1],  angles=\"xy\",color=\"#666666\")\n",
        "    plt.xlim([-2, 2])\n",
        "    plt.ylim([-2, 2])\n",
        "    plt.xlabel('x0')\n",
        "    plt.ylabel('x1')\n",
        "    plt.grid()\n",
        "    plt.draw()\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXIs0I1FmPfT"
      },
      "source": [
        "# 4.4.1 gradient_method.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_TS7geoSAy"
      },
      "source": [
        "!python gradient_method.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ObxlLojk0pJ"
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pylab as plt\n",
        "from gradient_2d import numerical_gradient\n",
        "\n",
        "\n",
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "    x = init_x\n",
        "    x_history = []\n",
        "\n",
        "    for i in range(step_num):\n",
        "        x_history.append( x.copy() )\n",
        "\n",
        "        grad = numerical_gradient(f, x)\n",
        "        x -= lr * grad\n",
        "\n",
        "    return x, np.array(x_history)\n",
        "\n",
        "\n",
        "def function_2(x):\n",
        "    return x[0]**2 + x[1]**2\n",
        "\n",
        "init_x = np.array([-3.0, 4.0])    \n",
        "\n",
        "lr = 0.1\n",
        "step_num = 20\n",
        "x, x_history = gradient_descent(function_2, init_x, lr=lr, step_num=step_num)\n",
        "\n",
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHdrlN0oyHZ"
      },
      "source": [
        "# 4.4.2 gradient_simplenet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTe8MJxao1x1"
      },
      "source": [
        "!python gradient_simplenet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZRQPboLo7Ao"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "\n",
        "class simpleNet:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(2,3)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return np.dot(x, self.W)\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        z = self.predict(x)\n",
        "        y = softmax(z)\n",
        "        loss = cross_entropy_error(y, t)\n",
        "\n",
        "        return loss\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "t = np.array([0, 0, 1])\n",
        "\n",
        "net = simpleNet()\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "dW = numerical_gradient(f, net.W)\n",
        "\n",
        "print(dW)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XW-BWK0JpIKV"
      },
      "source": [
        "# 4.5.1 two_layer_net.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9Z1eeazpLiW"
      },
      "source": [
        "!python two_layer_net.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTUgtjXlpOsL"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "from common.functions import *\n",
        "from common.gradient import numerical_gradient\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "        # 重みの初期化\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "    def predict(self, x):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "    \n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        return y\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        \n",
        "        return cross_entropy_error(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x:入力データ, t:教師データ\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        W1, W2 = self.params['W1'], self.params['W2']\n",
        "        b1, b2 = self.params['b1'], self.params['b2']\n",
        "        grads = {}\n",
        "        \n",
        "        batch_num = x.shape[0]\n",
        "        \n",
        "        # forward\n",
        "        a1 = np.dot(x, W1) + b1\n",
        "        z1 = sigmoid(a1)\n",
        "        a2 = np.dot(z1, W2) + b2\n",
        "        y = softmax(a2)\n",
        "        \n",
        "        # backward\n",
        "        dy = (y - t) / batch_num\n",
        "        grads['W2'] = np.dot(z1.T, dy)\n",
        "        grads['b2'] = np.sum(dy, axis=0)\n",
        "        \n",
        "        dz1 = np.dot(dy, W2.T)\n",
        "        da1 = sigmoid_grad(a1) * dz1\n",
        "        grads['W1'] = np.dot(x.T, da1)\n",
        "        grads['b1'] = np.sum(da1, axis=0)\n",
        "\n",
        "        return grads\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-vb0XEepfyx"
      },
      "source": [
        "# 4.5.2 train_neuralnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJGE0icupjtZ"
      },
      "source": [
        "!python train_neuralnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga34gNvXp5cX"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from two_layer_net import TwoLayerNet\n",
        "\n",
        "# データの読み込み\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000  # 繰り返しの回数を適宜設定する\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 勾配の計算\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
        "    grad = network.gradient(x_batch, t_batch)\n",
        "    \n",
        "    # パラメータの更新\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
        "\n",
        "# グラフの描画\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, label='train acc')\n",
        "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B_mM3VXWG1j"
      },
      "source": [
        "---\n",
        "Tools for intelligent interaction systems a  \n",
        "Master's and Docotal programs in intelligent and mechanical interaction systems, University of Tsukuba, Japan.  \n",
        "KAMEDA Yoshinari, SHIBUYA Takeshi  \n",
        "\n",
        "知能システムツール演習a  \n",
        "知能機能システム学位プログラム (筑波大学大学院)  \n",
        "担当：亀田能成，澁谷長史  \n",
        "\n",
        "2021/07/26.  \n"
      ]
    }
  ]
}