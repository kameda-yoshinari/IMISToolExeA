{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "460-DLfS-ch06.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kMC7qkSCUlvv"
      ],
      "authorship_tag": "ABX9TyMm9wG6hJ9H3wk1YAudKRpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2022/blob/main/400/460_DLfS_ch06.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunZb1eqUdur"
      },
      "source": [
        "# ゼロから作るDeep Learning ch06"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoB0f6lbUZP1"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgYXd3C3UhdE"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!echo \"Make a working folder and move to there.\"\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%cd IMIS_Tool-A/Work400/deep-learning-from-scratch-master\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk85QIoEUjR-"
      },
      "source": [
        "%cd ch06"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMC7qkSCUlvv"
      },
      "source": [
        "---\n",
        "# 6.1.7 optimizer_compare_naive.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV9YiId2t6JH"
      },
      "source": [
        "!python optimizer_compare_naive.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlnvBOqt9Tr"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import OrderedDict\n",
        "from common.optimizer import *\n",
        "\n",
        "\n",
        "def f(x, y):\n",
        "    return x**2 / 20.0 + y**2\n",
        "\n",
        "\n",
        "def df(x, y):\n",
        "    return x / 10.0, 2.0*y\n",
        "\n",
        "init_pos = (-7.0, 2.0)\n",
        "params = {}\n",
        "params['x'], params['y'] = init_pos[0], init_pos[1]\n",
        "grads = {}\n",
        "grads['x'], grads['y'] = 0, 0\n",
        "\n",
        "\n",
        "optimizers = OrderedDict()\n",
        "optimizers[\"SGD\"] = SGD(lr=0.95)\n",
        "optimizers[\"Momentum\"] = Momentum(lr=0.1)\n",
        "optimizers[\"AdaGrad\"] = AdaGrad(lr=1.5)\n",
        "optimizers[\"Adam\"] = Adam(lr=0.3)\n",
        "\n",
        "idx = 1\n",
        "\n",
        "for key in optimizers:\n",
        "    optimizer = optimizers[key]\n",
        "    x_history = []\n",
        "    y_history = []\n",
        "    params['x'], params['y'] = init_pos[0], init_pos[1]\n",
        "    \n",
        "    for i in range(30):\n",
        "        x_history.append(params['x'])\n",
        "        y_history.append(params['y'])\n",
        "        \n",
        "        grads['x'], grads['y'] = df(params['x'], params['y'])\n",
        "        optimizer.update(params, grads)\n",
        "    \n",
        "\n",
        "    x = np.arange(-10, 10, 0.01)\n",
        "    y = np.arange(-5, 5, 0.01)\n",
        "    \n",
        "    X, Y = np.meshgrid(x, y) \n",
        "    Z = f(X, Y)\n",
        "    \n",
        "    # for simple contour line  \n",
        "    mask = Z > 7\n",
        "    Z[mask] = 0\n",
        "    \n",
        "    # plot \n",
        "    plt.subplot(2, 2, idx)\n",
        "    idx += 1\n",
        "    plt.plot(x_history, y_history, 'o-', color=\"red\")\n",
        "    plt.contour(X, Y, Z)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.plot(0, 0, '+')\n",
        "    #colorbar()\n",
        "    #spring()\n",
        "    plt.title(key)\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"y\")\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_oUTRVaFb"
      },
      "source": [
        "# 6.1.8 optimizer_compare_mnist.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlFjr1_VhqV"
      },
      "source": [
        "!python optimizer_compare_mnist.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVw2WIMwVlkA"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import *\n",
        "\n",
        "\n",
        "# 0:MNISTデータの読み込み==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "\n",
        "# 1:実験の設定==========\n",
        "optimizers = {}\n",
        "optimizers['SGD'] = SGD()\n",
        "optimizers['Momentum'] = Momentum()\n",
        "optimizers['AdaGrad'] = AdaGrad()\n",
        "optimizers['Adam'] = Adam()\n",
        "#optimizers['RMSprop'] = RMSprop()\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "for key in optimizers.keys():\n",
        "    networks[key] = MultiLayerNet(\n",
        "        input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
        "        output_size=10)\n",
        "    train_loss[key] = []    \n",
        "\n",
        "\n",
        "# 2:訓練の開始==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    for key in optimizers.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizers[key].update(networks[key].params, grads)\n",
        "    \n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print( \"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
        "        for key in optimizers.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(key + \":\" + str(loss))\n",
        "\n",
        "\n",
        "# 3.グラフの描画==========\n",
        "markers = {\"SGD\": \"o\", \"Momentum\": \"x\", \"AdaGrad\": \"s\", \"Adam\": \"D\"}\n",
        "x = np.arange(max_iterations)\n",
        "for key in optimizers.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXIs0I1FmPfT"
      },
      "source": [
        "# \n",
        "6.2.2 weight_init_activation_histogram.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qf_TS7geoSAy"
      },
      "source": [
        "!python weight_init_activation_histogram.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ObxlLojk0pJ"
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def ReLU(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "    \n",
        "input_data = np.random.randn(1000, 100)  # 1000個のデータ\n",
        "node_num = 100  # 各隠れ層のノード（ニューロン）の数\n",
        "hidden_layer_size = 5  # 隠れ層が5層\n",
        "activations = {}  # ここにアクティベーションの結果を格納する\n",
        "\n",
        "x = input_data\n",
        "\n",
        "for i in range(hidden_layer_size):\n",
        "    if i != 0:\n",
        "        x = activations[i-1]\n",
        "\n",
        "    # 初期値の値をいろいろ変えて実験しよう！\n",
        "    w = np.random.randn(node_num, node_num) * 1\n",
        "    # w = np.random.randn(node_num, node_num) * 0.01\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)\n",
        "    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)\n",
        "\n",
        "\n",
        "    a = np.dot(x, w)\n",
        "\n",
        "\n",
        "    # 活性化関数の種類も変えて実験しよう！\n",
        "    z = sigmoid(a)\n",
        "    # z = ReLU(a)\n",
        "    # z = tanh(a)\n",
        "\n",
        "    activations[i] = z\n",
        "\n",
        "# ヒストグラムを描画\n",
        "for i, a in activations.items():\n",
        "    plt.subplot(1, len(activations), i+1)\n",
        "    plt.title(str(i+1) + \"-layer\")\n",
        "    if i != 0: plt.yticks([], [])\n",
        "    # plt.xlim(0.1, 1)\n",
        "    # plt.ylim(0, 7000)\n",
        "    plt.hist(a.flatten(), 30, range=(0,1))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHdrlN0oyHZ"
      },
      "source": [
        "# 6.2.4 weight_init_compare.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTe8MJxao1x1"
      },
      "source": [
        "!python weight_init_compare.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZRQPboLo7Ao"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.util import smooth_curve\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "\n",
        "# 0:MNISTデータの読み込み==========\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 128\n",
        "max_iterations = 2000\n",
        "\n",
        "\n",
        "# 1:実験の設定==========\n",
        "weight_init_types = {'std=0.01': 0.01, 'Xavier': 'sigmoid', 'He': 'relu'}\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "networks = {}\n",
        "train_loss = {}\n",
        "for key, weight_type in weight_init_types.items():\n",
        "    networks[key] = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100],\n",
        "                                  output_size=10, weight_init_std=weight_type)\n",
        "    train_loss[key] = []\n",
        "\n",
        "\n",
        "# 2:訓練の開始==========\n",
        "for i in range(max_iterations):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    for key in weight_init_types.keys():\n",
        "        grads = networks[key].gradient(x_batch, t_batch)\n",
        "        optimizer.update(networks[key].params, grads)\n",
        "    \n",
        "        loss = networks[key].loss(x_batch, t_batch)\n",
        "        train_loss[key].append(loss)\n",
        "    \n",
        "    if i % 100 == 0:\n",
        "        print(\"===========\" + \"iteration:\" + str(i) + \"===========\")\n",
        "        for key in weight_init_types.keys():\n",
        "            loss = networks[key].loss(x_batch, t_batch)\n",
        "            print(key + \":\" + str(loss))\n",
        "\n",
        "\n",
        "# 3.グラフの描画==========\n",
        "markers = {'std=0.01': 'o', 'Xavier': 's', 'He': 'D'}\n",
        "x = np.arange(max_iterations)\n",
        "for key in weight_init_types.keys():\n",
        "    plt.plot(x, smooth_curve(train_loss[key]), marker=markers[key], markevery=100, label=key)\n",
        "plt.xlabel(\"iterations\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.ylim(0, 2.5)\n",
        "plt.legend()\n",
        "plt.show()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnLlFQtEsjFm"
      },
      "source": [
        "# 6.3.2 batch_norm_test.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3WgE1CLspIt"
      },
      "source": [
        "!python batch_norm_test.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_XWJV_UsszL"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.optimizer import SGD, Adam\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 学習データを削減\n",
        "x_train = x_train[:1000]\n",
        "t_train = t_train[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "def __train(weight_init_std):\n",
        "    bn_network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10, \n",
        "                                    weight_init_std=weight_init_std, use_batchnorm=True)\n",
        "    network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100], output_size=10,\n",
        "                                weight_init_std=weight_init_std)\n",
        "    optimizer = SGD(lr=learning_rate)\n",
        "    \n",
        "    train_acc_list = []\n",
        "    bn_train_acc_list = []\n",
        "    \n",
        "    iter_per_epoch = max(train_size / batch_size, 1)\n",
        "    epoch_cnt = 0\n",
        "    \n",
        "    for i in range(1000000000):\n",
        "        batch_mask = np.random.choice(train_size, batch_size)\n",
        "        x_batch = x_train[batch_mask]\n",
        "        t_batch = t_train[batch_mask]\n",
        "    \n",
        "        for _network in (bn_network, network):\n",
        "            grads = _network.gradient(x_batch, t_batch)\n",
        "            optimizer.update(_network.params, grads)\n",
        "    \n",
        "        if i % iter_per_epoch == 0:\n",
        "            train_acc = network.accuracy(x_train, t_train)\n",
        "            bn_train_acc = bn_network.accuracy(x_train, t_train)\n",
        "            train_acc_list.append(train_acc)\n",
        "            bn_train_acc_list.append(bn_train_acc)\n",
        "    \n",
        "            print(\"epoch:\" + str(epoch_cnt) + \" | \" + str(train_acc) + \" - \" + str(bn_train_acc))\n",
        "    \n",
        "            epoch_cnt += 1\n",
        "            if epoch_cnt >= max_epochs:\n",
        "                break\n",
        "                \n",
        "    return train_acc_list, bn_train_acc_list\n",
        "\n",
        "\n",
        "# 3.グラフの描画==========\n",
        "weight_scale_list = np.logspace(0, -4, num=16)\n",
        "x = np.arange(max_epochs)\n",
        "\n",
        "for i, w in enumerate(weight_scale_list):\n",
        "    print( \"============== \" + str(i+1) + \"/16\" + \" ==============\")\n",
        "    train_acc_list, bn_train_acc_list = __train(w)\n",
        "    \n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.title(\"W:\" + str(w))\n",
        "    if i == 15:\n",
        "        plt.plot(x, bn_train_acc_list, label='Batch Normalization', markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle = \"--\", label='Normal(without BatchNorm)', markevery=2)\n",
        "    else:\n",
        "        plt.plot(x, bn_train_acc_list, markevery=2)\n",
        "        plt.plot(x, train_acc_list, linestyle=\"--\", markevery=2)\n",
        "\n",
        "    plt.ylim(0, 1.0)\n",
        "    if i % 4:\n",
        "        plt.yticks([])\n",
        "    else:\n",
        "        plt.ylabel(\"accuracy\")\n",
        "    if i < 12:\n",
        "        plt.xticks([])\n",
        "    else:\n",
        "        plt.xlabel(\"epochs\")\n",
        "    plt.legend(loc='lower right')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beCVmhOR8ufx"
      },
      "source": [
        "# 6.4.1 overfit_weight_decay.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHWc3pR38z-W"
      },
      "source": [
        "!python overfit_weight_decay.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uepYSPPg83Hr"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.optimizer import SGD\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 過学習を再現するために、学習データを削減\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# weight decay（荷重減衰）の設定 =======================\n",
        "#weight_decay_lambda = 0 # weight decayを使用しない場合\n",
        "weight_decay_lambda = 0.1\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
        "                        weight_decay_lambda=weight_decay_lambda)\n",
        "optimizer = SGD(lr=0.01)\n",
        "\n",
        "max_epochs = 201\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "epoch_cnt = 0\n",
        "\n",
        "for i in range(1000000000):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "\n",
        "    grads = network.gradient(x_batch, t_batch)\n",
        "    optimizer.update(network.params, grads)\n",
        "\n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "\n",
        "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
        "\n",
        "        epoch_cnt += 1\n",
        "        if epoch_cnt >= max_epochs:\n",
        "            break\n",
        "\n",
        "\n",
        "# 3.グラフの描画==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5r_KBqWu9o5U"
      },
      "source": [
        "# 6.4.3 overfit_dropout.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CvDJHGb9tk_"
      },
      "source": [
        "!python overfit_dropout.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcU4xj2K9xOb"
      },
      "source": [
        "# coding: utf-8\n",
        "import os\n",
        "import sys\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 過学習を再現するために、学習データを削減\n",
        "x_train = x_train[:300]\n",
        "t_train = t_train[:300]\n",
        "\n",
        "# Dropuoutの有無、割り合いの設定 ========================\n",
        "use_dropout = True  # Dropoutなしのときの場合はFalseに\n",
        "dropout_ratio = 0.2\n",
        "# ====================================================\n",
        "\n",
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=301, mini_batch_size=100,\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
        "trainer.train()\n",
        "\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
        "\n",
        "# グラフの描画==========\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(len(train_acc_list))\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LyGK_lrFKx-"
      },
      "source": [
        "# 6.5.3 hyperparameter_optimization.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylhUpk-iFQtC"
      },
      "source": [
        "!python hyperparameter_optimization.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds3nrMeyFSgQ"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.multi_layer_net import MultiLayerNet\n",
        "from common.util import shuffle_dataset\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
        "\n",
        "# 高速化のため訓練データの削減\n",
        "x_train = x_train[:500]\n",
        "t_train = t_train[:500]\n",
        "\n",
        "# 検証データの分離\n",
        "validation_rate = 0.20\n",
        "validation_num = int(x_train.shape[0] * validation_rate)\n",
        "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
        "x_val = x_train[:validation_num]\n",
        "t_val = t_train[:validation_num]\n",
        "x_train = x_train[validation_num:]\n",
        "t_train = t_train[validation_num:]\n",
        "\n",
        "\n",
        "def __train(lr, weight_decay, epocs=50):\n",
        "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
        "                            output_size=10, weight_decay_lambda=weight_decay)\n",
        "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
        "                      epochs=epocs, mini_batch_size=100,\n",
        "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
        "    trainer.train()\n",
        "\n",
        "    return trainer.test_acc_list, trainer.train_acc_list\n",
        "\n",
        "\n",
        "# ハイパーパラメータのランダム探索======================================\n",
        "optimization_trial = 100\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "for _ in range(optimization_trial):\n",
        "    # 探索したハイパーパラメータの範囲を指定===============\n",
        "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
        "    lr = 10 ** np.random.uniform(-6, -2)\n",
        "    # ================================================\n",
        "\n",
        "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
        "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
        "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
        "    results_val[key] = val_acc_list\n",
        "    results_train[key] = train_acc_list\n",
        "\n",
        "# グラフの描画========================================================\n",
        "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
        "graph_draw_num = 20\n",
        "col_num = 5\n",
        "row_num = int(np.ceil(graph_draw_num / col_num))\n",
        "i = 0\n",
        "\n",
        "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
        "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
        "\n",
        "    plt.subplot(row_num, col_num, i+1)\n",
        "    plt.title(\"Best-\" + str(i+1))\n",
        "    plt.ylim(0.0, 1.0)\n",
        "    if i % 5: plt.yticks([])\n",
        "    plt.xticks([])\n",
        "    x = np.arange(len(val_acc_list))\n",
        "    plt.plot(x, val_acc_list)\n",
        "    plt.plot(x, results_train[key], \"--\")\n",
        "    i += 1\n",
        "\n",
        "    if i >= graph_draw_num:\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B_mM3VXWG1j"
      },
      "source": [
        "---\n",
        "Tools and Practices for Intelligent Interaction Systems A  \n",
        "Master's and Docotal programs in intelligent and mechanical interaction systems, University of Tsukuba, Japan.  \n",
        "KAMEDA Yoshinari, SHIBUYA Takeshi  \n",
        "\n",
        "知能システムツール演習a  \n",
        "知能機能システム学位プログラム (筑波大学大学院)  \n",
        "担当：亀田能成，澁谷長史  \n",
        "\n",
        "2022/08/01.  \n",
        "2021/07/26.  \n"
      ]
    }
  ]
}