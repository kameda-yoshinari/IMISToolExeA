{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "480-DLfS-ch08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1yanK3ihZ2QPmz5nwcg2s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA2022/blob/main/400/480_DLfS_ch08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IunZb1eqUdur"
      },
      "source": [
        "# ゼロから作るDeep Learning ch08"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoB0f6lbUZP1"
      },
      "source": [
        "!echo \"Change to the JST notation.\"\n",
        "!rm /etc/localtime\n",
        "!ln -s /usr/share/zoneinfo/Japan /etc/localtime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgYXd3C3UhdE"
      },
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "!echo \"Make a working folder and move to there.\"\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%cd IMIS_Tool-A/Work400/deep-learning-from-scratch-master\n",
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk85QIoEUjR-"
      },
      "source": [
        "%cd ch08"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMC7qkSCUlvv"
      },
      "source": [
        "---\n",
        "# 8.1 deep_convnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV9YiId2t6JH"
      },
      "source": [
        "!python deep_convnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knlnvBOqt9Tr"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import pickle\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "\n",
        "\n",
        "class DeepConvNet:\n",
        "    \"\"\"認識率99%以上の高精度なConvNet\n",
        "\n",
        "    ネットワーク構成は下記の通り\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        conv - relu - conv- relu - pool -\n",
        "        affine - relu - dropout - affine - dropout - softmax\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28),\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=50, output_size=10):\n",
        "        # 重みの初期化===========\n",
        "        # 各層のニューロンひとつあたりが、前層のニューロンといくつのつながりがあるか（TODO:自動で計算する）\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\n",
        "        weight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLUを使う場合に推奨される初期値\n",
        "        \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\n",
        "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\n",
        "            pre_channel_num = conv_param['filter_num']\n",
        "        self.params['W7'] = weight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\n",
        "        self.params['b7'] = np.zeros(hidden_size)\n",
        "        self.params['W8'] = weight_init_scales[7] * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b8'] = np.zeros(output_size)\n",
        "\n",
        "        # レイヤの生成===========\n",
        "        self.layers = []\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\n",
        "        self.layers.append(Relu())\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\n",
        "        self.layers.append(Dropout(0.5))\n",
        "        \n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x, train_flg=False):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, Dropout):\n",
        "                x = layer.forward(x, train_flg)\n",
        "            else:\n",
        "                x = layer.forward(x)\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x, train_flg=True)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "\n",
        "        acc = 0.0\n",
        "\n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = np.argmax(y, axis=1)\n",
        "            acc += np.sum(y == tt)\n",
        "\n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 設定\n",
        "        grads = {}\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, layer_idx in enumerate((0, 2, 5, 7, 10, 12, 15, 18)):\n",
        "            self.layers[layer_idx].W = self.params['W' + str(i+1)]\n",
        "            self.layers[layer_idx].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paV_oUTRVaFb"
      },
      "source": [
        "# 8.1 train_deepnet.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnlFjr1_VhqV"
      },
      "source": [
        "!python train_deepnet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bVw2WIMwVlkA"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from deep_convnet import DeepConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()  \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=20, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# パラメータの保存\n",
        "network.save_params(\"deep_convnet_params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-wQAypFHZ35"
      },
      "source": [
        "# 8.3.4 half_float_network.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDb1ZqGsHea9"
      },
      "source": [
        "!python half_float_network.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wpm-ta5HihJ"
      },
      "source": [
        "# coding: utf-8\n",
        "import sys, os\n",
        "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from deep_convnet import DeepConvNet\n",
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "network = DeepConvNet()\n",
        "network.load_params(\"deep_convnet_params.pkl\")\n",
        "\n",
        "sampled = 10000 # 高速化のため\n",
        "x_test = x_test[:sampled]\n",
        "t_test = t_test[:sampled]\n",
        "\n",
        "print(\"caluculate accuracy (float64) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n",
        "\n",
        "# float16に型変換\n",
        "x_test = x_test.astype(np.float16)\n",
        "for param in network.params.values():\n",
        "    param[...] = param.astype(np.float16)\n",
        "\n",
        "print(\"caluculate accuracy (float16) ... \")\n",
        "print(network.accuracy(x_test, t_test))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6B_mM3VXWG1j"
      },
      "source": [
        "---\n",
        "Tools and Practices for Intelligent Interaction Systems A  \n",
        "Master's and Docotal programs in intelligent and mechanical interaction systems, University of Tsukuba, Japan.  \n",
        "KAMEDA Yoshinari, SHIBUYA Takeshi  \n",
        "\n",
        "知能システムツール演習a  \n",
        "知能機能システム学位プログラム (筑波大学大学院)  \n",
        "担当：亀田能成，澁谷長史  \n",
        "\n",
        "2022/08/01.  \n",
        "2021/07/26.  \n"
      ]
    }
  ]
}