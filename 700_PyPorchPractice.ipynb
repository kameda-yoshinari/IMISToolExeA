{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kameda-yoshinari/IMISToolExeA/blob/main/700_PyPorchPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. PyTorch Practice of Image Classification by Computer Vision"
      ],
      "metadata": {
        "id": "Lk2ARlSiGHfU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preface  \n",
        "\n",
        "**Let's actually build a new image classifier.**  \n",
        "\n",
        "Before you proceed beyond this point, think for a while how you will go if you are actually asked it?  \n",
        "\n",
        "\n",
        "The original of this lesson is taken from [\"Transfer learning tutorial\"](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) given by PyTorch tutorial.  \n",
        "As you see here, you can jump start by finding the right tutorial.\n",
        "\n",
        "Since learning from scratch is time-consuming, we use a method called Transfer Learning to shorten the learning time.  \n",
        "\n",
        "(Note that we are going to make a image classifier that can classify the images into the classes defined here.)\n"
      ],
      "metadata": {
        "id": "GyxUOAKSJAvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation as Google Colab\n",
        "\n",
        "All the files will be placed on your Google Drive."
      ],
      "metadata": {
        "id": "39tSAxSdLMEc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Start mounting your Google Drive.\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7n0HruAyLQz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"Make a working folder and move to there.\"\n",
        "%cd /content/drive/My\\ Drive/\n",
        "%mkdir -p IMIS_Tool-A/Work700\n",
        "%cd       IMIS_Tool-A/Work700\n",
        "!ls"
      ],
      "metadata": {
        "id": "AJW6O4wYLYQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G26j1SEuaT_Q"
      },
      "outputs": [],
      "source": [
        "!echo \"For tips on running notebooks in Google Colab, see https://pytorch.org/tutorials/beginner/colab \"\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Transfer Learning for Computer Vision Tutorial\n",
        "**Author**: [Sasank Chilamkurthy](https://chsasank.github.io)  \n",
        "**Author(minor changes)**: KAMEDA Yoshinari"
      ],
      "metadata": {
        "id": "o5JalsX7Iqpt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0mptpReaT_V"
      },
      "source": [
        "## Instroduction\n",
        "\n",
        "In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning.  \n",
        "You can read more about the transfer learning at [cs231n notes](https://cs231n.github.io/transfer-learning/)_\n",
        "\n",
        "Quoting these notes,\n",
        "\n",
        "    In practice, very few people train an entire Convolutional Network\n",
        "    from scratch (with random initialization), because it is relatively\n",
        "    rare to have a dataset of sufficient size. Instead, it is common to\n",
        "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
        "    contains 1.2 million images with 1000 categories), and then use the\n",
        "    ConvNet either as an initialization or a fixed feature extractor for\n",
        "    the task of interest.\n",
        "\n",
        "These two major transfer learning scenarios look as follows:\n",
        "\n",
        "-  **Finetuning the ConvNet (model_ft)** : Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n",
        "-  **ConvNet as fixed feature extractor (model_conv)**: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ImageNet\n",
        "\n",
        "You should learn the imagenet and understand how big the size of the image dataset is to build a good classifier.\n",
        "\n",
        "* ImageNet (official)  \n",
        "   https://www.image-net.org/download.php  \n",
        "* 微妙な日本語解説  \n",
        "   https://cvml-expertguide.net/terms/dataset/image-dataset/imagenet/"
      ],
      "metadata": {
        "id": "aEAdrYr7IJjJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation\n",
        "\n",
        "Loading necessary libraries (as PyTorch)."
      ],
      "metadata": {
        "id": "McAEqXYQKVg8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYSsV6XqaT_X"
      },
      "outputs": [],
      "source": [
        "# License: BSD\n",
        "# Author: Sasank Chilamkurthy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # interactive mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq1kfGhqaT_Y"
      },
      "source": [
        "## Load Data (from the Internet)\n",
        "\n",
        "We will use torchvision and torch.utils.data packages for loading the data.\n",
        "\n",
        "The problem we're going to solve today is to train a model to classify **ants** and **bees**. We have about 120 training images each for ants and bees.\n",
        "There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch.\n",
        "Since we are using transfer learning, we should be able to generalize reasonably well.\n",
        "\n",
        "This dataset is a very small subset of imagenet.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data and unzip by shell command\n",
        "\n",
        "%%shell\n",
        "rm -rf data/\n",
        "wget -P ./data https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
        "unzip ./data/hymenoptera_data.zip -d ./data"
      ],
      "metadata": {
        "id": "atWKFGnVa1WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that you do not need to run this process for the second time as the dataset is on your drive.  \n",
        "\n",
        "You should check the data very well (this time one data file is actually corrupted)."
      ],
      "metadata": {
        "id": "-waBclFYMkki"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset tuning for use here\n",
        "\n",
        "This process is sometimes critical for the recognition performance.\n",
        "\n",
        "* Augmentation and normalization for training\n",
        "* Normalization for validation\n",
        "\n",
        "(Why augmentation won't be applied to validation?)  \n",
        "(What is the normalized image size?)  \n",
        "(What happens if the image is larger or smaller? Not square?)  \n",
        "(What will be done for the color balance?)  \n",
        "(What is the magic number of [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]?)  \n",
        "(What are the \"augmentation\"?)  \n",
        "\n",
        "Check the actual commands to do these."
      ],
      "metadata": {
        "id": "Ra2RDxFxNcEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./data/hymenoptera_data/train\n",
        "!ls ./data/hymenoptera_data/val"
      ],
      "metadata": {
        "id": "ibPUh1xel4rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8s9Hr2DaT_Z"
      },
      "outputs": [],
      "source": [
        "# Dataset definition for use\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "# Just normalization for validation\n",
        "\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_dir = 'data/hymenoptera_data'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loader  \n",
        "\n",
        "You should obtain the Google Colab runtime with GPU.  \n",
        "\n",
        "(What is the batch_size?)  \n",
        "(What is controlled by \"shuffle=True\"?)  \n",
        "(What is num_workers?)  \n",
        "(What is the role of \"device\"?)  \n",
        "  \n",
        "(What the warning is saying? ... See the lesson of 200)\n"
      ],
      "metadata": {
        "id": "B_TPPW4OP1DG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data loader for train and val\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4, shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "eF49yY0XPVqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check what you have done\n",
        "\n",
        "You can see some variables by just typing the valiable name."
      ],
      "metadata": {
        "id": "rfHEPu7-Recw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment one of these\n",
        "#print(\"abc\")\n",
        "#data_transforms\n",
        "#data_transforms['train']\n",
        "#data_transforms['train'].transforms\n",
        "#data_transforms['val']\n",
        "#data_dir\n",
        "#image_datasets\n",
        "#image_datasets['train']\n",
        "#image_datasets['train'].classes\n",
        "#image_datasets['train'].imgs\n",
        "#image_datasets['train'].transforms\n",
        "#image_datasets['val']\n",
        "#dataloaders\n",
        "class_names\n",
        "#dataset_sizes"
      ],
      "metadata": {
        "id": "QBUNAOTelakz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -1 data/hymenoptera_data/train/ants | wc -l\n",
        "!ls -1 data/hymenoptera_data/train/bees | wc -l\n",
        "!ls -1 data/hymenoptera_data/val/ants | wc -l\n",
        "!ls -1 data/hymenoptera_data/val/bees | wc -l"
      ],
      "metadata": {
        "id": "fAWoGYQERtkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should find something goes wrong here!  \n",
        "\n",
        "(What is the \"wrong\" issue?)  \n",
        "(Find the reason why it happens and discuss it will harm the future process or not)  "
      ],
      "metadata": {
        "id": "ttwHNdHWmpiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment one these to see the image lists\n",
        "# Or you should check the google drive to check images\n",
        "\n",
        "!ls -1 data/hymenoptera_data/train/ants\n",
        "#!ls -1 data/hymenoptera_data/train/bees\n",
        "#!ls -1 data/hymenoptera_data/val/ants\n",
        "#!ls -1 data/hymenoptera_data/val/bees\n"
      ],
      "metadata": {
        "id": "sO_TYOj7SYGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vIAPuksaT_a"
      },
      "source": [
        "## Visualize a few images\n",
        "\n",
        "Let's visualize a few training images so as to understand the data augmentations.  \n",
        "\n",
        "(Note that the iter/next command pair is quite unique to python, and thanks to this pair, you can see new images everytime you run the second code cell.)  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def imshow(inp, title=None):\n",
        "    \"\"\"Display image for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
      ],
      "metadata": {
        "id": "BLLCTCpst-7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch of training data\n",
        "\n",
        "print(dataloaders['train'])\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "print(out.shape)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "# 0...ants, 1...bees\n",
        "print(classes)"
      ],
      "metadata": {
        "id": "hckPwPEtVlFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4a7skF3aT_a"
      },
      "outputs": [],
      "source": [
        "# See the dimensions (shape) of images here.\n",
        "# What do 4, 3, 224, and 224 mean?\n",
        "\n",
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# See how the true labels are given\n",
        "\n",
        "print(classes[0])\n",
        "print(classes[1])\n",
        "print(classes[2])\n",
        "print(classes[3])"
      ],
      "metadata": {
        "id": "7KmeXzEKrTdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generic preparation"
      ],
      "metadata": {
        "id": "yCWvWzmNflFM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUV9_ShQaT_b"
      },
      "source": [
        "### On training the model\n",
        "\n",
        "Now, let's write a **general** function to train a model. Here, we will illustrate:\n",
        "\n",
        "-  Scheduling the learning rate\n",
        "-  Saving the best model\n",
        "\n",
        "In the following, parameter ``scheduler`` is an LR scheduler object from ``torch.optim.lr_scheduler``.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USOtX3TZaT_b"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    # Create a temporary directory to save training checkpoints\n",
        "    with TemporaryDirectory() as tempdir:\n",
        "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
        "\n",
        "        torch.save(model.state_dict(), best_model_params_path)\n",
        "        best_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
        "            print('-' * 10)\n",
        "\n",
        "            # Each epoch has a training and validation phase\n",
        "            for phase in ['train', 'val']:\n",
        "                if phase == 'train':\n",
        "                    model.train()  # Set model to training mode\n",
        "                else:\n",
        "                    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                # Iterate over data.\n",
        "                for inputs, labels in dataloaders[phase]:\n",
        "                    inputs = inputs.to(device)\n",
        "                    labels = labels.to(device)\n",
        "\n",
        "                    # zero the parameter gradients\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    # forward\n",
        "                    # track history if only in train\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        outputs = model(inputs)\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        # backward + optimize only if in training phase\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "\n",
        "                    # statistics\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "                if phase == 'train':\n",
        "                    scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_sizes[phase]\n",
        "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'val' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    torch.save(model.state_dict(), best_model_params_path)\n",
        "\n",
        "            print()\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "        print(f'Best val Acc: {best_acc:4f}')\n",
        "\n",
        "        # load best model weights\n",
        "        model.load_state_dict(torch.load(best_model_params_path))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u950G0qfaT_c"
      },
      "source": [
        "### On visualizing the model predictions\n",
        "\n",
        "Generic function to display predictions for a few images.  \n",
        "Note that the images are taken from \"val\" dataset.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55mmXQx0aT_c"
      },
      "outputs": [],
      "source": [
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title(f'predicted: {class_names[preds[j]]}')\n",
        "                # 0 ant, 1 bee\n",
        "                # print(preds[j])\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj0PEI3MaT_c"
      },
      "source": [
        "## Finetuning the ConvNet (model_ft)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### A pretrained model\n",
        "\n",
        "Load a pretrained model (from strage space to memory) and reset the final fully connected layer."
      ],
      "metadata": {
        "id": "K9GQ8zSzZGbK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0vU2DQIaT_d"
      },
      "outputs": [],
      "source": [
        "model_ft = models.resnet18(weights='IMAGENET1K_V1')\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to ``nn.Linear(num_ftrs, len(class_names))``.\n",
        "model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3Fz0pmHaT_d"
      },
      "source": [
        "### Train !!\n",
        "\n",
        "It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.\n",
        "\n",
        "FYI: At the test run;\n",
        "\n",
        "* 25:03 on CPU (Intel(R) Xeon(R) CPU @ 2.20GHz x 2 processor)  \n",
        "*  1:30 on GPU (Intel(R) Xeon(R) CPU @ 2.30GHz x 2 + Tesla-T4)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "metadata": {
        "id": "LlMLq7HBklVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "JNTxKIb_bWcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2u029D1JaT_d"
      },
      "outputs": [],
      "source": [
        "# Warning: it takes time!\n",
        "\n",
        "model_ft   = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the result on \"val\" dataset"
      ],
      "metadata": {
        "id": "WvvbL78BgMUF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkyaRWbtaT_d"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfmL_ykhaT_d"
      },
      "source": [
        "## ConvNet as fixed feature extractor (model_conv)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### learning at the final layer only\n",
        "\n",
        "Here, we need to freeze all the network except the final layer.   \n",
        "We need to set ``requires_grad = False`` to freeze the parameters so that the gradients are not computed in ``backward()``.\n",
        "\n",
        "You can read more about this in the documentation [here](https://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward)_.\n"
      ],
      "metadata": {
        "id": "AHgl6GEoZdnA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt1GmPmtaT_d"
      },
      "outputs": [],
      "source": [
        "model_conv = torchvision.models.resnet18(weights='IMAGENET1K_V1')\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as\n",
        "# opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8ov4MXkaT_d"
      },
      "source": [
        "### Train !!\n",
        "\n",
        "On CPU this will take about half the time compared to previous scenario.\n",
        "This is expected as gradients don't need to be computed for most of the network. However, forward does need to be computed.\n",
        "\n",
        "FYI: At the test run;\n",
        "\n",
        "*  1:34 on GPU (Intel(R) Xeon(R) CPU @ 2.30GHz x 2 + Tesla-T4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJLvAZvbaT_d"
      },
      "outputs": [],
      "source": [
        "# Warning: it takes time!\n",
        "\n",
        "model_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check the results on \"val\" dataset"
      ],
      "metadata": {
        "id": "GDj1Ost6hrjW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqLut2WtaT_e"
      },
      "outputs": [],
      "source": [
        "visualize_model(model_conv)\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGeFU8IQaT_e"
      },
      "source": [
        "## Inference on custom images\n",
        "\n",
        "Use the trained model to make predictions on custom images and visualize\n",
        "the predicted class labels along with the images.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90IrPQCRaT_e"
      },
      "outputs": [],
      "source": [
        "def visualize_model_predictions(model,img_path):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    img = Image.open(img_path)\n",
        "    img = data_transforms['val'](img)\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(img)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        ax = plt.subplot(2,2,1)\n",
        "        ax.axis('on')\n",
        "        ax.set_title(f'Predicted: {class_names[preds[0]], preds[0]}')\n",
        "        imshow(img.cpu().data[0])\n",
        "\n",
        "        model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the results on val dataset. (Specify any of images.)"
      ],
      "metadata": {
        "id": "RbAfS5u0i2mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_path='data/hymenoptera_data/val/bees/72100438_73de9f17af.jpg'\n",
        "img_path='data/hymenoptera_data/val/ants/319494379_648fb5a1c6.jpg'\n",
        "img_path='usb_2.jpg'\n",
        "\n",
        "\n",
        "visualize_model_predictions(\n",
        "    model_ft,\n",
        "    img_path\n",
        ")\n",
        "\n",
        "visualize_model_predictions(\n",
        "    model_conv,\n",
        "    img_path\n",
        ")\n",
        "\n",
        "plt.ioff()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B1yWwC--pHbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJnLjy2-aT_e"
      },
      "source": [
        "## Further Learning\n",
        "\n",
        "If you would like to learn more about the applications of transfer learning,\n",
        "checkout our [Quantized Transfer Learning for Computer Vision Tutorial](https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html).\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n"
      ],
      "metadata": {
        "id": "qa_Y8DySdgNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building your own classifier with your images\n",
        "\n",
        "Let's build your won classifier.  \n",
        "\n",
        "Basic strategy is to keep the original structure as much as possible.\n",
        "\n",
        "* Two classes (By replacing ants and bees)\n",
        "* Images from a USB cameras"
      ],
      "metadata": {
        "id": "twl_9hoT6MJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Folders to cook"
      ],
      "metadata": {
        "id": "HQAWwdNQ6nuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete the unsed folders and add new folders (with the name of the new labels)\n",
        "\n",
        "# !rm -rf ./data/hymenoptera_data/train/ants\n",
        "# !mkdir  ./data/hymenoptera_data/train/face\n"
      ],
      "metadata": {
        "id": "W3ga3JSNt65r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips to get images from camera capture\n",
        "\n",
        "Taken from the help -> code snipet of google colab."
      ],
      "metadata": {
        "id": "rY6vygNUtca4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "metadata": {
        "id": "G9BK9_Qidkm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the initial number\n",
        "n=100"
      ],
      "metadata": {
        "id": "j_gQcCH6plJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# comment this line out to avoid error\n",
        "#from IPython.display import Image\n",
        "\n",
        "while n < 1000:\n",
        "\n",
        "    n = n+1\n",
        "\n",
        "    try:\n",
        "        # set the appropriate path to save images\n",
        "        filename = take_photo('./usb_{}.jpg'.format(n))\n",
        "        print('Saved to {}'.format(filename))\n",
        "\n",
        "        # Show the image which was just taken.\n",
        "        display(Image(filename))\n",
        "\n",
        "    except Exception as err:\n",
        "        # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "        # grant the page permission to access it.\n",
        "        print(str(err))"
      ],
      "metadata": {
        "id": "P66ZrjI0dkm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further study\n",
        "\n",
        "* More than two classes\n",
        "* Realtime USB camera image recognition on the fly\n",
        "* Investigation the factors that make influence on recognition performance\n",
        "* Code cleaning (as this course's code is a mix of snipets ...)"
      ],
      "metadata": {
        "id": "A0BjZHy06tfA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Tools and Practices for Intelligent Interaction Systems A  \n",
        "Master's and Docotal programs in intelligent and mechanical interaction systems, University of Tsukuba, Japan.  \n",
        "KAMEDA Yoshinari, SHIBUYA Takeshi  \n",
        "\n",
        "知能システムツール演習a  \n",
        "知能機能システム学位プログラム (筑波大学大学院)  \n",
        "担当：亀田能成，澁谷長史  \n",
        "\n",
        "2023/08/07.  \n"
      ],
      "metadata": {
        "id": "ROxUjHmIugLN"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}